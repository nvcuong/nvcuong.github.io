[["index.html", "Advanced Statistical Modelling III (second term) Module Information", " Advanced Statistical Modelling III (second term) Dr. Cuong Nguyen 2024-01-06 Module Information Module MATH3411: Advanced Statistical Modelling III (second term) Instructor: Cuong Nguyen Email: viet.c.nguyen@durham.ac.uk Acknowledgements: This material is based on the lecture notes in previous courses by Dr Samuel Jackson, Dr Jochen Einbeck, Dr Ric Crossman, Dr Emmanuel Ogundimu, Dr Ian Jermyn, Dr Louis Aslett, and Dr Reza Drikvandi. "],["generalised_linear_models.html", "Chapter 1 Review of Generalised Linear Models", " Chapter 1 Review of Generalised Linear Models Definition. A GLM is specified through the following components: A linear predictor: \\(\\eta = \\boldsymbol{\\beta}^{T}\\boldsymbol{x}\\). An injective response function \\(h\\), such that \\(\\mu = {\\mathrm E}[Y |\\boldsymbol{x}, \\boldsymbol{\\beta}] = h(\\eta) = h(\\boldsymbol{\\beta}^{T}\\boldsymbol{x})\\). Equivalently, one can write \\(g(\\mu) = \\boldsymbol{\\beta}^{T}\\boldsymbol{x}\\), where \\(g = h^{-1}\\) is the link function. The distributional assumption: \\(P_{}\\left(Y |\\boldsymbol{x}, \\boldsymbol{\\beta}\\right)\\) is an EDF, that is: \\[\\begin{equation} P_{}\\left(y |\\boldsymbol{x}, \\boldsymbol{\\beta}\\right) = P_{}\\left(y |\\theta(\\boldsymbol{x}, \\boldsymbol{\\beta}), \\phi(\\boldsymbol{x}, \\boldsymbol{\\beta})\\right) = \\exp \\Big( \\frac{y\\theta - b(\\theta)}{\\phi} + c(y, \\phi) \\Big) \\end{equation}\\] Thus, the mean and variance of this distribution are: \\[\\begin{align} {\\mathrm E}[Y |\\theta, \\phi] &amp;= \\mu = b&#39;(\\theta) \\\\ {\\mathrm{Var}}[Y |\\theta, \\phi] &amp;= \\phi \\, b&#39;&#39;(\\theta) = \\phi \\, b&#39;&#39;((b&#39;)^{-1}(\\mu)) = \\phi \\, \\mathcal{V}(\\mu) \\end{align}\\] We also assume independent data, that is: \\[\\begin{equation} P_{}\\left(\\left\\{y_{i}\\right\\} |\\left\\{\\boldsymbol{x}_{i}\\right\\}, \\boldsymbol{\\beta}\\right) = \\prod_{i=1}^n P_{}\\left(y_{i} |\\boldsymbol{x}_{i}, \\boldsymbol{\\beta}\\right) \\end{equation}\\] where \\(\\left\\{y_{i}, i = 1,...,n\\right\\}\\) are response data given the \\(\\left\\{\\boldsymbol{x}_i, i = 1,...,n\\right\\}\\). The Natural/Canonical Link. Recall that we have both: \\[\\begin{alignat}{4} \\mu &amp; = {\\mathrm E}[Y |\\theta, \\phi] &amp;&amp; = b&#39;(\\theta) \\tag{1.1} \\\\ \\mu &amp; = {\\mathrm E}[Y |\\boldsymbol{x}, \\boldsymbol{\\beta}] &amp;&amp; = h(\\boldsymbol{\\beta}^T\\boldsymbol{x}) = h(\\eta) \\tag{1.2} \\end{alignat}\\] with Equation (1.1) holding as a result of \\(P_{}\\left(y |\\theta, \\phi\\right)\\) following an EDF distribution, and Equation (1.2) holding by definition for a GLM. The natural link is the choice \\(h = b&#39;\\), or equivalently \\(g = (b&#39;)^{-1}\\), resulting in the equation \\[\\begin{equation} \\theta = \\boldsymbol{\\beta}^T\\boldsymbol{x} = \\eta. \\end{equation}\\] "],["estimation.html", "Chapter 2 Estimation 2.1 Likelihood 2.2 Log-Likelihood 2.3 Score Function and Equation 2.4 Fisher Information 2.5 Example: Poisson Regression 2.6 Properties of \\(\\boldsymbol{S}(\\boldsymbol{\\beta})\\) and \\(\\boldsymbol{F}(\\boldsymbol{\\beta})\\) 2.7 Matrix Notation 2.8 Iterative Solution of \\(\\boldsymbol{S}(\\hat{\\boldsymbol{\\beta}}) = 0\\) 2.9 Practical Example: Dataset B: US Polio Data 2.10 Estimation of \\(\\phi\\) 2.11 Asymptotic Properties of \\(\\hat{\\boldsymbol{\\beta}}\\)", " Chapter 2 Estimation 2.1 Likelihood Consider the grouped data setup where we have predictors and data with possible replicates \\(\\left\\{(\\boldsymbol{x}_{i}, y_{ir_{i}})\\right\\}_{i\\in[1..n], r_{i}\\in[1..m_{i}]}\\). Recall that under a GLM, given predictors \\(\\left\\{\\boldsymbol{x}_{i}\\right\\}_{i\\in[1..n]}\\), each response \\(y_{ir_{i}}\\) is independent of the other \\(y_{jr_{j}}\\), and of the values of all predictors \\(\\boldsymbol{x}_{j}\\) with \\(j\\neq i\\), so that the joint probability of the data — that is, the likelihood — is given by \\[\\begin{equation} L(\\boldsymbol{\\beta}) = P_{}\\left(\\left\\{y_{ir_{i}}\\right\\} |\\left\\{\\boldsymbol{x}_{i}\\right\\}, \\boldsymbol{\\beta}\\right) = P_{}\\left(\\left\\{y_{ir_{i}}\\right\\} |\\left\\{\\theta_{i}\\right\\}, \\phi\\right) = \\prod_{i = 1}^{n} \\prod_{r_{i} = 1}^{m_{i}} P_{}\\left(y_{ir_{i}} |\\theta_{i}, \\phi\\right) \\tag{2.1} \\end{equation}\\] where \\[\\begin{equation} P_{}\\left(y_{ir_{i}} |\\theta_{i}, \\phi\\right) = \\exp \\left( \\frac{y_{ir_{i}}\\theta_{i} - b(\\theta_{i})}{ \\phi} + c(y_{ir_{i}}, \\phi) \\right) \\end{equation}\\] with \\[\\begin{equation} \\theta_{i} = (b&#39;)^{-1}(\\mu_{i}) = (b&#39;)^{-1}(h(\\eta_{i})) = (b&#39;)^{-1}(h(\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i})) \\end{equation}\\] 2.2 Log-Likelihood The log probability of the data — or log-likelihood — is thus given by \\[\\begin{align} l(\\boldsymbol{\\beta}) &amp; = \\log P_{}\\left(\\left\\{y_{ir_{i}}\\right\\} |\\left\\{\\theta_{i}\\right\\}, \\phi\\right) \\\\ &amp; = \\sum_{i} \\sum_{r_{i}} \\left( \\frac{y_{ir_{i}}\\theta_{i} - b(\\theta_{i})}{\\phi} + c(y_{ir_{i}}, \\phi) \\right) \\\\ &amp; = \\sum_{i} \\left( m_{i} \\frac{y_{i}\\theta_{i} - b(\\theta_{i})}{\\phi} + \\sum_{r_{i}} c(y_{ir_{i}}, \\phi) \\right) \\\\ &amp; = \\sum_{i} l_{i} \\end{align}\\] where \\[\\begin{equation} l_i = \\frac{y_{i}\\theta_{i} - b(\\theta_{i})}{\\phi_i} + \\sum_{r_{i}} c(y_{ir_{i}}, \\phi) \\end{equation}\\] \\[\\begin{equation} \\phi_i = \\phi/m_i \\end{equation}\\] and \\[\\begin{equation} y_i = \\frac{1}{m_i} \\sum_{r_{i}} y_{ir_{i}} \\end{equation}\\] 2.3 Score Function and Equation The score function is given by \\[\\begin{equation} \\boldsymbol{S}(\\boldsymbol{\\beta}) = \\frac{\\partial l }{ \\partial \\boldsymbol{\\beta}^{T}} = \\sum_{i} \\frac{\\partial l_{i} }{ \\partial \\boldsymbol{\\beta}^{T}} = \\sum_{i} \\frac{\\partial l_{i} }{ \\partial \\theta_{i}} \\frac{\\partial \\theta_{i} }{ \\partial \\mu_{i}} \\frac{\\partial \\mu_{i} }{ \\partial \\eta_{i}} \\frac{\\partial \\eta_{i} }{ \\partial \\boldsymbol{\\beta}^{T}} \\tag{2.2} \\end{equation}\\] where, recalling that \\(\\mu_i = b&#39;(\\theta_i)\\), \\(\\mu_i = h(\\eta_i)\\) and \\(\\eta_i = \\boldsymbol{\\beta}^T\\boldsymbol{x}_i\\), we have1: \\[\\begin{align} \\frac{\\partial l_{i} }{ \\partial \\theta_{i}} &amp; = \\frac{y_{i} - b&#39;(\\theta_{i})}{ \\phi_{i}} = \\frac{y_{i} - \\mu_{i}}{ \\phi_{i}} \\\\ \\frac{\\partial \\theta_{i} }{ \\partial \\mu_{i}} &amp; = \\frac{\\partial (b&#39;)^{-1} }{ \\partial \\mu_{i}} = \\frac{1 }{ b&#39;&#39;((b&#39;)^{-1}(\\mu_{i}))} = \\frac{1 }{ \\mathcal{V}(\\mu_{i})} = \\frac{1 }{ b&#39;&#39;(\\theta_{i})} \\tag{2.3}\\\\ \\frac{\\partial \\mu_{i} }{ \\partial \\eta_{i}} &amp; = h&#39;(\\eta_{i}) \\tag{2.4}\\\\ \\frac{\\partial \\eta_{i} }{ \\partial \\boldsymbol{\\beta}^{T}} &amp; = \\boldsymbol{x}_{i} \\end{align}\\] The score function is thus given by \\[\\begin{align} \\boldsymbol{S}(\\boldsymbol{\\beta}) &amp; = \\sum_{i} \\left( \\frac{y_{i} - \\mu_{i}}{ \\phi_{i}} \\right)\\; \\left( \\frac{1 }{ \\mathcal{V}(\\mu_{i})} \\right)\\; h&#39;(\\eta_{i})\\;\\boldsymbol{x}_{i} \\\\ &amp; = \\frac{1}{\\phi}\\sum_{i} m_{i}(y_{i} - \\mu_{i})\\;\\frac{1}{ \\mathcal{V}(\\mu_{i})}\\; h&#39;(\\eta_{i})\\;\\boldsymbol{x}_{i} \\tag{2.5} \\end{align}\\] The maximum likelihood estimate \\(\\hat{\\boldsymbol{\\beta}}\\) must then satisfy the score equation: \\[\\begin{equation} \\boldsymbol{S}(\\hat{\\boldsymbol{\\beta}}) = 0 \\tag{2.6} \\end{equation}\\] Note that the dispersion parameter \\(\\phi\\) cancels from the score equation, which implies that \\(\\hat{\\boldsymbol{\\beta}}\\) does not depend on \\(\\phi\\). This is another important property of EDFs. 2.3.1 Natural Link For the natural link, \\(\\theta_{i} = \\eta_{i}\\), so Equations (2.3) and (2.4) combine to give \\[\\begin{equation} \\frac{h&#39;(\\eta_i)}{\\mathcal{V}(\\mu_i)} = \\frac{\\partial\\theta_{i}}{\\partial \\mu_{i}} \\frac{\\partial\\mu_{i}}{\\partial \\eta_{i}} = \\frac{\\partial\\theta_{i}}{\\partial \\eta_{i}} = 1 \\end{equation}\\] The score function thus simplifies to \\[\\begin{equation} \\boldsymbol{S}(\\boldsymbol{\\beta}) = \\frac{1}{\\phi}\\sum_{i} m_{i}(y_{i} - \\mu_{i})\\;\\boldsymbol{x}_{i} \\tag{2.7} \\end{equation}\\] 2.4 Fisher Information For future developments, we will also need the second derivative of the log likelihood. Up to a change of sign, this is called the Observed Fisher Information, defined as \\[\\begin{equation} \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}) = - \\frac{\\partial^{2} l }{ \\partial\\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} = - \\frac{\\partial \\boldsymbol{S}}{\\partial \\boldsymbol{\\beta}} \\tag{2.8} \\end{equation}\\] Note that, at the MLE, \\(\\boldsymbol{F}_{\\text{obs}}(\\hat{\\boldsymbol{\\beta}})\\) is positive by definition. Because it is a function of the data \\(\\left\\{y_{i}\\right\\}\\), \\(\\boldsymbol{F}_{\\text{obs}}\\) has a probability distribution. In practice, the Observed Fisher Information is often approximated by the Expected Fisher Information2, otherwise known simply as the Fisher Information: \\[\\begin{equation} \\boldsymbol{F}(\\boldsymbol{\\beta}) = E \\left[ - \\frac{\\partial \\boldsymbol{S} }{ \\partial \\boldsymbol{\\beta}} \\right] \\tag{2.9} \\end{equation}\\] where the expectation is taken over the joint probability distribution of the data \\(P_{}\\left(\\left\\{y_{ir_{i}}\\right\\} |\\boldsymbol{\\beta}, \\left\\{\\boldsymbol{x}_{i}\\right\\}\\right)\\). 2.5 Example: Poisson Regression We look at two example calculations of the score function and Fisher Information for Poisson Regression, that is we have \\(y |\\boldsymbol{x}, \\boldsymbol{\\beta} \\sim \\text{Poi}(\\lambda(\\boldsymbol{x}))\\) \\(\\phi = 1\\) 2.5.1 With Natural Link We have that \\(\\lambda(\\boldsymbol{x}) = \\mu(\\boldsymbol{x}) = h(\\eta(\\boldsymbol{x})) = e^{\\eta(\\boldsymbol{x})} = e^{\\boldsymbol{\\beta}^T\\boldsymbol{x}}\\) Equation (2.7) then gives: \\[\\begin{equation} \\boldsymbol{S}(\\boldsymbol{\\beta}) = \\sum_{i} (y_{i} - e^{\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i}})\\;\\boldsymbol{x}_{i} \\end{equation}\\] while Equation (2.8) gives \\[\\begin{equation} \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}) = \\sum_{i} e^{\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i}}\\;\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\end{equation}\\] Note that this does not depend on the data, so that Equation (2.9) gives \\[\\begin{equation} \\boldsymbol{F}(\\boldsymbol{\\beta}) = {\\mathrm E}[\\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta})] = \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}) \\end{equation}\\] 2.5.2 With Identity Link The identity link is defined such that \\(h(\\eta) = \\eta\\). In this case we have that: \\(\\lambda(\\boldsymbol{x}) = \\mu(\\boldsymbol{x}) = h(\\eta(\\boldsymbol{x})) = \\eta(\\boldsymbol{x}) = \\boldsymbol{\\beta}^T\\boldsymbol{x}\\) \\(\\mathcal{V}(\\mu) = \\mu\\) (see Poisson example in EDF chapter) \\(h&#39;(\\eta) = 1\\) Equation (2.5) gives \\[\\begin{align} \\boldsymbol{S}(\\boldsymbol{\\beta}) &amp; = \\sum_{i} (y_{i} - \\mu_{i})\\;\\frac{1}{ \\mu_{i}}\\;1 \\;\\boldsymbol{x}_{i} \\\\ &amp; = \\sum_{i} (y_{i} - \\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i}) \\;\\frac{1}{ \\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i}}\\; \\boldsymbol{x}_{i} \\\\ &amp; = \\sum_{i} \\left( \\frac{y_{i}}{ \\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i}} - 1 \\right)\\; \\boldsymbol{x}_{i} \\end{align}\\] The Observed Fisher Information is given by \\[\\begin{equation} \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}) = \\sum_{i} \\frac{y_{i}}{ (\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i})^{2}}\\;\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\end{equation}\\] and the Fisher Information is given by \\[\\begin{align} \\boldsymbol{F}(\\boldsymbol{\\beta}) &amp; = {\\mathrm E}[\\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta})] \\\\ &amp; = \\textrm{E} \\left[ \\sum_{i} \\frac{Y_{i}}{(\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i})^{2}}\\;\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\right] \\\\ &amp; = \\sum_{i} \\frac{{\\mathrm E}[Y_{i} |\\boldsymbol{\\beta}, \\boldsymbol{x}_{i}] }{ (\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i})^{2}}\\; \\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\\\ &amp; = \\sum_{i} \\frac{\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i} }{ (\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i})^{2}}\\; \\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\\\ &amp; = \\sum_{i} \\frac{1 }{ \\boldsymbol{\\beta}^{T}\\boldsymbol{x}_{i}}\\;\\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\end{align}\\] Note that \\(\\boldsymbol{F}(\\boldsymbol{\\beta}) \\neq \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta})\\) in this case. 2.6 Properties of \\(\\boldsymbol{S}(\\boldsymbol{\\beta})\\) and \\(\\boldsymbol{F}(\\boldsymbol{\\beta})\\) Defining \\(S_i(\\boldsymbol{\\beta}) = \\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta}}\\), we have that \\(\\boldsymbol{S}(\\boldsymbol{\\beta}) = \\sum_{i} \\boldsymbol{S}_{i}(\\boldsymbol{\\beta})\\). 2.6.1 Expectation of \\(\\boldsymbol{S}(\\boldsymbol{\\beta})\\) The expectation of \\(\\boldsymbol{S}(\\boldsymbol{\\beta})\\) can be computed from Equation (2.2) as follows: \\[\\begin{align} {\\mathrm E}[\\boldsymbol{S}(\\boldsymbol{\\beta})] &amp; = \\sum_{i} {\\mathrm E}[\\boldsymbol{S}_{i}(\\boldsymbol{\\beta})] \\\\ &amp; = \\sum_{i} \\frac{{\\mathrm E}[Y_{i} |\\boldsymbol{\\beta}, \\boldsymbol{x}_{i}] - \\mu_{i}}{ \\phi_{i}} \\frac{1}{\\mathcal{V}(\\mu_{i})} h&#39;(\\eta_{i})\\;\\boldsymbol{x}_i \\\\ &amp; = 0 \\tag{2.10} \\end{align}\\] because \\({\\mathrm E}[Y_{i} |\\boldsymbol{\\beta}, \\boldsymbol{x}_{i}] = \\mu_{i}\\). 2.6.2 Variance of \\(\\boldsymbol{S}(\\boldsymbol{\\beta})\\) To calculate the variance of \\(\\boldsymbol{S}(\\boldsymbol{\\beta})\\), we proceed as follows. First, we note that \\[\\begin{align} {\\mathrm{Var}}[\\boldsymbol{S}(\\boldsymbol{\\beta})] &amp; = {\\mathrm E}[\\boldsymbol{S}(\\boldsymbol{\\beta})\\boldsymbol{S}(\\boldsymbol{\\beta})^{T}] - {\\mathrm E}[\\boldsymbol{S}(\\boldsymbol{\\beta})]{\\mathrm E}[\\boldsymbol{S}(\\boldsymbol{\\beta})^T] \\\\ &amp; = {\\mathrm E}[\\boldsymbol{S}(\\boldsymbol{\\beta})\\boldsymbol{S}(\\boldsymbol{\\beta})^{T}] \\end{align}\\] because \\({\\mathrm E}[\\boldsymbol{S}(\\boldsymbol{\\beta})] = 0\\) from Equation (2.10). We then have (dropping the argument \\(\\boldsymbol{\\beta}\\)) \\[\\begin{equation} {\\mathrm E}[\\boldsymbol{S} \\boldsymbol{S}^{T}] = \\mathrm{E} \\left[ \\left( \\sum_{i} \\boldsymbol{S}_{i} \\right) \\left( \\sum_{j}\\boldsymbol{S}_{j}^{T} \\right) \\right] = \\sum_{i, j}{\\mathrm E}[\\boldsymbol{S}_{i}\\boldsymbol{S}_{j}^{T}] \\tag{2.11} \\end{equation}\\] with \\[\\begin{align} {\\mathrm E}[\\boldsymbol{S}_{i}\\boldsymbol{S}_{j}^{T}] &amp; = \\mathrm{E} \\left[ \\left( \\frac{Y_{i} - \\mu_{i}}{ \\phi_{i}} \\frac{1}{\\mathcal{V}(\\mu_{i})} h&#39;(\\eta_{i})\\;\\boldsymbol{x}_i \\right) \\left( \\frac{Y_{j} - \\mu_{j}}{ \\phi_{j}} \\frac{1}{\\mathcal{V}(\\mu_{j})} h&#39;(\\eta_{j})\\;\\boldsymbol{x}_j \\right) \\right] \\\\ &amp; = \\delta_{ij}\\; \\frac{\\phi_{i}\\mathcal{V}(\\mu_{i})}{ \\phi_{i}^2\\mathcal{V}(\\mu_{i})^2}\\;h&#39;(\\eta_{i})^{2}\\; \\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\\\ &amp; = \\delta_{ij}\\; \\frac{1}{ \\phi_{i}\\mathcal{V}(\\mu_{i})}\\;h&#39;(\\eta_{i})^{2}\\; \\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\end{align}\\] since \\[\\begin{align} {\\mathrm E}[(Y_{i} - \\mu_{i})^{2}] &amp; = {\\mathrm{Var}}[Y_{i} |\\boldsymbol{\\beta}, \\boldsymbol{x}_{i}] = \\phi_{i}\\mathcal{V}(\\mu_{i}) \\\\ {\\mathrm E}[(Y_{i} - \\mu_{i})(Y_{j} - \\mu_{j})] &amp; = {\\mathrm{Cov}}[Y_{i}, Y_{j} |\\boldsymbol{\\beta}, \\boldsymbol{x}_{i}] = 0 &amp; &amp; \\text{$i\\neq j$} \\end{align}\\] so that \\[\\begin{equation} {\\mathrm{Var}}[\\boldsymbol{S}(\\boldsymbol{\\beta})] = \\sum_{i} \\frac{h&#39;(\\eta_{i})^{2}}{ \\phi_{i}\\mathcal{V}(\\mu_{i})} \\; \\boldsymbol{x}_{i}\\boldsymbol{x}_{i}^{T} \\tag{2.12} \\end{equation}\\] 2.6.3 \\(\\boldsymbol{F}(\\boldsymbol{\\beta})\\) 2.6.3.1 An Important Identity Let \\(\\rho = e^l\\), where \\(l\\) is the log-likelihood, so that \\(\\rho = L(\\boldsymbol{\\beta}) = P_{}\\left(\\left\\{y_{ir_{i}}\\right\\} |\\left\\{\\boldsymbol{x}_{i}\\right\\}, \\boldsymbol{\\beta}\\right)\\) is the likelihood/probability of the data. Then \\[\\begin{equation} \\frac{\\partial l}{\\partial \\boldsymbol{\\beta}^{T}} = \\frac{\\partial l}{\\partial \\rho}\\frac{\\partial \\rho}{\\partial \\boldsymbol{\\beta}^{T}} = \\frac{1}{\\rho}\\frac{\\partial \\rho}{\\partial \\boldsymbol{\\beta}^{T}} \\end{equation}\\] and3 \\[\\begin{align} \\frac{\\partial^{2} l}{\\partial\\boldsymbol{\\beta}^{T}\\partial \\boldsymbol{\\beta}} &amp; = - \\frac{1}{\\rho^{2}} \\frac{\\partial \\rho}{\\partial \\boldsymbol{\\beta}^{T}} \\frac{\\partial \\rho}{\\partial \\boldsymbol{\\beta}} + \\frac{1}{\\rho} \\frac{\\partial^{2} \\rho}{ \\partial \\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} \\\\ &amp; = - \\frac{\\partial l}{\\partial \\boldsymbol{\\beta}^{T}} \\frac{\\partial l}{\\partial \\boldsymbol{\\beta}} + \\frac{1}{\\rho} \\frac{\\partial^{2} \\rho}{\\partial \\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} \\tag{2.13} \\end{align}\\] The expectation (over the data) of the second term is then \\[\\begin{equation} \\mathrm{E} \\left[ \\frac{1 }{ \\rho}\\frac{\\partial^{2} \\rho}{ \\partial \\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} \\right] = \\int \\rho \\;\\frac{1 }{ \\rho} \\frac{\\partial^{2} \\rho}{ \\partial \\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} = \\int \\frac{\\partial^{2} \\rho}{ \\partial \\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} = \\frac{\\partial^{2} }{ \\partial \\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} \\int \\rho = \\frac{\\partial^{2} }{ \\partial \\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}}\\;1 = 0 \\tag{2.14} \\end{equation}\\] 2.6.3.2 Relating \\(\\boldsymbol{F}(\\boldsymbol{\\beta})\\) and \\({\\mathrm{Var}}[\\boldsymbol{S}(\\boldsymbol{\\beta})]\\) Using Equations (2.13) and (2.14), we have that \\[\\begin{equation} \\boldsymbol{F}(\\boldsymbol{\\beta}) = - \\mathrm{E} \\left[ \\frac{\\partial^{2}l }{ \\partial \\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} \\right] = \\mathrm{E} \\left[ \\frac{\\partial l }{ \\partial \\boldsymbol{\\beta}^{T}} \\frac{\\partial l }{ \\partial\\boldsymbol{\\beta}} \\right] = {\\mathrm E}[\\boldsymbol{S}(\\boldsymbol{\\beta})\\boldsymbol{S}(\\boldsymbol{\\beta})^{T}] = {\\mathrm{Var}}[\\boldsymbol{S}(\\boldsymbol{\\beta})] \\end{equation}\\] 2.6.3.3 Natural Link For the natural link, recall that \\(\\frac{h&#39;(\\eta_i)}{\\mathcal{V}(\\mu_i)} = 1\\), so that: \\[\\begin{eqnarray} \\boldsymbol{S}(\\boldsymbol{\\beta}) &amp; = &amp; \\sum_i \\frac{1}{\\phi_i} (y_i - h(\\eta_i))\\boldsymbol{x}_i \\\\ \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}) &amp; = &amp; - \\boldsymbol{S}&#39;(\\boldsymbol{\\beta}) = \\sum_i \\frac{h&#39;(\\eta_i)}{\\phi_i} \\boldsymbol{x}_i\\boldsymbol{x}_i^T \\\\ \\boldsymbol{F}(\\boldsymbol{\\beta}) &amp; = &amp; {\\mathrm{Var}}[\\boldsymbol{S}(\\boldsymbol{\\beta})] = \\sum_i \\frac{h&#39;(\\eta_i)}{\\phi_i}\\boldsymbol{x}_i\\boldsymbol{x}_i^T \\end{eqnarray}\\] Thus, for the natural link, we see that \\(\\boldsymbol{F}(\\boldsymbol{\\beta}) = \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta})\\). 2.7 Matrix Notation For the next section, it is useful to establish a condensed, matrix notation for some of the previous quantities, analogous to the matrix notation used for linear models. Let \\(\\boldsymbol{Y}\\in{\\mathbb R}^{n}\\) be the random vector with components \\(Y_{i}\\), the response values. This is exactly the same quantity as in the linear model case. Let \\(\\boldsymbol{X}\\in{\\mathbb R}^{n\\times p}\\) be the design matrix, the matrix with components \\(x_{i, a}\\), the value of the \\(a^{\\text{th}}\\) component of the predictor vector for the \\(i^{\\text{th}}\\) data point. This is exactly the same quantity as in the linear model case. Let \\(\\boldsymbol{\\mu}\\in{\\mathbb R}^{n}\\) be the vector with components \\(\\mu_{i} = h(\\boldsymbol{\\beta}^{T}x_{i})\\), so that \\[\\begin{equation} \\boldsymbol{\\mu} = {\\mathrm E}[\\boldsymbol{Y}] \\end{equation}\\] Let \\(\\boldsymbol{D}\\in {\\mathbb R}^{n\\times n}\\) be the diagonal matrix with components \\(D_{ii} = h&#39;(\\eta_{i})\\). For example, if \\(h(\\eta) = e^{\\eta}\\), then \\[\\begin{equation} \\boldsymbol{D} = \\begin{pmatrix} e^{\\boldsymbol{\\beta}^{T}x_{1}} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; e^{\\boldsymbol{\\beta}^{T}x_{n}} \\end{pmatrix} \\end{equation}\\] Let \\(\\boldsymbol{\\Sigma}\\in{\\mathbb R}^{n\\times n}\\) be the covariance matrix \\(\\boldsymbol{Y}\\), with components: \\[\\begin{equation} \\Sigma_{ij} = \\text{Cov}[Y_{i}, Y_{j}] = \\text{Var}[Y_{i}]\\;\\delta_{ij} = \\phi_{i}\\mathcal{V}(\\mu_{i})\\;\\delta_{ij} \\end{equation}\\] that is, \\[\\begin{equation} \\boldsymbol{\\Sigma} = \\begin{pmatrix} \\text{Var}[Y_{1}] &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; \\text{Var}[Y_{n}] \\end{pmatrix} = \\begin{pmatrix} \\phi_{1}\\mathcal{V}(\\mu_{1}) &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\ddots &amp; \\ddots &amp; \\vdots \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0 &amp; \\phi_{n}\\mathcal{V}(\\mu_{n}) \\end{pmatrix} \\end{equation}\\] 2.7.1 Score Function and Fisher Information Recall that \\[\\begin{align} \\boldsymbol{S}(\\boldsymbol{\\beta}) &amp; = \\sum_{i} \\left( \\frac{y_{i} - \\mu_{i}}{ \\phi_{i}\\mathcal{V}(\\mu_{i})} \\right)\\; h&#39;(\\eta_{i})\\;x_{i} \\\\ \\boldsymbol{F}(\\boldsymbol{\\beta}) &amp; = \\sum_{i} \\frac{h&#39;(\\eta_{i})^{2}}{ \\phi_{i}\\mathcal{V}(\\mu_{i})} \\;x_{i}x_{i}^{T} \\end{align}\\] In terms of the matrix notation, these become \\[\\begin{align} \\boldsymbol{S} &amp; = \\boldsymbol{X}^{T}\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y} - \\boldsymbol{\\mu}) \\\\ \\boldsymbol{F} &amp; = \\boldsymbol{X}^{T}\\boldsymbol{D}^{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{D}\\boldsymbol{X} \\end{align}\\] 2.7.2 Natural Link Note that for the natural link, \\[\\begin{equation} \\frac{\\partial \\theta_{i}}{ \\partial \\eta_{i}} = \\frac{h&#39;(\\eta_{i}) }{ \\mathcal{V}(\\mu_{i})} = 1 \\end{equation}\\] Thus \\[\\begin{equation} h&#39;(\\eta_{i}) = \\mathcal{V}(\\mu_{i}) = \\frac{\\text{Var}[Y_{i}] }{ \\phi_{i}} = m_{i}\\frac{{\\mathrm{Var}}[Y_{i}] }{ \\phi} \\end{equation}\\] if \\(\\phi_{i} = \\phi/m_{i}\\). Now Let \\(\\boldsymbol{G} \\in {\\mathbb R}^{n \\times n}\\) be the diagonal matrix with components \\(m_{i}\\delta_{ij}\\), known as the grouping matrix. Then \\[\\begin{equation} \\boldsymbol{D} = \\frac{1}{\\phi} \\boldsymbol{G}\\boldsymbol{\\Sigma} = \\frac{1}{\\phi}\\boldsymbol{\\Sigma} \\boldsymbol{G} \\end{equation}\\] and thus \\[\\begin{align} \\boldsymbol{S}(\\boldsymbol{\\beta}) &amp; = \\frac{1}{\\phi} \\boldsymbol{X}^{T}\\boldsymbol{G}(\\boldsymbol{Y} - \\boldsymbol{\\mu}) \\\\ \\boldsymbol{F}(\\boldsymbol{\\beta}) &amp; = \\frac{1}{\\phi^{2}} \\boldsymbol{X}^{T}\\boldsymbol{G}^{T}\\boldsymbol{\\Sigma}\\boldsymbol{G}\\boldsymbol{X} \\end{align}\\] 2.8 Iterative Solution of \\(\\boldsymbol{S}(\\hat{\\boldsymbol{\\beta}}) = 0\\) So far we have seen how to set up the score equation for the maximum likelihood estimate, and some of its properties, as well as those of the Fisher Information. We now turn to the question of how to solve the score equation. As we have seen, except in rare cases, this cannot be done in closed form, and so we turn to numerical methods, implemented on a computer. We have the same two options here as in the binary regression case. We can try to optimise \\(l\\) directly, or we can attempt to solve the score equation. There are many algorithms that can be used to perform these tasks. Here we focus on one: iteratively reweighted least squares (IRLS), also known as iteratively weighted least squares (IWLS).4 We start by recalling the Newton-Raphson method for finding the zero of a function. We wish to solve an equation \\[\\begin{equation} \\boldsymbol{S}(\\hat{\\boldsymbol{\\beta}}) = 0 \\end{equation}\\] We then approximate \\(\\boldsymbol{S}\\) linearly about some point: \\[\\begin{equation} \\boldsymbol{S}(\\boldsymbol{\\beta}_{0} + \\delta\\boldsymbol{\\beta}_{0}) = \\boldsymbol{S}(\\boldsymbol{\\beta}_{0}) + \\frac{\\partial \\boldsymbol{S}(\\boldsymbol{\\beta}_{0})}{\\partial\\boldsymbol{\\beta}}\\delta\\boldsymbol{\\beta}_{0} + \\mathcal{O}(\\delta\\boldsymbol{\\beta}_{0}^{2}) \\end{equation}\\] where the reason for the subscript \\(0\\) will become apparent soon. In the case when \\(\\boldsymbol{S}(\\boldsymbol{\\beta}_{0} + \\delta\\boldsymbol{\\beta}_{0}) = 0\\) (such as we are interested in), we have approximately that \\[\\begin{equation} \\frac{\\partial \\boldsymbol{S}(\\boldsymbol{\\beta}_{0})}{\\partial\\boldsymbol{\\beta}}\\delta\\boldsymbol{\\beta}_{0} = -\\boldsymbol{S}(\\boldsymbol{\\beta}_{0}) \\tag{2.15} \\end{equation}\\] Now in our case \\[\\begin{equation} -\\frac{\\partial \\boldsymbol{S}(\\boldsymbol{\\beta}_{0})}{\\partial\\boldsymbol{\\beta}} = \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}_{0}) \\end{equation}\\] so Equation (2.15) becomes \\[\\begin{equation} \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}_{0})\\delta\\boldsymbol{\\beta}_{0} = \\boldsymbol{S}(\\boldsymbol{\\beta}_{0}) \\end{equation}\\] or equivalently \\[\\begin{equation} \\delta\\boldsymbol{\\beta}_{0} = \\left( \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}_{0}) \\right)^{-1} \\boldsymbol{S}(\\boldsymbol{\\beta}_{0}) \\end{equation}\\] This then gives a new value \\[\\begin{equation} \\boldsymbol{\\beta}_{1} = \\boldsymbol{\\beta}_{0} + \\delta\\boldsymbol{\\beta}_{0} \\end{equation}\\], and we then iterate: \\[\\begin{align} \\boldsymbol{\\beta}_{m+1} &amp; = \\boldsymbol{\\beta}_{m} + \\delta\\boldsymbol{\\beta}_{m} \\end{align}\\] where \\[\\begin{align} \\delta\\boldsymbol{\\beta}_{m} &amp; = \\left( \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}_{m}) \\right)^{-1} \\boldsymbol{S}(\\boldsymbol{\\beta}_{m}) \\end{align}\\] Because \\(\\boldsymbol{F}_{\\text{obs}}\\) is hard to find and hard to invert in general, we approximate it with the expected Fisher Information. This is known as Fisher scoring: \\[\\begin{equation} \\delta\\boldsymbol{\\beta}_{m} = \\left( \\boldsymbol{F}(\\boldsymbol{\\beta}_{m}) \\right)^{-1} \\boldsymbol{S}(\\boldsymbol{\\beta}_{m}) \\end{equation}\\] More usefully, we have that \\[\\begin{equation} \\boldsymbol{F}(\\boldsymbol{\\beta}_{m})\\delta\\boldsymbol{\\beta}_{m} = \\boldsymbol{S}(\\boldsymbol{\\beta}_{m}) \\end{equation}\\] or equivalently that \\[\\begin{equation} \\boldsymbol{F}(\\boldsymbol{\\beta}_{m})\\boldsymbol{\\beta}_{m+1} = \\boldsymbol{F}(\\boldsymbol{\\beta}_{m})\\boldsymbol{\\beta}_{m} + \\boldsymbol{S}(\\boldsymbol{\\beta}_{m}) \\tag{2.16} \\end{equation}\\] By defining \\(\\boldsymbol{W} = \\boldsymbol{D}^{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{D}\\), we can write \\[\\begin{equation} \\boldsymbol{F} = \\boldsymbol{X}^{T}\\boldsymbol{D}^{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{D}\\boldsymbol{X} = \\boldsymbol{X}^{T}\\boldsymbol{W}\\boldsymbol{X} \\end{equation}\\] and \\[\\begin{equation} \\boldsymbol{S} = \\boldsymbol{X}^{T}\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y} - \\boldsymbol{\\mu}) = \\boldsymbol{X}^{T}\\boldsymbol{W}\\boldsymbol{D}^{-1}(\\boldsymbol{Y} - \\boldsymbol{\\mu}) \\end{equation}\\] Thus we can calculate the right-hand side of Equation (2.16) from5: \\[\\begin{equation} \\boldsymbol{F}_{m}\\boldsymbol{\\beta}_{m} + \\boldsymbol{S}_{m} = \\boldsymbol{X}^{T}\\boldsymbol{W}_{m}\\boldsymbol{X}\\boldsymbol{\\beta}_{m} + \\boldsymbol{X}^{T}\\boldsymbol{W}_{m}\\boldsymbol{D}_{m}^{-1}(\\boldsymbol{Y} - \\boldsymbol{\\mu}_{m}) = \\boldsymbol{X}^{T}\\boldsymbol{W}_{m}\\boldsymbol{\\tilde{Y}}_{m} \\end{equation}\\] where \\[\\begin{equation} \\boldsymbol{\\tilde{Y}}_{m} = \\boldsymbol{X}\\boldsymbol{\\beta}_{m} + \\boldsymbol{D}_{m}^{-1}(\\boldsymbol{Y} - \\boldsymbol{\\mu}_{m}) \\end{equation}\\] are the so-called working observations. By replacing the left hand side of Equation (2.16) with \\(\\boldsymbol{X}^{T}\\boldsymbol{W}_{m}\\boldsymbol{X}\\), we have that \\[\\begin{align} (\\boldsymbol{X}^{T}\\boldsymbol{W}_{m}\\boldsymbol{X}) \\boldsymbol{\\beta}_{m+1} &amp; = \\boldsymbol{X}^{T}\\boldsymbol{W}_{m}\\boldsymbol{\\tilde{Y}}_{m} \\\\ \\boldsymbol{\\beta}_{m+1} &amp; = (\\boldsymbol{X}^{T}\\boldsymbol{W}_{m}\\boldsymbol{X})^{-1}\\boldsymbol{X}^{T}\\boldsymbol{W}_{m}\\boldsymbol{\\tilde{Y}}_{m} \\end{align}\\] This sequence of iterated operations is called iteratively re-weighted least squares (IRLS) or iterative weighted least squares (IWLS) since each iteration is the solution to the following least squares problem: minimize the quantity \\(l_{m}(\\boldsymbol{\\beta})\\) with respect to \\(\\boldsymbol{\\beta}\\), where \\[\\begin{equation} l_{m}(\\boldsymbol{\\beta}) = (\\boldsymbol{\\tilde{Y}}_{m} - \\boldsymbol{X}\\boldsymbol{\\beta})^{T} \\boldsymbol{W}_{m} (\\boldsymbol{\\tilde{Y}}_{m} - \\boldsymbol{X}\\boldsymbol{\\beta}) \\end{equation}\\] As a result, \\(\\boldsymbol{W}\\) is known as the weight matrix. 2.8.1 IRLS Pseudo-Code Note that the following is pseudo-code for running IRLS, as without computing \\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{D}\\) and \\(\\boldsymbol{W}\\) using a specific example this will not run. IRLS &lt;- function(Y, X, phi, epsilon) { # Pick an initial value for hatBeta. hatbeta = initializeBeta() # Set up convergence. converged = false # Loop as long as convergence condition is not satisfied. while not converged loop { # Compute mu, D, and Sigma (use h, h&#39;, V as subroutines) mu = computeMu(hatBeta, X) D = computeD(hatBeta, X) Sigma = computeSigma(hatBeta, phi) # Compute the weight matrix, W. W = t(D) %*% solve(Sigma) %*% D # Compute the working observations, tildeY. tildeY = X %*% hatBeta + solve(D) %*% (Y - mu) # Compute the new value of hatBeta. newHatBeta = solve(t(X) %*% W %*% X) %*% (t(X) %*% W %*% tildeY) # Check whether we have converged. converged = ((norm(newHatBeta - hatBeta) / norm(hatBeta)) &lt;= epsilon) # Store new value of hatBeta ready for next iteration or return. hatBeta = newHatBeta } return hatBeta } 2.9 Practical Example: Dataset B: US Polio Data We start by loading the amended polio data from library gamlss.data as discussed in Section @ref(polio_data). library( &quot;gamlss.data&quot; ) ## ## Attaching package: &#39;gamlss.data&#39; ## The following object is masked from &#39;package:datasets&#39;: ## ## sleep data( &quot;polio&quot; ) uspolio &lt;- as.data.frame( matrix( c( 1:168, t( polio ) ), ncol = 2 ) ) colnames( uspolio ) &lt;- c(&quot;time&quot;, &quot;cases&quot;) We begin by fitting a Poisson model with a linear time trend. # Poisson model with linear time trend polio.glm &lt;- glm( cases ~ time, family = poisson( link = log ), data = uspolio ) # Look at the summary. summary( polio.glm ) ## ## Call: ## glm(formula = cases ~ time, family = poisson(link = log), data = uspolio) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.626639 0.123641 5.068 4.02e-07 *** ## time -0.004263 0.001395 -3.055 0.00225 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 333.55 on 166 degrees of freedom ## AIC: 594.59 ## ## Number of Fisher Scoring iterations: 5 We can then plot the model as follows. plot(1970 + ((uspolio$time - 1)/12), uspolio$cases, type=&quot;h&quot;) lines(1970 + ((uspolio$time - 1)/12), polio.glm$fitted) We can see that this is perhaps unsatisfactory. We explore a linear trend with seasonal (annual) component. # Poisson model with linear trend and seasonal (annual) component polio1.glm&lt;- glm(cases~time + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)), family=poisson(link=log), data=uspolio) summary(polio1.glm) ## ## Call: ## glm(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * ## pi * time/12)), family = poisson(link = log), data = uspolio) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.606612 0.124800 4.861 1.17e-06 *** ## time -0.004644 0.001401 -3.315 0.000916 *** ## I(cos(2 * pi * time/12)) 0.181254 0.096160 1.885 0.059442 . ## I(sin(2 * pi * time/12)) -0.423187 0.097590 -4.336 1.45e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 310.72 on 164 degrees of freedom ## AIC: 575.77 ## ## Number of Fisher Scoring iterations: 5 plot(1970 + ((uspolio$time - 1)/12), uspolio$cases, type=&quot;h&quot;) lines(1970 + ((uspolio$time - 1)/12), polio1.glm$fitted,col=2) How about now with a six-monthly component. # Poisson model with linear trend and seasonal (annual + sixmonthly) component polio2.glm&lt;- glm(cases~time + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)), family=poisson(link=log), data=uspolio) summary(polio2.glm) ## ## Call: ## glm(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * ## pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * pi * ## time/6)), family = poisson(link = log), data = uspolio) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.557241 0.127303 4.377 1.20e-05 *** ## time -0.004799 0.001403 -3.421 0.000625 *** ## I(cos(2 * pi * time/12)) 0.137132 0.089479 1.533 0.125384 ## I(sin(2 * pi * time/12)) -0.534985 0.115476 -4.633 3.61e-06 *** ## I(cos(2 * pi * time/6)) 0.458797 0.101467 4.522 6.14e-06 *** ## I(sin(2 * pi * time/6)) -0.069627 0.098123 -0.710 0.477957 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 288.85 on 162 degrees of freedom ## AIC: 557.9 ## ## Number of Fisher Scoring iterations: 5 plot(1970 + ((uspolio$time - 1)/12), uspolio$cases, type=&quot;h&quot;) lines(1970 + ((uspolio$time - 1)/12), polio2.glm$fitted,col=3) Add in temperature data. # average annual temperature data over the 14 years. temp_data &lt;- rep(c(5.195, 5.138, 5.316, 5.242, 5.094, 5.108, 5.260, 5.153, 5.155, 5.231, 5.234, 5.142, 5.173, 5.167), each = 12 ) # scale the data so that it plots nicely. scaled_temp = 10 * (temp_data - min(temp_data))/(max(temp_data) - min(temp_data)) uspolio$temp = scaled_temp # Plot temperature data against cases data to see interest. plot(1970 + ((uspolio$time - 1)/12), uspolio$cases, type=&quot;h&quot;) lines(1970 + ((uspolio$time - 1)/12), uspolio$temp, col=&quot;red&quot;) Poisson GLM with temp data. # Construct GLM. polio3.glm&lt;- glm(cases~time + temp + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)) , family=poisson(link=log), data=uspolio) summary(polio.glm) ## ## Call: ## glm(formula = cases ~ time, family = poisson(link = log), data = uspolio) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.626639 0.123641 5.068 4.02e-07 *** ## time -0.004263 0.001395 -3.055 0.00225 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 333.55 on 166 degrees of freedom ## AIC: 594.59 ## ## Number of Fisher Scoring iterations: 5 plot(1970 + ((uspolio$time - 1)/12), uspolio$cases, type=&quot;h&quot;) lines(1970 + ((uspolio$time - 1)/12), polio3.glm$fitted, col=&quot;red&quot;) Compare to moving average window… # Compare to simple moving average # Size of averaging window. # Try m = 3, 6, 12, 60, 120 m = 12 MA = rep(0, length(uspolio$time)) for (time in uspolio$time) { times = time:min(time + m - 1, length(uspolio$time)) n = length(times) sum = 0 for (newtime in times) { sum = sum + uspolio$cases[newtime] } MA[time] = sum / m } plot(1970 + ((uspolio$time - 1)/12), MA, type = &quot;l&quot;) lines(1970 + ((uspolio$time - 1)/12), 0.2*uspolio$temp, col=&quot;red&quot;) 2.10 Estimation of \\(\\phi\\) There is no need to estimate the dispersion \\(\\phi\\) in order to estimate \\(\\boldsymbol{\\beta}\\), because \\(\\phi\\) cancels from the equation \\(\\boldsymbol{S}(\\hat{\\boldsymbol{\\beta}}) = 0\\). However, \\({\\mathrm{Var}}[\\hat{\\boldsymbol{\\beta}}]\\) does depend on \\(\\phi\\), as one might expect. If necessary, or of interest, \\(\\phi\\) can be estimated via: \\[\\begin{equation} \\hat{\\phi} = \\frac{1}{n - p} \\sum_{i} m_{i} \\frac{(y_{i} - \\hat{\\mu}_{i})^{2}}{\\mathcal{V}(\\hat{\\mu}_{i})} \\end{equation}\\] The motivation is that \\[\\begin{equation} {\\mathrm{Var}}[y_{i}] = {\\mathrm E}[((y_{i} - \\mu_{i})^{2})] = \\phi_{i}\\mathcal{V}(\\mu_{i}) = \\frac{\\phi}{m_{i}} \\mathcal{V}(\\mu_{i}) \\end{equation}\\] 2.10.1 Special Cases Two special cases are as follows: 2.10.1.1 Gaussian When \\(Y |\\boldsymbol{\\beta}, x \\sim {\\mathcal N}(\\mu, \\sigma^{2})\\), with \\(m = 1\\), we have \\[\\begin{equation} \\hat{\\phi} = \\frac{1}{n - p} \\sum_{i} (y_{i} - \\hat{\\mu}_{i})^{2} = \\hat{\\sigma}^{2} \\end{equation}\\] 2.10.1.2 Gamma When \\(Y |\\boldsymbol{\\beta}, x \\sim \\text{Gamma}(\\mu, \\sigma^{2})\\)6, we have \\[\\begin{equation} \\frac{1}{\\hat{\\nu}} = \\hat{\\phi} = \\frac{1}{n - p} \\sum_{i} m_{i} \\frac{(y_{i} - \\hat{\\mu}_{i})^{2}}{\\hat{\\mu}_{i}^{2}} \\end{equation}\\] 2.10.2 Practical Example: Dataset C: Hospital Data library(npmlreg) data(hosp) hosp.glm &lt;- glm(duration~age+temp1, data=hosp, family=Gamma(link=log)) summary(hosp.glm) ## ## Call: ## glm(formula = duration ~ age + temp1, family = Gamma(link = log), ## data = hosp) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.654096 16.621018 -1.724 0.0987 . ## age 0.014900 0.005698 2.615 0.0158 * ## temp1 0.306624 0.168141 1.824 0.0818 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.2690233) ## ## Null deviance: 8.1722 on 24 degrees of freedom ## Residual deviance: 5.7849 on 22 degrees of freedom ## AIC: 142.73 ## ## Number of Fisher Scoring iterations: 6 #(Dispersion parameter for Gamma family taken to be 0.2690233) # by hand: 1/(hosp.glm$df.res)*sum( (hosp$duration-hosp.glm$fitted)^2/(hosp.glm$fitted^2)) ## [1] 0.2690233 #[1] 0.2690233 2.11 Asymptotic Properties of \\(\\hat{\\boldsymbol{\\beta}}\\) In our context, asymptotic means that \\(M = \\sum_{i = 1}^{n} m_{i}\\rightarrow \\infty\\). This could be because \\(n\\rightarrow \\infty\\), or because the \\(m_{i}\\rightarrow\\infty\\), or a combination of both. Let us denote the true value of \\(\\boldsymbol{\\beta}\\) by \\(\\boldsymbol{\\beta}_{0}\\). In the following, we assume consistency of \\(\\hat{\\boldsymbol{\\beta}}\\), i.e. that \\(\\hat{\\boldsymbol{\\beta}}\\stackrel{p}{\\rightarrow}\\boldsymbol{\\beta}_{0}\\), meaning convergence in probability, the probability that \\(||\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0}|| \\geq \\varepsilon\\) tends to \\(0\\) as \\(n\\) tends to infinity. We will also denote this by \\(\\hat{\\boldsymbol{\\beta}}\\stackrel{a}{=} \\boldsymbol{\\beta}\\), and we will abuse notation by also using this to mean tends to asymptotically for expectations. Asymptotically, \\(\\hat{\\boldsymbol{\\beta}}\\) will thus be close to \\(\\boldsymbol{\\beta}_{0}\\), and we can expand \\(\\boldsymbol{S}\\) around it: \\[\\begin{align} \\boldsymbol{S}(\\hat{\\boldsymbol{\\beta}}) = 0 &amp; \\stackrel{a}{=} \\boldsymbol{S}(\\boldsymbol{\\beta}_{0}) + \\frac{\\partial \\boldsymbol{S}(\\boldsymbol{\\beta}_{0})}{\\partial \\boldsymbol{\\beta}^{T}}(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0}) \\\\ &amp; = \\boldsymbol{S}(\\boldsymbol{\\beta}_{0}) - \\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}_0) (\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0}) \\tag{2.17} \\end{align}\\] or equivalently \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0} = (\\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}_0))^{-1} \\boldsymbol{S}(\\boldsymbol{\\beta}_0) \\end{equation}\\] 2.11.1 Fisher Scoring In Section 2.8, we stated that we often use the (expected) Fisher Information in place of the Observed Fisher Information (known as Fisher Scoring). Doing so in the context of asymptotic arguments is acceptable. We can roughly see this as follows: \\[\\begin{equation} \\frac{1}{n}\\boldsymbol{F}_{\\text{obs}}(\\boldsymbol{\\beta}_0) = - \\frac{1}{n} \\frac{\\partial l}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} (\\boldsymbol{\\beta}_0) = - \\frac{1}{n} \\sum_{i-1}^n \\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} (\\boldsymbol{\\beta}_0) \\rightarrow - \\mathrm{E} \\left[\\frac{\\partial l_i}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^T} (\\boldsymbol{\\beta}_0) \\right] = F_1(\\boldsymbol{\\beta}) \\end{equation}\\] where \\(F_1(\\boldsymbol{\\beta})\\) is the expected Fisher Information for a sample of size \\(1\\), by the law of large numbers as \\(n \\rightarrow \\infty\\). It can be shown that \\(\\boldsymbol{F}(\\boldsymbol{\\beta}) = nF_1(\\boldsymbol{\\beta})\\)7, thus justifying use of \\(\\boldsymbol{F}(\\boldsymbol{\\beta})\\) in the forthcoming asymptotic arguments. 2.11.2 Expectation We have \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0} \\stackrel{a}{=} \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0})\\;\\boldsymbol{S}(\\boldsymbol{\\beta}_{0}) \\tag{2.18} \\end{equation}\\] Because convergence in probability implies convergence in distribution, this in turn implies that \\[\\begin{equation} E[\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0}] \\stackrel{a}{=} \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0})\\;E[\\boldsymbol{S}(\\boldsymbol{\\beta}_{0})] = 0 \\end{equation}\\] In other words, \\(\\hat{\\boldsymbol{\\beta}}\\) is asymptotically unbiased. 2.11.3 Variance Similarly, we have that \\[\\begin{align} {\\mathrm{Var}}[\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0}] &amp; = {\\mathrm E}[(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0})(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0})^{T}] \\\\ &amp; \\stackrel{a}{=} {\\mathrm E}[\\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0})\\;\\boldsymbol{S}(\\boldsymbol{\\beta}_{0})\\;\\boldsymbol{S}(\\boldsymbol{\\beta}_{0})^{T}\\; \\boldsymbol{F}^{-T}(\\boldsymbol{\\beta}_{0})] \\\\ &amp; = \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0})\\;{\\mathrm E}[\\boldsymbol{S}(\\boldsymbol{\\beta}_{0})\\;\\boldsymbol{S}(\\boldsymbol{\\beta}_{0})^{T}]\\; \\boldsymbol{F}^{-T}(\\boldsymbol{\\beta}_{0}) \\\\ &amp; = \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0})\\;{\\mathrm{Var}}[\\boldsymbol{S}(\\boldsymbol{\\beta}_{0})] \\; \\boldsymbol{F}^{-T}(\\boldsymbol{\\beta}_{0}) \\\\ &amp; = \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0}) \\end{align}\\] where we have used symmetry of \\(\\boldsymbol{F}\\). Thus \\[\\begin{equation} {\\mathrm{Var}}[\\hat{\\boldsymbol{\\beta}}] = {\\mathrm{Var}}[\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0}] \\stackrel{a}{=} \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0}) \\tag{2.19} \\end{equation}\\] Note that \\[\\begin{equation} \\boldsymbol{F}(\\boldsymbol{\\beta}) = \\mathrm{E} \\left[ -\\frac{\\partial^{2} l}{\\partial\\boldsymbol{\\beta}^{T}\\partial\\boldsymbol{\\beta}} \\right] \\end{equation}\\] so that the variance of \\(\\hat{\\boldsymbol{\\beta}}\\) can be seen as the inverse precision, or the `curvature’, of the log-likelihood function. Note that the greater the curvature, the more precise the inference about \\(\\boldsymbol{\\beta}\\). 2.11.4 Asymptotic Normality The following is a sketch of the argument of asymptotic normality. We start from \\[\\begin{equation} \\boldsymbol{S}(\\boldsymbol{\\beta}) = \\sum_{i} \\boldsymbol{S}_{i}(\\boldsymbol{\\beta}) \\end{equation}\\] which defines the \\(\\boldsymbol{S}_{i}\\). This is a sum of independent random variables, with zero mean and finite variance. As the number of terms in the sum tends to infinity, then under a certain condition, the distribution of the sum converges in distribution to a normal distribution: \\[\\begin{equation} \\boldsymbol{S}(\\boldsymbol{\\beta}) \\stackrel{a}{\\sim} {\\mathcal N}(0, \\boldsymbol{F}(\\boldsymbol{\\beta})) \\end{equation}\\] Hence \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0} \\stackrel{a}{=} \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0})\\;\\boldsymbol{S}(\\boldsymbol{\\beta}_{0}) \\stackrel{a}{\\sim} {\\mathcal N}(0, \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0})\\;\\boldsymbol{F}(\\boldsymbol{\\beta}_{0})\\;\\boldsymbol{F}^{-T}(\\boldsymbol{\\beta}_{0})) \\end{equation}\\] Convergence in probability implies convergence in distribution, so \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0} \\stackrel{a}{\\sim} {\\mathcal N}(0, \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}_{0})) \\tag{2.20} \\end{equation}\\] This also implies that the Mahalanobis distance between \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\boldsymbol{\\beta}_{0}\\) is asymptotically chi-square distributed: \\[\\begin{equation} (\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0})^{T}\\;\\boldsymbol{F}(\\boldsymbol{\\beta}_{0})\\;(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{\\beta}_{0}) \\stackrel{a}{\\sim} \\chi^{2}(p) \\tag{2.21} \\end{equation}\\] 2.11.5 Closing The Circle Under some regularity conditions, \\[\\begin{equation} \\boldsymbol{F}^{-1}(\\boldsymbol{\\beta}) = \\left( \\sum_{i} m_{i} \\ldots \\right)^{-1} \\rightarrow 0 \\end{equation}\\] as \\(M\\rightarrow\\infty\\). Thus \\(\\hat{\\boldsymbol{\\beta}}\\) converges in distribution to a constant random variable, which means that it converges in probability too, which is what we were assuming. Equations (2.19), (2.20), and (2.21) remain valid when \\(\\boldsymbol{F}(\\boldsymbol{\\beta}_{0})\\) is replaced by \\(\\boldsymbol{F}(\\hat{\\boldsymbol{\\beta}})\\). 2.11.6 Next Step Now that we have seen how to estimate the parameters, and some of their sampling properties (asymptotically), we can move on to how to use these estimates to make inferences: predictions about new values and confidence intervals. References "],["prediction_and_inference.html", "Chapter 3 Prediction and Inference 3.1 Prediction 3.2 Hypothesis Tests 3.3 Confidence Regions for \\(\\hat{\\boldsymbol{\\beta}}\\) 3.4 Issues with GLMs and the Wald Test", " Chapter 3 Prediction and Inference 3.1 Prediction Assume a GLM has been fitted, yielding \\(\\hat{\\boldsymbol{\\beta}}\\in{\\mathbb R}^{p}\\). If we are given a new predictor vector \\(\\boldsymbol{x}_{0}\\), we can compute \\[\\begin{equation} \\hat{\\eta}_{0} = \\hat{\\boldsymbol{\\beta}}^{T}\\boldsymbol{x}_{0} \\end{equation}\\] Now, \\[\\begin{equation} {\\mathrm{Var}}[\\hat{\\boldsymbol{\\beta}}] \\stackrel{a}{=} F^{-1}(\\hat{\\boldsymbol{\\beta}}) \\end{equation}\\] so \\[\\begin{equation} {\\mathrm{Var}}[\\hat{\\eta}_{0}] \\stackrel{a}{=} \\boldsymbol{x}_{0}^{T}\\;{\\mathrm{Var}}[\\hat{\\boldsymbol{\\beta}}] \\;\\boldsymbol{x}_{0} \\stackrel{a}{=} \\boldsymbol{x}_{0}^{T}\\;F^{-1}(\\hat{\\boldsymbol{\\beta}})\\;\\boldsymbol{x}_{0} \\end{equation}\\] or, alternatively, \\[\\begin{equation} \\text{SE}[\\hat{\\eta}_{0}] \\stackrel{a}{=} \\sqrt{x_{0}^{T}\\;F^{-1}(\\hat{\\boldsymbol{\\beta}})\\;x_{0}} \\end{equation}\\] A prediction for a new, unobserved \\(y_{0}\\) is then \\[\\begin{equation} \\hat{y}_{0} = {\\mathrm E}[Y |\\hat{\\boldsymbol{\\beta}}, x_{0}] = h(\\hat{\\boldsymbol{\\beta}}^{T}x_{0}) \\end{equation}\\] An approximate \\((1 - \\alpha)\\) confidence interval for \\({\\mathrm E}[Y |\\boldsymbol{\\beta}, x_{0}]\\) is then \\[\\begin{equation} CI = \\left[ h \\left( \\hat{\\boldsymbol{\\beta}}^{T} x_{0} - z_\\frac{\\alpha}{2} \\sqrt{x_{0}^{T}\\;F^{-1}(\\hat{\\boldsymbol{\\beta}})\\;x_{0}} \\right) \\;, \\; h \\left( \\hat{\\boldsymbol{\\beta}}^{T} x_{0} + z_\\frac{\\alpha}{2} \\sqrt{x_{0}^{T}\\;F^{-1}(\\hat{\\boldsymbol{\\beta}})\\;x_{0}} \\right) \\right] \\end{equation}\\] Note that this is not, in general, symmetric about \\(h(\\hat{\\boldsymbol{\\beta}}^{T}x_{0})\\). What about predictive intervals for \\(y_{0}\\), by analogy with the linear model case? These are more complicated, as they depend on the response distribution, and we do not consider them here. 3.1.1 Example: Dataset B Predict the duration of stay for a new individual with age \\(60\\), and temperature \\(99\\). We could try predict(hosp.glm, newdata=data.frame(age=60, temp1=99)) ## 1 ## 2.595732 But this is not what we want - for GLMs, the predict function gives by default the value of the linear predictor. To predict on the scale of the response, one needs exp(predict(hosp.glm, newdata=data.frame(age=60, temp1=99))) ## 1 ## 13.4064 or predict(hosp.glm, newdata=data.frame(age=60, temp1=99), type=&quot;response&quot;) ## 1 ## 13.4064 Similarly we aim to achieve a 95% confidence interval for the expected mean function at age 60 and temperature 99 as for the linear model as follows: # Attempt, as for lm: predict(hosp.glm, newdata=data.frame(age=60, temp1=99), type=&quot;response&quot;, interval=&quot;confidence&quot;) ## 1 ## 13.4064 This does not work, so we need to do it manually! # Compute the predicted linear predictor as above. lphat &lt;- predict(hosp.glm, newdata=data.frame(age=60, temp1=99)) # Extract the covariance. varhat &lt;- summary(hosp.glm)$cov.scaled # = F^(-1)(betahat) # Define new data point x0 = c(1, 60, 99) # Compute the width of the interval for the linear predictor. span &lt;- qnorm(0.975) * sqrt( x0 %*% varhat %*% x0) # Compute the interval for the mean. c(exp(lphat-span), exp(lphat+span)) ## [1] 8.836973 20.338601 # Note that this is quite large, as the dataset is small! 3.2 Hypothesis Tests We wish to test the values of \\(\\hat{\\boldsymbol{\\beta}}\\), just as for linear models. 3.2.1 Simple Tests We take as hypotheses \\(\\mathcal{H}_{0}: \\boldsymbol{\\beta} = \\boldsymbol{b}\\) and \\(\\mathcal{H}_{1}: \\boldsymbol{\\beta} \\neq \\boldsymbol{b}\\). 3.2.1.1 Wald Test An obvious candidate for a test statistic is the Mahalanobis distance of \\(\\hat{\\boldsymbol{\\beta}}\\) from \\(\\boldsymbol{\\beta}\\), otherwise know as the Wald statistic. Under \\(\\mathcal{H}_{0}\\), \\[\\begin{equation} W = (\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{b})^{T}\\;F(\\hat{\\boldsymbol{\\beta}})\\;(\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{b}) \\stackrel{a}{\\sim} \\chi^{2}(p) \\end{equation}\\] The test is then: Reject \\(\\mathcal{H}_{0}\\) at significance level \\(\\alpha\\) if \\(W &gt; \\chi^{2}_{p, \\alpha}\\). 3.2.1.2 Likelihood Ratio Test An alternative is a likelihood ratio test. Define \\[\\begin{equation} \\Lambda = 2\\log \\left( \\frac{L(\\hat{\\boldsymbol{\\beta}})}{L(\\boldsymbol{\\beta})} \\right) = 2(\\ell(\\hat{\\boldsymbol{\\beta}}) - \\ell(\\boldsymbol{\\beta})) \\end{equation}\\] What is the distribution of \\(\\Lambda\\)? Taylor-expanding \\(\\ell\\), we find \\[\\begin{equation} \\ell(\\boldsymbol{\\beta}) \\stackrel{a}{=} \\ell(\\hat{\\boldsymbol{\\beta}}) + (\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}})^{T}S(\\hat{\\boldsymbol{\\beta}}) - \\frac{1}{2} (\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}})^{T}\\;F(\\hat{\\boldsymbol{\\beta}})\\;(\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}}) \\end{equation}\\] But \\(S(\\hat{\\boldsymbol{\\beta}}) = 0\\), so we have \\[\\begin{equation} 2(\\ell(\\hat{\\boldsymbol{\\beta}}) - \\ell(\\boldsymbol{\\beta})) \\stackrel{a}{=} (\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}})^{T}\\;F(\\hat{\\boldsymbol{\\beta}})\\;(\\boldsymbol{\\beta} - \\hat{\\boldsymbol{\\beta}}) \\stackrel{a}{\\sim} \\chi^{2}(p) \\end{equation}\\] Under \\(\\mathcal{H}_{0}\\), \\(\\boldsymbol{\\beta} = \\boldsymbol{b}\\), so we have \\[\\begin{equation} \\Lambda = 2(\\ell(\\hat{\\boldsymbol{\\beta}}) - \\ell(\\boldsymbol{b})) \\stackrel{a}{\\sim} \\chi^{2}(p) \\end{equation}\\] We then Reject \\(\\mathcal{H}_{0}\\) at significance level \\(\\alpha\\) if \\(\\Lambda &gt; \\chi^{2}_{p, \\alpha}\\). 3.2.2 Generalisation to Nested Models Our hypotheses are now \\[\\begin{align} \\mathcal{H}_{0}&amp;: C\\boldsymbol{\\beta} = \\gamma \\\\ \\mathcal{H}_{1}&amp;: C\\boldsymbol{\\beta} \\neq \\gamma \\end{align}\\] where: \\(C\\in{\\mathbb R}^{s\\times p}\\); \\(\\dim(\\text{image}(C)) = s\\); \\(\\gamma\\in{\\mathbb R}^{s}\\). The equation \\(C\\boldsymbol{\\beta} = \\gamma\\) constrains the possible values of \\(\\boldsymbol{\\beta}\\), reducing the dimensionality of the space of possible solutions by \\(s\\). The set of \\(\\boldsymbol{\\beta}\\in{\\mathbb R}^{p}\\) satisfying \\(C\\boldsymbol{\\beta} = \\gamma\\) forms a \\((p - s)\\)-dimensional affine subspace of \\({\\mathbb R}^{p}\\). This therefore corresponds to a restricted or reduced model, as against \\(\\mathcal{H}_{1}\\), which corresponds to the full model. We may sometimes say that \\(\\mathcal{H}_{0}\\) is a submodel of \\(\\mathcal{H}_{1}\\) because the parameter space of \\(\\mathcal{H}_{0}\\) is a subset of the parameter space of \\(\\mathcal{H}_{1}\\). Figure 3.1: Illustration of the relationship between the Wald test and the likelihood ratio test. 3.2.2.1 Example Let \\[\\begin{align} C &amp; = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\end{pmatrix} \\in {\\mathbb R}^{2\\times p} \\\\ \\gamma &amp; = 0 \\in {\\mathbb R}^{2} \\end{align}\\] with \\(\\boldsymbol{\\beta} \\in{\\mathbb R}^{p}\\). Then \\[\\begin{equation} \\mathcal{H}_{0}: \\begin{cases} \\beta_{1} = 0 &amp; \\\\ \\beta_{2} = 0 &amp; \\end{cases} \\end{equation}\\] whereas \\(\\mathcal{H}_{1}\\) has \\(\\beta_{1}\\) and \\(\\beta_{2}\\) unrestricted. 3.2.2.2 Wald Test We have \\[\\begin{equation} W = (C\\boldsymbol{\\beta} - \\gamma)^{T}\\; \\left( C \\;F^{-1}(\\hat{\\boldsymbol{\\beta}})\\;C^{T} \\right)^{-1} \\; (C\\boldsymbol{\\beta} - \\gamma) \\stackrel{a}{\\sim} \\chi^{2}(s) \\end{equation}\\] where, recall, \\(s\\) is the number of constraints; or, equivalently, the difference in the number of parameters; or, more abstractly, the difference in the dimensions of the parameter spaces. 3.2.2.3 Likelihood Ratio Test We have \\[\\begin{equation} \\Lambda = 2(\\ell(\\hat{\\boldsymbol{\\beta}}) - \\ell(\\tilde{\\boldsymbol{\\beta}})) \\end{equation}\\] where \\(\\hat{\\boldsymbol{\\beta}}\\) is the MLE under \\(\\mathcal{H}_{1}\\), while \\(\\tilde{\\boldsymbol{\\beta}}\\) is the MLE under \\(\\mathcal{H}_{0}\\), that is, the MLE for the restricted model.8 3.2.3 Example: Dataset C Consider the hospital data. Construct a Gamma GLM with log link for the duration of hospital stay as a function of age and temp1, the temperature at admission, as follows \\[\\begin{equation} \\eta = \\beta_{1} + \\beta_{2}\\texttt{age} + \\beta_{3}\\texttt{temp1} \\end{equation}\\] where \\[\\begin{equation} \\texttt{duration} |\\texttt{age}, \\texttt{temp1} \\sim \\text{Gamma}(\\nu, \\nu e^{-\\eta}) \\end{equation}\\] which can be coded in R as follows: data(hosp, package=&quot;npmlreg&quot;) hosp.glm &lt;- glm(duration~age + temp1, data=hosp, family=Gamma(link=log)) Note that the parameterisation chosen means that, from properties of the Gamma distribution: \\[\\begin{align} \\textrm{E}[\\texttt{duration} |\\texttt{age}, \\texttt{temp1}] &amp; = {\\nu \\over \\nu e^{-\\eta}} = e^{\\eta} \\\\ \\textrm{Var}[\\texttt{duration} |\\texttt{age}, \\texttt{temp1}] &amp; = {\\nu \\over (\\nu e^{-\\eta})^{2}} = {e^{2\\eta} \\over \\nu} \\end{align}\\] The first equation says that we are using a log link, i.e. an exponential response; the second equation identifies \\(\\phi = \\tfrac{1}{\\nu}\\) and \\(\\mathcal{V}(\\mu) = \\mu^{2}\\). We now wish to test \\[\\begin{equation} \\mathcal{H}_{0}: \\beta_{3} = 0 \\end{equation}\\] against \\[\\begin{equation} \\mathcal{H}_{1}: \\beta_{3} \\neq 0 \\end{equation}\\] We note that \\[\\begin{equation} C = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\end{pmatrix} \\in {\\mathbb R}^{1 \\times 3} \\end{equation}\\] while \\(\\gamma = 0\\), so that the constraint equation can be written \\[\\begin{equation} \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} \\beta_{1} \\\\ \\beta_{2} \\\\ \\beta_{3} \\end{pmatrix} = 0 \\end{equation}\\] The variance we are looking for is \\[\\begin{equation} C\\;F^{-1}(\\hat{\\boldsymbol{\\beta}})\\;C^{T} = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\end{pmatrix} F^{-1}(\\hat{\\boldsymbol{\\beta}}) \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\text{Var}[\\hat{\\boldsymbol{\\beta}}_{3}] = 0.028 \\end{equation}\\] which can be obtained from R as follows: ( varhat &lt;- summary(hosp.glm)$cov.scaled ) ## (Intercept) age temp1 ## (Intercept) 276.25822713 -3.728838e-02 -2.7943778049 ## age -0.03728838 3.246846e-05 0.0003656812 ## temp1 -2.79437780 3.656812e-04 0.0282713219 The Wald statistic is then given by \\[\\begin{equation} W = {\\hat{\\boldsymbol{\\beta}}_{3}^{2} \\over \\textrm{Var}[\\hat{\\boldsymbol{\\beta}}_{3}]} = {(0.31)^{2} \\over 0.028} = 3.32 \\end{equation}\\] Since \\(\\chi^{2}_{1, 0.05} = 3.84\\) and \\(\\chi^{2}_{1, 0.1} = 2.71\\), we see that we do not reject \\(\\mathcal{H}_{0}\\) at the \\(5\\%\\) level, but do reject at the \\(10\\%\\) level. 3.2.3.1 Does R give what one expects? Notice that when \\(\\phi\\) has to be estimated as well as \\(\\boldsymbol{\\beta}\\), we have a situation similar to an unknown variance in the testing of means, going under the title `small sample t-tests’. For the example we are looking at, we have \\[\\begin{equation} \\sqrt{W} = {\\hat{\\boldsymbol{\\beta}}_{3} \\over \\text{SE}(\\hat{\\boldsymbol{\\beta}}_{3})} = {0.31 \\over 0.17} = 1.82 \\end{equation}\\] This is the same as the number in the `\\(t\\)-value’ column in the R summary: summary(hosp.glm) ## ## Call: ## glm(formula = duration ~ age + temp1, family = Gamma(link = log), ## data = hosp) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.654096 16.621018 -1.724 0.0987 . ## age 0.014900 0.005698 2.615 0.0158 * ## temp1 0.306624 0.168141 1.824 0.0818 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.2690233) ## ## Null deviance: 8.1722 on 24 degrees of freedom ## Residual deviance: 5.7849 on 22 degrees of freedom ## AIC: 142.73 ## ## Number of Fisher Scoring iterations: 6 However, if \\(W \\sim \\chi^{2}(1)\\), then \\(\\sqrt{W} \\sim {\\mathcal N}(0, 1)\\), leading to \\[\\begin{equation} p = 2(1 - \\Phi(1.82)) = 0.068 \\end{equation}\\] This number does not appear in the R summary. The explanation is that if \\(\\phi\\) is estimated, R uses \\(t_{n - p}\\) rather than \\({\\mathcal N}(0, 1)\\), leading to \\[\\begin{equation} p = 2(1 - \\Phi_{t}(1.82)) = 0.082 \\end{equation}\\] and this number does appear in the R summary. The use of the \\(t\\) distribution rather than the Gaussian distribution accounts for the extra variability introduced by estimating \\(\\phi\\). It still uses asymptotic normality as a foundation. In fact, R also allows one to assume the dispersion is known: summary(hosp.glm, dispersion= 0.2690233) ## ## Call: ## glm(formula = duration ~ age + temp1, family = Gamma(link = log), ## data = hosp) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -28.654096 16.621017 -1.724 0.08471 . ## age 0.014900 0.005698 2.615 0.00892 ** ## temp1 0.306624 0.168141 1.824 0.06821 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.2690233) ## ## Null deviance: 8.1722 on 24 degrees of freedom ## Residual deviance: 5.7849 on 22 degrees of freedom ## AIC: 142.73 ## ## Number of Fisher Scoring iterations: 6 The resulting summary talks of a \\(z\\)-value rather than a \\(t\\)-value, and computes the corresponding \\(p\\) using a Gaussian distribution. We will use \\(\\chi^{2}\\) tests exclusively, thereby ignoring the variability introduced by the estimation of \\(\\phi\\). 3.3 Confidence Regions for \\(\\hat{\\boldsymbol{\\beta}}\\) These follow from standard maximum likelihood theory. There are two popular types, which in general are not equivalent. 3.3.1 \\((1 - \\alpha)\\) Hessian CR This is: \\[\\begin{equation*} R^{H}_{1 - \\alpha} = \\left\\{\\boldsymbol{\\beta} :\\,(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^{T}F(\\hat{\\boldsymbol{\\beta}})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})) \\leq \\chi^{2}_{p,\\alpha}\\right\\} \\end{equation*}\\] 3.3.2 \\((1 - \\alpha)\\) method of support CR This is: \\[\\begin{equation*} R_{1 - \\alpha} = \\left\\{\\boldsymbol{\\beta} :\\,\\ell(\\boldsymbol{\\beta}) \\geq \\ell(\\hat{\\boldsymbol{\\beta}}) - \\frac{1}{2} \\chi^{2}_{p,\\alpha}\\right\\} \\end{equation*}\\] 3.4 Issues with GLMs and the Wald Test 3.4.1 Separation Consider a logistic regression problem with linear predictor: \\(\\eta = \\beta_{1} + \\beta_{2} x\\). Suppose that the data has the following property (not so unreasonable): the \\(x\\) values of all the points with \\(y = 0\\) are less than the \\(x\\) values of all the points with \\(y = 1\\). This is illustrated in Figure 3.2. x &lt;- runif( n = 21, min = 0, max = 4 ) y &lt;- as.numeric( x &gt; 1.8 ) plot( x, y, pch = 16 ) Figure 3.2: Illustration of `separated’ binary data. Problem What will be the estimated value \\(\\hat{\\beta}_{2}\\) of \\(\\beta_{2}\\)? This question seems unanswerable, but in fact it has a very simple answer. First, consider reparameterising the linear predictor. Define \\[\\begin{align} \\beta &amp; = \\beta_{2} \\\\ x_{0} &amp; = - \\frac{\\beta_{1}}{\\beta_{2}} \\end{align}\\] The linear predictor is thus: \\[\\begin{equation} \\eta = \\beta (x - x_{0}) \\end{equation}\\] The expression for the mean, that is, the probability that \\(y = 1\\) given \\(x\\), is then \\[\\begin{equation} \\pi(x) = \\frac{e^{\\beta (x - x_{0})}}{1 + e^{\\beta (x - x_{0})}} \\end{equation}\\] The estimation task is to pick values of \\(\\beta\\) and \\(x_{0}\\) that maximize the probability of the data. Clearly, if we can choose the parameters so that \\(\\pi(x) = 1\\) for those points with \\(y = 1\\) and \\(\\pi(x) = 0\\) for those points with \\(y = 0\\), we cannot do better: this is the maximum achievable with the model, equivalent to the saturated model in fact. Call this a perfect fit. Now consider the following. Pick \\(x_{0}\\) so that it lies between the \\(x\\) with \\(y = 0\\) and the \\(x\\) with \\(y = 1\\). This must be possible because of the initial assumption about the data. Now note that for all the \\(x\\) with \\(y = 0\\), \\((x - x_{0}) &lt; 0\\). If we let \\(\\beta\\rightarrow\\infty\\), then \\(\\pi(x)\\rightarrow 0\\). On the other hand, for all the \\(x\\) with \\(y = 1\\), \\((x - x_{0}) &gt; 0\\), so that as \\(\\beta\\rightarrow\\infty\\), \\(\\pi(x)\\rightarrow 1\\). The limiting solution is thus a step function with the step at \\(x_{0}\\). We can therefore achieve a perfect fit by allowing \\(\\beta\\rightarrow\\infty\\). Unfortunately, this means that the linear predictor is not defined, and, practically speaking, the estimation algorithm will not converge.9 Of course, in this simple case, we can see what is happening, and can anticipate that a step function might be a solution. In general, however, this situation might be hard to detect, and hard to correct, at least within the framework of GLMs. So what is to be done? Solution Remember that we set out to model functional relationships. GLMs are one way to do this, by constraining the form of the function in a useful way. In this case, however, they seem to be too limiting. There are two reasons why this might be the case. One is that the step function solution is appropriate for the data and context with which we are dealing. In this case, the main problem is that our set of functions is poorly parameterised, and includes the step function only as a singular limiting case. There is no real solution for this in the context of GLMs, although more general models could be used. The other is that the step function solution is not appropriate, and that we really would expect a smoother solution. This is much harder to deal with in the context of classical statistics. We are saying that we expect the value of \\(\\beta\\) to be finite, larger values becoming less and less probable, until in the limit, an infinite value is impossible. The only real way to deal with this situation is via a prior probability distribution on \\(\\beta\\) or by imposing some regularising constraint, but those are another story and another course. Within the GLM world, one has simply to be aware of the possibility of separation, and that it may be caused by overly subdividing the data via categorical variables, that is, essentially by overfitting. 3.4.2 Hauck-Donner Effect A related but independent effect was noted by Hauck and Donner (1976). Consider \\[\\begin{equation} W = \\frac{\\hat{\\boldsymbol{\\beta}}^{2} }{{\\mathrm{Var}}[\\hat{\\boldsymbol{\\beta}}]} \\end{equation}\\] If \\(\\hat{\\boldsymbol{\\beta}}\\rightarrow\\infty\\) (e.g. in cases of separation), then it is quite likely that \\({\\mathrm{Var}}[\\hat{\\boldsymbol{\\beta}}]\\rightarrow\\infty\\) also. The result can be that the test statistic becomes very small, and in fact tends to zero! Hauck and Donner showed that: Wald’s statistic decreased to zero as the distance between the parameter estimate and the null value increased. So, as one’s null hypothesis gets more and more wrong, the Wald statistic gets smaller and smaller, and one is decreasingly able to reject the increasingly wrong null. 3.4.3 Next Step We have seen how to set up and describe GLMs, and how to estimate their parameters. We have also seen how to use these parameters to make predictions and generate confidence intervals. We now move on to study how we can evaluate the effectiveness of our models. References "],["deviance_and_diagnostics.html", "Chapter 4 Deviance 4.1 Goodness-of-Fit 4.2 Asymptotic Properties 4.3 Pearson Statistic 4.4 Residuals and Diagnostics 4.5 Analysis of Deviance", " Chapter 4 Deviance 4.1 Goodness-of-Fit We would like to find a measure for goodness-of-fit, or, to put it another way, a measure for the discrepancy between the data \\(\\boldsymbol{y}\\in{\\mathbb R}^{n}\\) and the fit \\(\\hat{\\mu}\\in{\\mathbb R}^{n}\\), where \\(\\hat{\\mu}_{i} = h(\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_i)\\). First we need to understand how well any GLM could be expected to fit. 4.1.1 Saturated Model The log likelihood at the MLE \\(\\hat{\\boldsymbol{\\beta}}\\) is \\[\\begin{equation} \\ell(\\hat{\\boldsymbol{\\beta}}) = \\sum_{i} \\left( \\frac{y_{i}\\hat{\\theta}_{i} - b(\\hat{\\theta}_{i})}{\\phi_{i}} + c(y_{i}, \\phi_{i}) \\right) \\tag{4.1} \\end{equation}\\] The larger \\(\\ell(\\hat{\\boldsymbol{\\beta}})\\), the better the fit, but what is large? Consider the following. In the GLM, \\(\\mu\\), or equivalently \\(\\theta\\), takes values in \\({\\mathbb R}^{n}\\). However, \\(\\mu_{i} = h(\\boldsymbol{\\beta}^{T}\\boldsymbol{x}_i)\\). Thus, as \\(\\boldsymbol{\\beta}\\in{\\mathbb R}^{p}\\) varies, \\(\\mu = \\left\\{\\mu_{i}\\right\\}\\) can only trace out a \\(p\\)-dimensional submanifold of \\({\\mathbb R}^{n}\\): the possible values are constrained by the model structure. (Indeed, as we saw at the beginning, this is the whole point of the model in the first place.) An upper bound for \\(\\ell(\\hat{\\boldsymbol{\\beta}})\\) would therefore be attained by a model that placed less constraints on \\(\\mu\\) (since maximization over a superset necessarily produces a larger value). This can be achieved by simply allowing \\(\\mu\\) to range over all of \\({\\mathbb R}^{n}\\), or in other words by allowing as many parameters as there are data points. This means intuitively that we end up `joining the dots’. The maximum likelihood problem then breaks down into \\(n\\) simpler problems, as each term in equation (4.1) can be maximised separately. Differentiation with respect to \\(\\theta_{i}\\) then gives \\[\\begin{equation} \\ell_{i}&#39;(\\theta_{i}) = \\frac{y_{i} - b&#39;(\\theta_{i})}{\\phi_{i}} \\end{equation}\\] leading to the MLE \\(\\hat{\\theta}\\), or equivalently, \\(\\hat{\\mu}\\), given by \\[\\begin{equation} y_{i} = b&#39;(\\hat{\\theta}_{i}) = \\hat{\\mu}_{i} \\end{equation}\\] This model, in which \\(\\mu\\) may vary over the whole of \\({\\mathbb R}^{n}\\), and there is thus one parameter for each data point, is known as the saturated model. Its log likelihood at the MLE value \\(\\hat{\\mu}_{\\text{sat}}\\) is denoted \\(\\ell_{\\text{sat}}\\). This then leads us to the notion of deviance. 4.1.2 Deviance The deviance of a GLM is defined as follows: \\[\\begin{equation} D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}}) = 2\\;\\phi\\; \\left( \\ell_{\\text{sat}} - \\ell(\\hat{\\boldsymbol{\\beta}}) \\right) \\end{equation}\\] while the scaled deviance is defined as \\[\\begin{equation} D_{\\text{sc}}(\\boldsymbol{Y}, \\hat{\\mu}) = 2\\; \\left( \\ell_{\\text{sat}} - \\ell(\\hat{\\boldsymbol{\\beta}}) \\right) \\end{equation}\\] Now, \\[\\begin{equation} \\ell(\\hat{\\boldsymbol{\\beta}}) = \\frac{1}{\\phi} \\sum_{i} m_{i}\\; \\left( y_{i}\\hat\\theta_{i} - b(\\hat\\theta_{i}) \\right) + \\sum_{i} c(y_{i}, \\phi_{i}) \\end{equation}\\] with \\(\\hat{\\theta}_{i} = (b&#39;)^{-1}(\\hat{\\mu}_{i})\\), and \\(\\phi_{i} = \\frac{\\phi}{m_{i}}\\); and \\[\\begin{equation} \\ell_{\\text{sat}} = \\frac{1}{\\phi} \\sum_{i} m_{i}\\; \\left( y_{i}\\hat\\theta_{\\text{sat},i} - b(\\hat{\\theta}_{\\text{sat},i}) \\right) + \\sum_{i} c(y_{i}, \\phi_{i}) \\end{equation}\\] with \\(\\hat{\\theta}_{\\text{sat},i} = (b&#39;)^{-1}(y_{i})\\), that is, \\(\\hat{\\mu}_{\\text{sat},i} = y_{i}\\). We thus have that \\[\\begin{equation} D(\\boldsymbol{Y}, \\hat{\\mu}) = 2 \\sum_{i} m_{i} \\left\\{ y_{i} \\left( \\hat\\theta_{\\text{sat},i} - \\hat\\theta_{i} \\right) - \\left( b(\\hat\\theta_{\\text{sat},i}) - b(\\hat\\theta_{i}) \\right) \\right\\} \\end{equation}\\] The deviance is thus independent of \\(\\phi\\). 4.1.3 Example Special Cases 4.1.3.1 Gaussian We have \\(b(\\theta) = \\frac{1}{2}\\theta^{2}\\) \\(\\theta = (b&#39;)^{-1}(\\mu) = \\mu\\) We thus find that \\[\\begin{align} D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}}) &amp; = 2 \\sum_{i} \\left( y_{i}(y_{i} - \\hat{\\boldsymbol{\\mu}}_{i}) - \\left( \\frac{1}{2} y_{i}^{2} - \\frac{1}{2} \\hat{\\boldsymbol{\\mu}}_{i}^{2} \\right) \\right) \\\\ &amp; = 2 \\sum_{i} \\left( \\frac{1}{2} y_{i}^{2} - y_{i}\\hat{\\boldsymbol{\\mu}}_{i} + \\frac{1}{2} \\hat{\\boldsymbol{\\mu}}_{i}^{2} \\right) \\\\ &amp; = \\sum_{i} (y_{i} - \\hat{\\boldsymbol{\\mu}}_{i})^{2} \\end{align}\\] But this is just RSS! 4.1.3.2 Poisson We have \\(b(\\theta) = e^{\\theta}\\) \\(\\theta = (b&#39;)^{-1}(\\mu) = \\log \\mu\\). We thus find that \\[\\begin{align} D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}}) &amp; = 2 \\sum_{i} \\left( y_{i}(\\log y_{i} - \\log \\hat{\\boldsymbol{\\mu}}_{i}) - (y_{i} - \\hat{\\boldsymbol{\\mu}}_{i}) \\right) \\\\ &amp; = 2 \\sum_{i} \\left( y_{i} \\log \\left( \\frac{y_{i}}{\\hat{\\boldsymbol{\\mu}}_{i}} \\right) - (y_{i} - \\hat{\\boldsymbol{\\mu}}_{i}) \\right) \\end{align}\\] 4.1.3.3 Bernoulli We have \\(b(\\theta) = \\log(1 + e^{\\theta})\\) \\(\\mu = \\frac{e^{\\theta}}{1 + e^{\\theta}}\\). \\(\\theta = \\log \\frac{\\mu}{1-\\mu}\\) However, there is a problem. We have \\[\\begin{equation} \\hat\\theta_{\\text{sat},i} = \\log \\left( \\frac{y_{i}}{1 - y_{i}} \\right) \\end{equation}\\] for \\(y_{i}\\in\\left\\{0, 1\\right\\}\\): the MLE \\(\\hat\\theta_{\\text{sat}}\\) is apparently not defined. However, this is easily solved. It is easiest to see if we write the maximum likelihood in terms of \\(\\hat{\\boldsymbol{\\mu}}= \\hat\\pi\\): \\[\\begin{equation} \\ell(\\hat\\pi) = \\sum_{i} y_{i}\\log\\hat\\pi_{i} + (1 - y_{i})\\log(1 - \\hat\\pi_{i}) \\end{equation}\\] The saturated log likelihood is therefore \\[\\begin{equation} \\ell(\\hat\\pi) = \\sum_{i} y_{i}\\log y_{i} + (1 - y_{i})\\log(1 - y_{i}) = 0 \\end{equation}\\] for \\(y_{i}\\in \\left\\{0, 1\\right\\}\\) by continuity. We thus have \\[\\begin{align} D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}}) &amp; = -2\\;\\ell(\\hat{\\boldsymbol{\\beta}}) \\\\ &amp; = - 2 \\sum_{i} y_{i}\\log\\hat\\pi_{i} + (1 - y_{i})\\log(1 - \\hat\\pi_{i}) \\\\ &amp; = - 2 \\left( \\sum_{i:\\,y_{i} = 0} \\log(1 - \\hat\\pi_{i}) + \\sum_{i:\\,y_{i} = 1} \\log\\hat\\pi_{i} \\right) \\end{align}\\] 4.2 Asymptotic Properties In order to use deviance effectively as a measure of goodness-of-fit, we need to be able to analyse its probabilistic behaviour, in order to perform tests, etc. Does deviance have, at least asymptotically, a nice distribution that we can use? Looking at the form of the deviance. \\[\\begin{equation} \\frac{D( \\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}})}{\\phi} = 2 \\;( \\ell_{\\text{sat}} - \\ell(\\hat{\\boldsymbol{\\beta}}) ) \\end{equation}\\] one might suppose, that to be analogous with the quantities used in likelihood ratio tests, it would be \\(\\chi^{2}(n - p)\\)-distributed asymptotically, since the saturated model has \\(n\\) parameters, and the model in which we are interested has \\(p\\). If this were the case, then we could say that if \\[\\begin{equation} \\frac{D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}})}{\\phi} &gt; \\chi^{2}_{p, \\alpha} \\tag{4.2} \\end{equation}\\] then the model does not fit well. Unfortunately, it is not true that \\(\\frac{D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}})}{\\phi}\\) is asymptotically \\(\\chi^{2}\\)-distributed in general. This is because the limit theorems that give the \\(\\chi^{2}\\) distribution do not apply when the number of parameters varies as the amount of data increases. Here that is the case, as the dimensionality of the saturated model is not fixed, but \\(n\\). In special cases, most notably for the Poisson distribution, or when the \\(m_{i} \\gg 1\\), the asymptotics do hold, and we can use Equation (4.2) as a test of goodness-of-fit. In general, however, this is not the case. Thus, as promising as deviance appears, it cannot serve as a complete replacement for RSS, even though this is a special case. We will see, however, that deviance is still extremely useful.10 4.3 Pearson Statistic We now take a slight detour to discuss an alternative measure of goodness-of-fit. This bears the same relationship to deviance that the Wald test bears to the likelihood ratio test: one works in the domain of the probability distribution; and one in its codomain, or in other words, in terms of probability itself. The Pearson statistic is defined as \\[\\begin{equation} \\chi^{2}_{P} = \\sum_{i} m_{i} \\frac{(y_{i} - \\hat{\\boldsymbol{\\mu}}_{i})^{2}}{\\mathcal{V}(\\hat{\\boldsymbol{\\mu}}_{i})} \\end{equation}\\] We then see that \\[\\begin{align} \\frac{\\chi^{2}_{P}}{\\phi} &amp; = \\sum_{i} \\frac{(y_{i} - \\hat{\\boldsymbol{\\mu}}_{i})^{2}}{\\frac{\\phi}{m_{i}}\\mathcal{V}(\\hat{\\boldsymbol{\\mu}}_{i})} \\\\ &amp; = \\sum_{i} \\frac{(y_{i} - \\hat{\\boldsymbol{\\mu}}_{i})^{2}}{\\widehat{\\text{Var}[y_{i}]}} \\\\ &amp; \\stackrel{a}{\\sim} \\chi^{2}(n - p) \\end{align}\\] Hence \\[\\begin{equation} \\chi^{2}_{P} \\stackrel{a}{\\sim} \\phi \\;\\chi^{2}(n - p) \\end{equation}\\] Thus \\(\\chi^{2}_{P}\\) can be used to measure goodness-of-fit. 4.3.1 Relation to Deviance Consider \\(D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}})\\) for the Poisson model: \\[\\begin{equation} D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}}) = 2 \\sum_{i} y_{i} \\log \\frac{y_{i}}{\\hat{\\boldsymbol{\\mu}}_{i}} - (y_{i} - \\hat{\\boldsymbol{\\mu}}_{i}) \\end{equation}\\] Expanding this as a function of \\(\\boldsymbol{y}\\) around \\(\\hat{\\boldsymbol{\\mu}}\\), we find \\[\\begin{equation} D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}}) \\simeq \\sum_{i} \\frac{(y_{i} - \\hat{\\boldsymbol{\\mu}}_{i})^{2}}{\\hat{\\boldsymbol{\\mu}}_{i}} \\end{equation}\\] This is just the normal Pearson statistic for the Poisson distribution. 4.3.2 Pearson Residuals We will see soon that we can define several types of residual for GLMs. One is defined based on the Pearson statistic, the `Pearson residual’: \\[\\begin{equation} r^{P}_{i} = \\sqrt{m_{i}} \\frac{y_{i} - \\hat{\\boldsymbol{\\mu}}_{i} }{ \\sqrt{\\mathcal{V}(\\hat{\\boldsymbol{\\mu}}_{i})}} = \\sqrt{\\hat\\phi} \\frac{y_{i} - \\hat{\\boldsymbol{\\mu}}_{i} }{ \\sqrt{\\widehat{\\text{Var}[y_{i}]}}} \\end{equation}\\] If \\(y_{i} \\sim {\\mathcal N}(\\mu_{i}, \\sigma^{2})\\), with \\(m_{i} = 1\\), then \\(\\mathcal{V}\\equiv 1\\), so that \\[\\begin{equation} r^{P}_{i} = y_{i} - \\hat{\\boldsymbol{\\mu}}_{i} = \\epsilon_{i} \\end{equation}\\] Thus in a linear model, the Pearson residuals are just the `usual’ residuals. 4.3.3 Example: Dataset B The example concerns a Poisson model, so we can use deviance as a goodness-of-fit measure. The R code for this example is presented here: # First, load the data. library( &quot;gamlss.data&quot; ) data( &quot;polio&quot; ) # GLM polio2.glm&lt;- glm(cases~time + I(cos(2*pi*time/12))+I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)) , family=poisson(link=log), data=uspolio) summary(polio2.glm) ## ## Call: ## glm(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * ## pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * pi * ## time/6)), family = poisson(link = log), data = uspolio) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.557241 0.127303 4.377 1.20e-05 *** ## time -0.004799 0.001403 -3.421 0.000625 *** ## I(cos(2 * pi * time/12)) 0.137132 0.089479 1.533 0.125384 ## I(sin(2 * pi * time/12)) -0.534985 0.115476 -4.633 3.61e-06 *** ## I(cos(2 * pi * time/6)) 0.458797 0.101467 4.522 6.14e-06 *** ## I(sin(2 * pi * time/6)) -0.069627 0.098123 -0.710 0.477957 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 288.85 on 162 degrees of freedom ## AIC: 557.9 ## ## Number of Fisher Scoring iterations: 5 # Deviance polio2.glm$dev ## [1] 288.8549 #[1] 292.2668 # Pearson-Statistic sum((uspolio$cases-polio2.glm$fitted)^2/ polio2.glm$fitted) ## [1] 318.7216 #[1] 325.1807 # Either way, critical value at 5% level qchisq(0.95,162) ## [1] 192.7001 #[1] 192.7001 The deviance is \\(292.3\\), while the Pearson statistic is \\(325.2\\). We have that \\(\\chi^{2}_{162, 0.05} = 192.7\\), so either way we reject \\(\\mathcal{H}_{0}\\), which is that the model is adequate, at \\(5\\%\\). (The test is of a model with \\(6\\) parameters against a model with \\(168\\) parameters, hence the \\(\\chi^{2}\\) distribution has \\(162\\) degrees of freedom.) 4.4 Residuals and Diagnostics Just as there were two types of hypothesis test, and two measures of goodness-of-fit, there are two types of residual typically used for GLMs. These are as follows: \\[\\begin{align*} \\textrm{Deviance residuals} &amp; \\qquad &amp; \\textrm{Pearson residuals} \\\\ D = \\sum_{i} d_{i} &amp; \\qquad &amp; \\chi^{2}_{P} = \\sum_{i} m_{i}\\frac{(y_{i} - \\hat{\\boldsymbol{\\mu}}_{i})^{2} }{ \\mathcal{V}(\\hat{\\boldsymbol{\\mu}}_{i})} \\\\ r^{D}_{i} = \\textrm{sign}(y_{i} - \\hat{\\boldsymbol{\\mu}}_{i})\\sqrt{d_{i}} &amp; &amp; r^{P}_{i} = \\sqrt{m_{i}}\\frac{y_{i} - \\hat{\\boldsymbol{\\mu}}_{i} }{ \\sqrt{\\mathcal{V}(\\hat{\\boldsymbol{\\mu}}_{i})}} \\end{align*}\\] Just as in a linear model, the \\(r^{D}_{i}\\) or \\(r^{P}_{i}\\) can be plotted against \\(i\\) or against individual predictors, to detect violations of model assumptions. There is a problem though: neither \\(r^{D}_{i}\\) nor \\(r^{P}_{i}\\) is Gaussian. This makes `knowing what to look for’ in such plots somewhat tricky. As a result, many modifications and transformations have been suggested: ‘adjusted deviance residuals’, ‘Anscombe residuals’, etc. We will not study these, but content ourselves with checking plots for suspicious looking patterns. 4.4.1 Example: Dataset C data(hosp, package=&quot;npmlreg&quot;) hosp.glm &lt;- glm(duration~age + temp1, data=hosp, family=Gamma(link=log)) par(mfrow=c(2,2)) plot(residuals(hosp.glm, type=&quot;deviance&quot;)) plot(hosp.glm$fitted, residuals(hosp.glm, type=&quot;pearson&quot;)) plot(hosp$age, residuals(hosp.glm, type=&quot;deviance&quot;)) plot(hosp$temp1, residuals(hosp.glm, type=&quot;deviance&quot;)) There are no obvious patterns here, but the sample size is quite small, which makes it more difficult. 4.4.2 Example: Dataset B polio2.glm&lt;- glm(cases~time + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)), family=poisson(link=log), data=uspolio) polio3.glm&lt;- glm(cases~time + temp + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)), family=poisson(link=log), data=uspolio) par(mfrow=c(2,2)) plot(uspolio$time, uspolio$cases, type=&quot;h&quot;) lines(uspolio$time, polio2.glm$fitted, col=&quot;red&quot;) plot(uspolio$time, residuals(polio2.glm, type=&quot;deviance&quot;), type=&quot;b&quot;) abline(a=0,b=0) plot(uspolio$time, uspolio$cases, type=&quot;h&quot;) lines(uspolio$time, polio3.glm$fitted, col=&quot;red&quot;) plot(uspolio$time, residuals(polio3.glm, type=&quot;deviance&quot;), type=&quot;b&quot;) abline(a=0,b=0) Here there is clearly residual autocorrelation present, so that the independence of different \\(y_{i}\\) is violated. 4.5 Analysis of Deviance The analysis of deviance is based on comparing the deviance of a model, not with the `perfect’ saturated model (which in practice is not perfect, since it overfits), but with the deviance of other competing models. These differences of deviances are much more useful in practice than the deviance itself. Consider two nested GLMs \\(\\tilde{\\mathcal{M}}\\subset \\mathcal{M}\\),11 e.g. \\(\\tilde{\\mathcal{M}}\\) with \\(g(\\mu) = \\boldsymbol{\\beta}_{1}\\) and \\(\\mathcal{M}\\) with \\(g(\\mu) = \\boldsymbol{\\beta}^{T}\\boldsymbol{x}\\), with \\(\\boldsymbol{\\beta}\\in{\\mathbb R}{p}\\). More generally: Let \\(\\mathcal{M}\\) be a GLM, the `full’ model; Let \\(\\tilde{\\mathcal{M}}\\) be a GLM nested in \\(\\mathcal{M}\\), the `reduced’ model, with \\[\\begin{equation} C\\boldsymbol{\\beta} = \\gamma \\end{equation}\\] with \\(C \\in {\\mathbb R}^{s\\times p}\\). Let \\(\\hat{\\boldsymbol{\\beta}}\\) be the MLE under \\(\\mathcal{M}\\); Let \\(\\tilde\\beta\\) be the MLE under \\(\\tilde{\\mathcal{M}}\\). Then we define \\[\\begin{align} D(\\tilde{\\mathcal{M}}, \\mathcal{M}) &amp; = D(\\tilde{\\mathcal{M}}) - D(\\mathcal{M}) \\\\ &amp; = 2\\;\\phi\\; \\big( \\ell_{\\text{sat}} - \\ell(\\tilde\\beta) \\bigr) - 2\\;\\phi\\; \\bigl( \\ell_{\\text{sat}} - \\ell(\\hat{\\boldsymbol{\\beta}}) \\bigr) \\\\ &amp; = 2\\;\\phi\\; \\bigl( \\ell(\\hat{\\boldsymbol{\\beta}}) - \\ell(\\tilde\\beta) \\bigr) \\end{align}\\] Note that \\[\\begin{equation} \\frac{1 }{ \\phi} D(\\tilde{\\mathcal{M}}, \\mathcal{M}) = 2\\bigl(\\ell(\\hat{\\boldsymbol{\\beta}}) - \\ell(\\tilde\\beta)\\bigr) \\end{equation}\\] This is just the likelihood ratio statistics, and thus \\[\\begin{equation} \\frac{1 }{ \\phi} D(\\tilde{\\mathcal{M}}, \\mathcal{M}) \\stackrel{a}{\\sim} \\chi^{2}(s) \\end{equation}\\] where \\(s\\) is the number of constraints, that is, the difference in the dimensions of the parameter spaces, or the difference in the number of parameters. From the definition of \\(D(\\tilde{\\mathcal{M}}, \\mathcal{M})\\), we have that \\[\\begin{equation} D(\\tilde{\\mathcal{M}}) = D(\\tilde{\\mathcal{M}}, \\mathcal{M}) + D(\\mathcal{M}) \\end{equation}\\] In words, `the discrepancy between the data and \\(\\tilde{\\mathcal{M}}\\) is equal to the discrepancy between the data and \\(\\mathcal{M}\\) plus the discrepancy between \\(\\mathcal{M}\\) and \\(\\tilde{\\mathcal{M}}\\)’. 4.5.1 Interpretation and Testing Consider applying this idea in the linear model case. There we have the following: \\[\\begin{equation} \\sum_{i}(y_{i} - \\tilde\\beta^{T}\\boldsymbol{x}_i)^{2} = D(\\tilde{\\mathcal{M}}, \\mathcal{M}) + \\sum_{i}(y_{i} - \\hat{\\boldsymbol{\\beta}}^{T}\\boldsymbol{x}_i)^{2} \\end{equation}\\] The left-hand side is the RSS of the reduced model, while the second term on the right-hand side is the RSS of the full model. In this context, we know we have the partial \\(F\\)-test, based on the statistic: \\[\\begin{equation} F = \\frac{ \\bigl( \\text{RSS reduced} - \\text{RSS full} \\bigr) / s }{ \\bigl( \\text{RSS full} / (n - p) \\bigr)} \\sim F(s, n - p) \\end{equation}\\] We then apply this as follows: if \\(F &gt; F_{s, n - p, \\alpha}\\), we reject \\(\\mathcal{H}_{0}: \\tilde{\\mathcal{M}}\\) in favour of \\(\\mathcal{H}_{1}: \\mathcal{M}\\) at level \\(\\alpha\\). How should we adapt this to the GLM case? By strict analogy, we have \\[\\begin{equation} F = \\frac{D(\\tilde{\\mathcal{M}}, \\mathcal{M}) / s }{ \\hat\\phi} = \\frac{1}{s} \\frac{D(\\tilde{\\mathcal{M}}, \\mathcal{M}) }{ \\hat\\phi} \\sim \\frac{1}{s} \\chi^{2}(s) \\end{equation}\\] where the latter, distributional result is true if we know \\(\\phi\\), or if we simply ignore the extra variability introduced by estimating it. In practice, we compute \\(s F = \\frac{D(\\tilde{\\mathcal{M}}, \\mathcal{M})}{\\hat\\phi}\\) and reject \\(\\mathcal{H}_{0}: \\tilde{\\mathcal{M}}\\) if \\(\\frac{D(\\tilde{\\mathcal{M}}, \\mathcal{M})}{\\hat\\phi} &gt; \\chi^{2}_{s, \\alpha}\\). 4.5.2 General Case More generally we may have a series of nest models: \\(\\mathcal{M}_{1}\\subset \\mathcal{M}_{2}\\subset \\dotsb \\subset \\mathcal{M}_{N}\\), that is, \\(\\mathcal{M}_{i}\\subset \\mathcal{M}_{i + 1}\\) for \\(i\\in [1.. (N - 1)]\\). We can then write down the following telescoping sum: \\[\\begin{align} D(\\mathcal{M}_{1}) &amp; = \\sum_{i = 1}^{N - 1} D(\\mathcal{M}_{i}, \\mathcal{M}_{i + 1}) + D(\\mathcal{M}_{N}) \\\\ &amp; = \\sum_{i = 1}^{N - 1} \\bigl( D(\\mathcal{M}_{i}) - D(\\mathcal{M}_{i + 1}) \\bigr) + D(\\mathcal{M}_{N}) \\\\ &amp; = \\sum_{i = 1}^{N - 1} D(\\mathcal{M}_{i}) - \\sum_{i = 2}^{N} D(\\mathcal{M}_{i}) + D(\\mathcal{M}_{N}) \\\\ &amp; = D(\\mathcal{M}_{1}) - D(\\mathcal{M}_{N}) + D(\\mathcal{M}_{N}) \\\\ &amp; = D(\\mathcal{M}_{1}) \\end{align}\\] A tabular representation of this sum is produced in R when the anova command is applies to a fitted GLM, as the next example demonstrates. 4.5.3 Example: Dataset C Here analysis of deviance is applied to the full model for the hospital data, with linear predictor: \\[\\begin{equation} \\eta = \\beta_{1} + \\beta_{2}\\texttt{age} + \\beta_{3}\\texttt{temp1} + \\beta_{4}\\texttt{wbc1} + \\beta_{5}\\texttt{antib} + \\beta_{6}\\texttt{bact} + \\beta_{7}\\texttt{serv} \\end{equation}\\] as shown below: data(hosp, package=&quot;npmlreg&quot;) # Full model fit1&lt;- glm(duration~age+temp1+wbc1+antib+bact+serv, data=hosp, family=Gamma(link=log)) fit1 ## ## Call: glm(formula = duration ~ age + temp1 + wbc1 + antib + bact + ## serv, family = Gamma(link = log), data = hosp) ## ## Coefficients: ## (Intercept) age temp1 wbc1 antib bact ## -18.925401 0.010026 0.219006 0.001654 -0.346060 0.075859 ## serv ## -0.291875 ## ## Degrees of Freedom: 24 Total (i.e. Null); 18 Residual ## Null Deviance: 8.172 ## Residual Deviance: 5.12 AIC: 147.6 summary(fit1)$dispersion ## [1] 0.2661922 anova(fit1) ## Analysis of Deviance Table ## ## Model: Gamma, link: log ## ## Response: duration ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 24 8.1722 ## age 1 1.38428 23 6.7879 ## temp1 1 1.00299 22 5.7849 ## wbc1 1 0.03236 21 5.7526 ## antib 1 0.31246 20 5.4401 ## bact 1 0.00017 19 5.4400 ## serv 1 0.31995 18 5.1200 # Df Deviance Resid. Df Resid. Dev # NULL 24 8.1722 # age 1 1.38428 23 6.7879 # temp1 1 1.00299 22 5.7849 # wbc1 1 0.03236 21 5.7526 # antib 1 0.31246 20 5.4401 # bact 1 0.00017 19 5.4400 # serv 1 0.31995 18 5.1200 # test problem a) 1-pchisq( (8.1722-5.1200)/0.2661922,6) ## [1] 0.07499453 # test problem b) 1-pchisq(1.00299/0.2662922 ,1) ## [1] 0.05228889 # test problem c) 1-pchisq((5.7849-5.12)/0.2662922 ,4) ## [1] 0.6451943 anova(fit1, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: Gamma, link: log ## ## Response: duration ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 24 8.1722 ## age 1 1.38428 23 6.7879 0.02258 * ## temp1 1 1.00299 22 5.7849 0.05224 . ## wbc1 1 0.03236 21 5.7526 0.72735 ## antib 1 0.31246 20 5.4401 0.27862 ## bact 1 0.00017 19 5.4400 0.97990 ## serv 1 0.31995 18 5.1200 0.27293 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model is a Gamma model with log link. The resulting anova table has the significance shown in the table in Figure 4.1. Figure 4.1: R output from an anova command on a GLM. Each row represents the model containing the predictors in that row and all the previous rows. From the definition of \\(D(\\tilde{\\mathcal{M}}, \\mathcal{M})\\), in each row, the sum of the and the \\(\\texttt{deviance}\\) gives the in the row above. If the command is run with the argument , then there will be an extra column in the table. This represents the \\(p\\) value of a \\(\\chi^{2}\\) test applied to the in that row. It therefore tests the model in row above against the model in the row in which it appears. 4.5.3.1 Example Test Problems 4.5.3.1.1 Example 1 Take \\(\\mathcal{H}_{0}: \\mathcal{M}_{1}\\) to be the null model, that is, \\(g(\\eta) = \\beta_{1}\\), and \\(\\mathcal{H}_{1}: \\mathcal{M}\\) to be the full model, \\(\\mathcal{M}_{7}\\). (This is the analogue of a full \\(F\\) test.) From the table we read that \\(D(\\mathcal{M}_{1}) = 8.17\\) while \\(D(\\mathcal{M}_{7}) = 5.12\\). We also see from the R output that \\(\\hat\\phi= 0.27\\). We therefore have \\[\\begin{equation} \\frac{D(\\mathcal{M}_{1}, \\mathcal{M}_{7}) }{ \\hat\\phi} = \\frac{8.17 - 5.12 }{ 0.27} = \\frac{3.05 }{ 0.27} = 11.47 \\end{equation}\\] This quantity is approximately \\(\\chi^{2}(6)\\) distributed as \\(\\mathcal{M}_{7}\\) has \\(6\\) more parameters \\(\\mathcal{M}_{1}\\). We find a \\(p\\) value \\[\\begin{equation} p = 1 - \\texttt{pchisq}(11.47, 6) = 0.075 \\end{equation}\\] We would thus just reject \\(\\mathcal{H}_{0}\\) at the \\(7.5\\%\\) level, quite weak evidence that the model explains anything at all. Note the intuition here. If \\(D(\\tilde{\\mathcal{M}}, \\mathcal{M})\\) is large, it means that the more complex model \\(\\mathcal{M}\\) is doing a much better job at explaining the data than the simpler model \\(\\tilde{\\mathcal{M}}\\). If it is enough better, then we will reject \\(\\mathcal{H}_{0}: \\tilde{\\mathcal{M}}\\). At the same time, when \\(D(\\tilde{\\mathcal{M}}, \\mathcal{M})\\) is large, it means that the \\(\\chi^{2}\\) value will be large and thus that the \\(p\\) value will be small, meaning that there is a small probability of finding our value of \\(D(\\tilde{\\mathcal{M}}, \\mathcal{M})\\) or greater, if \\(\\mathcal{H}_{0}\\) is true and \\(\\tilde{\\mathcal{M}}\\) is the correct model. Thus the smaller the \\(p\\) value, the less favourably we look on the null hypothesis and the more significant the level at which we can reject (more significance but smaller level number: we need \\(p &lt; 0.05\\) to reject at \\(5\\%\\), but \\(p &lt; 0.01\\) to reject at \\(1\\%\\)). 4.5.3.1.2 Example 2 Now we take \\(\\mathcal{H}_{0}: \\mathcal{M}_{2}\\), with \\(\\eta = \\beta_{1} + \\beta_{2}\\texttt{age}\\), and \\(\\mathcal{H}_{1}: \\mathcal{M}_{3}\\), with \\(\\eta = \\beta_{1} + \\beta_{2}\\texttt{age} + \\beta_{3}\\texttt{temp1}\\). These correspond to successive levels in the table, and so we can read the deviance directly from the table. We then find \\[\\begin{equation} \\frac{D(\\tilde{\\mathcal{M}}, \\mathcal{M}) }{ \\hat\\phi} = \\frac{1.003 }{ 0.27} = 3.77 \\end{equation}\\] This quantity is (approximately) \\(\\chi^{2}(1)\\) distributed, as there is one parameter difference between the two models. The \\(p\\) value is \\[\\begin{equation} p = 1 - \\texttt{pchisq}(3.77, 1) = 0.052 \\end{equation}\\] Thus, given , there is some weak evidence to support adding to the model: we can reject \\(\\mathcal{H}_{0}\\) at the \\(5.2\\%\\) level. 4.5.3.1.3 Example 3 Now we take \\(\\mathcal{H}_{0}: \\mathcal{M}_{3}\\), with \\(\\eta = \\beta_{1} + \\beta_{2}\\texttt{age} + \\beta_{3}\\texttt{temp1}\\), and \\(\\mathcal{H}_{1}: \\mathcal{M}_{7}\\). Reading from the table, we find \\(D(\\mathcal{M}_{7}) = 5.12\\), while \\(D(\\mathcal{M}_{3}) = 5.79\\). This gives \\[\\begin{equation} \\frac{D(\\tilde{\\mathcal{M}}, \\mathcal{M}) }{ \\hat\\phi} = \\frac{5.79 - 5.12 }{ 0.27} = 2.50 \\end{equation}\\] This quantity is (approximately) \\(\\chi^{2}(4)\\) distributed. The \\(p\\) value is \\[\\begin{equation} p = 1 - \\texttt{pchisq}(2.50, 4) = 0.65 \\end{equation}\\] There is thus no evidence at all for including any variable beyond and . One could argue that it is a good thing that deviance cannot be used as a general measure of goodness-of-fit, as it forces one to consider comparing one model against another. The idea that there is a measure of goodness-of-fit that applies in the absence of an alternative model is quite a dubious one.↩︎ Model \\(\\tilde{\\mathcal{M}}\\) is `nested’ in model \\(\\mathcal{M}\\) when the parameter space of \\(\\tilde{\\mathcal{M}}\\) is a subset of the parameter space of \\(\\mathcal{M}\\).↩︎ "],["quasi_likelihood.html", "Chapter 5 Quasi-Likelihood methods 5.1 Dispersion 5.2 Overdispersion 5.3 Generalized estimating equations", " Chapter 5 Quasi-Likelihood methods 5.1 Dispersion Recall the exponential dispersion family \\[ P(y_i|\\theta_i, \\phi_i)= \\exp\\left\\{\\frac{y_i\\theta_i-b(\\theta_i)}{\\phi_i}+c(y_i, \\phi_i)\\right\\}. \\] Assume grouping has been taken care of, that is \\(\\phi_i=\\phi/m_i\\) and so \\[ P(y_i|\\theta_i, \\phi)= \\exp\\left\\{ \\frac{y_i\\theta_i-b(\\theta_i)}{\\phi}+c(y_i, \\phi, m_i)\\right\\}. \\] Basic property (directly from GLM theory, see Section 6.2.2): \\[ \\mbox{Var}(y_i|\\theta_i, \\phi_i)= \\phi_i\\mathcal{V}(\\mu_i)= \\phi\\mathcal{V}(\\mu_i)/m_i \\] So, dispersion \\(\\phi\\) scales the variance but does but does not affect \\(E(y_i| \\theta_i,\\phi_i)=\\mu_i=h(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})\\). We know that the function \\(\\mathcal{V}(\\mu)\\) is characteristic for the response distribution at play: \\(Y\\) \\(\\mathcal{V}(\\mu)\\) \\(\\phi\\) \\(N(\\mu, \\sigma^2)\\) \\(1\\) \\(\\sigma^2\\) \\(\\mbox{Bernoulli}(p)\\) \\(p(1-p)\\) \\(1\\) \\(\\mbox{Poisson}(\\mu)\\) \\(\\mu\\) \\(1\\) \\(\\mbox{Gamma}(\\mu, \\nu)\\) \\(\\mu^2\\) \\(1/\\nu\\) \\(IG(\\mu, \\sigma^2)\\) \\(\\mu^3\\) \\(\\sigma^2\\) What is then the relevance of dispersion? For estimation of \\(\\boldsymbol{\\beta}\\), we note from (8.17) \\[ S(\\boldsymbol{\\beta})= \\frac{1}{\\phi}\\sum_{i=1}^n m_i \\left\\{...\\right\\} \\stackrel{!}{=}0 \\] that is \\(\\phi\\) cancels out when setting the score-function to 0. Hence, dispersion is irrelevant for the estimation of \\(\\boldsymbol{\\beta}\\). But for the variance of \\(\\hat{\\boldsymbol{\\beta}}\\) we have \\[ \\mbox{Var}(\\hat{\\boldsymbol{\\beta}})= F_{(\\phi)}^{-1}(\\hat{\\boldsymbol{\\beta}}) )= \\left[ \\frac{1}{\\phi}\\sum_{i=1}^{n} m_i \\left\\{\\ldots\\right\\}\\right]^{-1} = \\phi F_{(\\phi=1)}^{-1}(\\hat{\\boldsymbol{\\beta}}) \\] where \\(F_{(\\phi)}(\\cdot)\\) is the (expected) Fisher information when using dispersion \\(\\phi\\). This result implies that a dispersion of \\(\\phi\\) will inflate all standard errors, \\(SE(\\hat{\\beta}_j)\\), by the factor \\(\\sqrt{\\phi}\\) (when compared to using \\(\\phi=1\\)). Estimation of dispersion can be motivated through goodness-of-fit statistics (see also Sec 8.6): via Pearson goodness-of-fit statistic \\(\\chi^2_P= \\sum_{i=1}^nm_i\\frac{(y_i-\\hat{\\mu}_i)^2}{\\mathcal{V}(\\hat{\\mu}_i)}\\): \\[ \\begin{aligned} \\chi^2_P \\stackrel{a}{\\sim} \\phi \\chi^2(n-p)\\\\ E(\\chi^2_P)= \\phi \\times (n-p) \\end{aligned} \\] suggesting \\[ \\hat{\\phi}_{\\mbox{Pearson}}= \\frac{1}{n-p} \\chi^2_P \\] via Deviance, \\(D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}}) =2\\phi(\\ell_{sat}-\\ell(\\hat{\\boldsymbol{\\beta}}))\\): \\[ \\begin{aligned} D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}})\\stackrel{a}{\\sim} \\phi \\chi^2(n-p)\\\\ E(D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}}))= \\phi \\times (n-p) \\end{aligned} \\] suggesting \\[ \\hat{\\phi}_{\\mbox{dev}}= \\frac{D(\\boldsymbol{Y}, \\hat{\\boldsymbol{\\mu}})}{n-p} \\] The notation \\(a\\) in the distributional expressions highlights that these are just approximations. For the deviance, we know that the approximation can be very poor! Therefore the deviance-based estimate is sometimes called the “quick-and-dirty” dispersion estimate. 5.1.1 Example 5.1 (Hospital stay data) require(npmlreg) data(hosp) plot(hosp[,c(&quot;duration&quot;,&quot;age&quot;,&quot;temp1&quot;,&quot;wbc1&quot;)]) hosp.glm &lt;- glm(duration~age+temp1, data=hosp, family=Gamma(link=log)) summary(hosp.glm) ## ## Call: ## glm(formula = duration ~ age + temp1, family = Gamma(link = log), ## data = hosp) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -28.654096 16.621018 -1.724 0.0987 . ## age 0.014900 0.005698 2.615 0.0158 * ## temp1 0.306624 0.168141 1.824 0.0818 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.2690233) ## ## Null deviance: 8.1722 on 24 degrees of freedom ## Residual deviance: 5.7849 on 22 degrees of freedom ## AIC: 142.73 ## ## Number of Fisher Scoring iterations: 6 Dispersion estimate (Pearson): hosp.disp &lt;- 1/(hosp.glm$df.res)*sum( (hosp$duration-hosp.glm$fitted)^2/(hosp.glm$fitted^2)) hosp.disp ## [1] 0.2690233 Dispersion estimate (Deviance): hosp.glm$deviance/hosp.glm$df.residual ## [1] 0.2629518 summary(hosp.glm)$cov.unscaled # F^(-1) under phi=1 ## (Intercept) age temp1 ## (Intercept) 1026.8932335 -0.1386065114 -10.387121099 ## age -0.1386065 0.0001206901 0.001359292 ## temp1 -10.3871211 0.0013592917 0.105088741 sqrt(hosp.disp)* sqrt(diag(summary(hosp.glm)$cov.unscaled)) ## (Intercept) age temp1 ## 16.62101763 0.00569811 0.16814078 summary(hosp.glm)$coef[,&quot;Std. Error&quot;] ## (Intercept) age temp1 ## 16.62101763 0.00569811 0.16814078 5.2 Overdispersion Recall, for the Poisson model, one has \\(\\phi=1\\), i.e. \\[ \\mbox{Var}(y_i|\\theta_i)=1 \\times \\mathcal{V}(\\mu_i)=\\mathcal{V}(\\mu_i)=\\mu_i=E(y_i|\\theta_i) \\] i.e. \\[ \\frac{\\mbox{Var}(y_i|\\theta_i)}{E(y_i|\\theta_i)}=1, \\] a property referred to as “equidispersion”. 5.2.1 Example 5.2 (US Polio data) require(gamlss.data) data(polio) uspolio &lt;- as.data.frame( matrix( c( 1:168, t( polio ) ), ncol = 2 ) ) colnames( uspolio ) &lt;- c(&quot;time&quot;, &quot;cases&quot;) plot( uspolio, type = &quot;h&quot; ) Simple linear model: polio.glm &lt;- glm( cases ~ time, family = poisson( link = log ), data = uspolio ) summary( polio.glm ) ## ## Call: ## glm(formula = cases ~ time, family = poisson(link = log), data = uspolio) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.626639 0.123641 5.068 4.02e-07 *** ## time -0.004263 0.001395 -3.055 0.00225 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 333.55 on 166 degrees of freedom ## AIC: 594.59 ## ## Number of Fisher Scoring iterations: 5 Look at the summary; we see \\(\\phi=1\\). But let’s get a quick dispersion estimate: polio.disp &lt;- 333.55/166 polio.disp ## [1] 2.009337 Now with seasonal model (annual and semi-annual cycles): polio2.glm &lt;- glm(cases~time + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)), family=poisson(link=log), data=uspolio) summary(polio2.glm) ## ## Call: ## glm(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * ## pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * pi * ## time/6)), family = poisson(link = log), data = uspolio) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.557241 0.127303 4.377 1.20e-05 *** ## time -0.004799 0.001403 -3.421 0.000625 *** ## I(cos(2 * pi * time/12)) 0.137132 0.089479 1.533 0.125384 ## I(sin(2 * pi * time/12)) -0.534985 0.115476 -4.633 3.61e-06 *** ## I(cos(2 * pi * time/6)) 0.458797 0.101467 4.522 6.14e-06 *** ## I(sin(2 * pi * time/6)) -0.069627 0.098123 -0.710 0.477957 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 288.85 on 162 degrees of freedom ## AIC: 557.9 ## ## Number of Fisher Scoring iterations: 5 polio2.disp&lt;- 288.65/162 polio2.disp ## [1] 1.78179 We see that the dispersion reduces when improving the fit of the model, but it is unlikely to `disappear’ fully. In the exammple above there is overdispersion, i.e. more dispersion in the data than supported by the assumption \\(\\phi=1\\). Overdispersion can arise for all one-parameter exponential family distributions (Poisson, Bernoulli/Binomial,…). Possible reasons for overdispersion can include: Model misspecification (such as omitted covariates) latent clusters/subpopulations in the data (“unobserved heterogeneity”) What is the impact of overdispersion? There is no impact on \\(\\hat{\\beta}\\). The standard error, \\(SE(\\hat{\\beta}_j)\\), when estimated using \\(\\phi=1\\), is underestimated by the factor \\(\\sqrt{\\phi}\\) (with \\(\\phi\\) denoting the “true” \\(\\phi\\).). As a consequence, confidence intervals will be too small; p-values will be too small; hence, significances overstated and potentially wrong decisions. Fortunately, there is a simple remedy to this problem: Fit a Poisson or Bernoulli/Binomial model as usual Estimate \\(\\phi\\) as discussed above, yielding \\(\\hat{\\phi}\\). Multiply all standard errors by \\(\\sqrt{\\hat{\\phi}}\\). De facto, this amounts to the model \\[ \\begin{aligned} \\mu_i = h(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})\\\\ \\mbox{Var}(y_i)= \\phi\\mathcal{V}(\\mu_i) \\end{aligned} \\] with “quasi-score function” (of which clearly \\(\\phi\\) cancels out) \\[ S(\\boldsymbol{\\beta})= \\sum_{i=1}^n m_i \\boldsymbol{x}_i \\frac{y_i-\\mu_i}{\\phi\\mathcal{V}(\\mu_i)}h^{\\prime}(\\boldsymbol{x}_i^T\\boldsymbol{\\beta})\\stackrel{!}{=}0 \\] and variance estimate \\[ \\hat{\\text{Var}}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\phi}F^{-1}(\\hat{\\boldsymbol{\\beta}}) \\] Note that doing all this requires no actual distributional assumption (and there could also not be any: The `Poisson’-type EDF with \\(\\phi&gt;1\\) is not a valid pdf!) Estimation methods using quasi-score functions are also referred to as quasi-likelihood methods. 5.2.2 Example 5.3 (US Polio data) Let us firstly produce the Pearson-type dispersion estimates. polio.phi &lt;- 1/(polio.glm$df.res)*sum( (uspolio$cases-polio.glm$fitted)^2/(polio.glm$fitted)) polio.phi ## [1] 2.481818 polio2.phi &lt;- 1/(polio2.glm$df.res)*sum( (uspolio$cases-polio2.glm$fitted)^2/(polio2.glm$fitted)) polio2.phi ## [1] 1.967417 Then adjust the standard errors: polio.se &lt;- sqrt(polio.phi)*summary(polio.glm)$coef[,2] polio.se ## (Intercept) time ## 0.194781773 0.002198181 polio2.se &lt;- sqrt(polio2.phi)*summary(polio2.glm)$coef[,2] polio2.se ## (Intercept) time I(cos(2 * pi * time/12)) ## 0.178560577 0.001967754 0.125507133 ## I(sin(2 * pi * time/12)) I(cos(2 * pi * time/6)) I(sin(2 * pi * time/6)) ## 0.161971821 0.142321901 0.137631208 These results can be obtained in R directly through the use of the quasipoisson (or, similarly, quasibinomial) commands: polio.qglm&lt;- glm(cases ~ time, family=quasipoisson(link=log), data=uspolio) summary(polio.qglm) ## ## Call: ## glm(formula = cases ~ time, family = quasipoisson(link = log), ## data = uspolio) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.626639 0.194788 3.217 0.00156 ** ## time -0.004263 0.002198 -1.939 0.05415 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 2.481976) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 333.55 on 166 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 polio2.qglm&lt;- glm(cases~time + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)), family=quasipoisson(link=log), data=uspolio) summary(polio2.qglm) ## ## Call: ## glm(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * ## pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * pi * ## time/6)), family = quasipoisson(link = log), data = uspolio) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.557241 0.178566 3.121 0.00214 ** ## time -0.004799 0.001968 -2.439 0.01583 * ## I(cos(2 * pi * time/12)) 0.137132 0.125511 1.093 0.27620 ## I(sin(2 * pi * time/12)) -0.534985 0.161977 -3.303 0.00118 ** ## I(cos(2 * pi * time/6)) 0.458797 0.142326 3.224 0.00153 ** ## I(sin(2 * pi * time/6)) -0.069627 0.137635 -0.506 0.61363 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 1.96754) ## ## Null deviance: 343.00 on 167 degrees of freedom ## Residual deviance: 288.85 on 162 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 One can test for the presence of overdispersion by comparing \\(\\hat{\\phi}\\) to \\(\\chi^2_{n-p,\\alpha}/(n-p)\\). For instance, for the two models under consideration, the critical values for \\(H_0\\): \\(\\phi=0\\) at the \\(5\\%\\) level of significance would be qchisq(0.95,polio.glm$df.res)/polio.glm$df.res ## [1] 1.187132 qchisq(0.95,polio2.glm$df.res)/polio2.glm$df.res ## [1] 1.189507 so that \\(H_0\\) would be rejected in both cases. 5.3 Generalized estimating equations The Quasi-likelihood techniques discussed in the previous subsection motivate a more general concept. Therefore, rewrite \\[ S(\\boldsymbol{\\beta})= \\sum_{i=1}^n \\boldsymbol{x}_i\\frac{y_i-\\mu_i}{\\phi_i \\mathcal{V}(\\mu_i)}h^{\\prime}(\\eta_i)= \\boldsymbol{X}^T\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{\\mu}) \\] which is known from (8.61) and where as usual \\(\\phi_i=\\phi/m_i\\), \\(\\eta_i=\\boldsymbol{x}_i^T\\boldsymbol{\\beta}\\), \\(\\boldsymbol{D}=\\mbox{diag}(h^{\\prime}(\\eta_i))\\), and \\(\\boldsymbol{\\Sigma}= \\mbox{diag}(\\mbox{var}(y_i))= \\mbox{diag}(\\phi_i \\mathcal{V}(\\mu_i))\\). The idea is now to replace the matrix \\(\\boldsymbol{\\Sigma}\\) (the shape of which was originally motivated directly by GLM properties) by any appropriate ``working covariance’’ matrix \\(\\boldsymbol{\\Sigma}= \\mbox{var}(\\boldsymbol{Y})\\), in this manner trying to capture any correlation structures in the data as correctly as possible. From here we would compute (just as for GLMs) \\[ F(\\boldsymbol{\\beta})= \\boldsymbol{X}^T\\boldsymbol{D}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{D}\\boldsymbol{X} \\] allowing us to estimate the variance of \\(\\hat{\\boldsymbol{\\beta}}\\) as \\[ \\mbox{var}(\\hat{\\boldsymbol{\\beta}}) \\approx F^{-1}(\\hat{\\boldsymbol{\\beta}}). \\] Important theoretical result: Under some regularity conditions, the estimator \\(\\hat{\\boldsymbol{\\beta}}\\) is consistent and asymptotically normal even if \\(\\boldsymbol{\\Sigma}\\) is wrong! 5.3.1 Example 5.4 We first use GEEs to provide an equivalent estimate of the quasipoisson appraoch. require(gee) ## Loading required package: gee uspolio.gee &lt;- gee(cases~time + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)), family=poisson(link=log),id=rep(1,168), corstr = &quot;independence&quot;, data=uspolio) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) time I(cos(2 * pi * time/12)) ## 0.557240558 -0.004798661 0.137131634 ## I(sin(2 * pi * time/12)) I(cos(2 * pi * time/6)) I(sin(2 * pi * time/6)) ## -0.534985461 0.458797164 -0.069627044 uspolio.gee ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Logarithm ## Variance to Mean Relation: Poisson ## Correlation Structure: Independent ## ## Call: ## gee(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * ## pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * pi * ## time/6)), id = rep(1, 168), data = uspolio, family = poisson(link = log), ## corstr = &quot;independence&quot;) ## ## Number of observations : 168 ## ## Maximum cluster size : 168 ## ## ## Coefficients: ## (Intercept) time I(cos(2 * pi * time/12)) ## 0.557240556 -0.004798661 0.137131637 ## I(sin(2 * pi * time/12)) I(cos(2 * pi * time/6)) I(sin(2 * pi * time/6)) ## -0.534985464 0.458797164 -0.069627041 ## ## Estimated Scale Parameter: 1.967417 ## Number of Iterations: 1 ## ## Working Correlation[1:4,1:4] ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## ## ## Returned Error Value: ## [1] 0 summary(uspolio.gee)$coef ## Estimate Naive S.E. Naive z Robust S.E. ## (Intercept) 0.557240556 0.1785640 3.1206774 1.950799e-16 ## time -0.004798661 0.0019678 -2.4385923 2.214098e-18 ## I(cos(2 * pi * time/12)) 0.137131637 0.1255114 1.0925831 6.458247e-17 ## I(sin(2 * pi * time/12)) -0.534985464 0.1619769 -3.3028513 5.215875e-16 ## I(cos(2 * pi * time/6)) 0.458797164 0.1423255 3.2235766 4.591807e-16 ## I(sin(2 * pi * time/6)) -0.069627041 0.1376347 -0.5058827 1.095963e-17 ## Robust z ## (Intercept) 2.856473e+15 ## time -2.167321e+15 ## I(cos(2 * pi * time/12)) 2.123357e+15 ## I(sin(2 * pi * time/12)) -1.025687e+15 ## I(cos(2 * pi * time/6)) 9.991647e+14 ## I(sin(2 * pi * time/6)) -6.353046e+15 But then, there is very likely serial correlation! So # uspolio.gee2 &lt;- gee(cases~time + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) # + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)), family=poisson(link=log), # id=rep(1,168), data=uspolio, corstr=&quot;AR-M&quot;, Mv=1) Does not fit! We can try another package…. require(geepack) ## Loading required package: geepack uspolio.gee3 &lt;- geeglm( cases~time+ I(cos(2*pi*time/12))+I(sin(2*pi*time/12)) + I(cos(2*pi*time/6))+ I(sin(2*pi*time/6)), family=poisson(link=log), id=rep(1,168), corstr=&quot;ar1&quot;, data=uspolio) uspolio.gee3 ## ## Call: ## geeglm(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * ## pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * pi * ## time/6)), family = poisson(link = log), data = uspolio, id = rep(1, ## 168), corstr = &quot;ar1&quot;) ## ## Coefficients: ## (Intercept) time I(cos(2 * pi * time/12)) ## 0.533915758 -0.004450645 0.143705437 ## I(sin(2 * pi * time/12)) I(cos(2 * pi * time/6)) I(sin(2 * pi * time/6)) ## -0.529102025 0.455450494 -0.065291103 ## ## Degrees of Freedom: 168 Total (i.e. Null); 162 Residual ## ## Scale Link: identity ## Estimated Scale Parameters: [1] 1.892594 ## ## Correlation: Structure = ar1 Link = identity ## Estimated Correlation Parameters: ## alpha ## 0.2790038 ## ## Number of clusters: 1 Maximum cluster size: 168 … or better simplify the model. For instance, let us assume we have an AR(1) correlation structure within each year, but different years are independent: require(gee) id=rep(1:14,each=12) id ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 ## [26] 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 5 5 ## [51] 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 ## [76] 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 9 9 9 9 ## [101] 9 9 9 9 9 9 9 9 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 ## [126] 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 ## [151] 13 13 13 13 13 13 14 14 14 14 14 14 14 14 14 14 14 14 uspolio.gee4 &lt;- gee(cases~time + I(cos(2*pi*time/12)) + I(sin(2*pi*time/12)) + I(cos(2*pi*time/6)) + I(sin(2*pi*time/6)), family=poisson(link=log), id=id, data=uspolio, corstr=&quot;AR-M&quot;, Mv=1) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) time I(cos(2 * pi * time/12)) ## 0.557240558 -0.004798661 0.137131634 ## I(sin(2 * pi * time/12)) I(cos(2 * pi * time/6)) I(sin(2 * pi * time/6)) ## -0.534985461 0.458797164 -0.069627044 uspolio.gee4 ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Logarithm ## Variance to Mean Relation: Poisson ## Correlation Structure: AR-M , M = 1 ## ## Call: ## gee(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * ## pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * pi * ## time/6)), id = id, data = uspolio, family = poisson(link = log), ## corstr = &quot;AR-M&quot;, Mv = 1) ## ## Number of observations : 168 ## ## Maximum cluster size : 12 ## ## ## Coefficients: ## (Intercept) time I(cos(2 * pi * time/12)) ## 0.534670137 -0.004504214 0.127605025 ## I(sin(2 * pi * time/12)) I(cos(2 * pi * time/6)) I(sin(2 * pi * time/6)) ## -0.518732586 0.434976179 -0.059598999 ## ## Estimated Scale Parameter: 1.983319 ## Number of Iterations: 3 ## ## Working Correlation[1:4,1:4] ## [,1] [,2] [,3] [,4] ## [1,] 1.00000000 0.26087713 0.06805688 0.01775448 ## [2,] 0.26087713 1.00000000 0.26087713 0.06805688 ## [3,] 0.06805688 0.26087713 1.00000000 0.26087713 ## [4,] 0.01775448 0.06805688 0.26087713 1.00000000 ## ## ## Returned Error Value: ## [1] 0 round(summary(uspolio.gee4)$working.correlation, digits=3) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## [1,] 1.000 0.261 0.068 0.018 0.005 0.001 0.000 0.000 0.000 0.000 0.000 0.000 ## [2,] 0.261 1.000 0.261 0.068 0.018 0.005 0.001 0.000 0.000 0.000 0.000 0.000 ## [3,] 0.068 0.261 1.000 0.261 0.068 0.018 0.005 0.001 0.000 0.000 0.000 0.000 ## [4,] 0.018 0.068 0.261 1.000 0.261 0.068 0.018 0.005 0.001 0.000 0.000 0.000 ## [5,] 0.005 0.018 0.068 0.261 1.000 0.261 0.068 0.018 0.005 0.001 0.000 0.000 ## [6,] 0.001 0.005 0.018 0.068 0.261 1.000 0.261 0.068 0.018 0.005 0.001 0.000 ## [7,] 0.000 0.001 0.005 0.018 0.068 0.261 1.000 0.261 0.068 0.018 0.005 0.001 ## [8,] 0.000 0.000 0.001 0.005 0.018 0.068 0.261 1.000 0.261 0.068 0.018 0.005 ## [9,] 0.000 0.000 0.000 0.001 0.005 0.018 0.068 0.261 1.000 0.261 0.068 0.018 ## [10,] 0.000 0.000 0.000 0.000 0.001 0.005 0.018 0.068 0.261 1.000 0.261 0.068 ## [11,] 0.000 0.000 0.000 0.000 0.000 0.001 0.005 0.018 0.068 0.261 1.000 0.261 ## [12,] 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.005 0.018 0.068 0.261 1.000 This last assumption on the correlation structure of the data leads us to repeated measures data, which is the actual strength of GEEs. "],["marginal_models.html", "Chapter 6 Marginal models 6.1 Repeated measures data 6.2 The marginal model for repeated measures 6.3 Estimation", " Chapter 6 Marginal models 6.1 Repeated measures data 6.1.1 Example 6.1 (Oxford boys data) We consider a data set giving repeated measures of height (cm) of 26 boys over eight time points (i.e. total sample size \\(N=234\\).) require(nlme) ## Loading required package: nlme data(Oxboys) # ?Oxboys require(ggplot2) ## Loading required package: ggplot2 ggplot(data = Oxboys, aes(x = age, y = height, col = Subject))+ geom_point(size = 1.2, alpha = .8) + labs(title = &quot;Height vs. Age&quot;, subtitle=&quot;&quot;) theme(legend.position = &quot;none&quot;) ## List of 1 ## $ legend.position: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi FALSE ## - attr(*, &quot;validate&quot;)= logi TRUE Different look at the data: ggplot(data = Oxboys, aes(x = age, y = height, col = Subject)) + geom_line(size = 1.2, alpha = .8) + labs(title = &quot;Height vs. Age&quot;, subtitle=&quot;by subject&quot;)+ theme(legend.position = &quot;none&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Clearly, there is within-subject correlation: Measures from one subject are more similar to each other than those from different subjects. This is a special type of repeated measures data which is commonly referred to as longitudinal data: Repeated measures on certain individuals over time. 6.1.2 Example 6.2 (Mathematics achievement data) The data used here represent Mathematics achievement scores of a subsample of subjects from the 1982 High School and Beyond Survey. The full dataset can be found within the package merTools. For each of 30 schools, we have pupil-level data on Mathematics achievement and a few covariates including socioeconomic status. load(&quot;Datasets/sub_hsb.RData&quot;) # Insert directory head(sub_hsb) ## schid minority female ses mathach size schtype meanses ## 1 1224 0 1 -1.528 5.876 842 0 -0.428 ## 2 1224 0 1 -0.588 19.708 842 0 -0.428 ## 3 1224 0 0 -0.528 20.349 842 0 -0.428 ## 4 1224 0 0 -0.668 8.781 842 0 -0.428 ## 5 1224 0 0 -0.158 17.898 842 0 -0.428 ## 6 1224 0 0 0.022 4.583 842 0 -0.428 dim(sub_hsb) ## [1] 1329 8 school.id &lt;- as.factor(sub_hsb$schid) length(levels(school.id)) ## [1] 30 ggplot(data = sub_hsb, aes(x = ses, y = mathach))+ geom_point(size = 0.8, alpha = .8)+ geom_smooth(method=&quot;lm&quot;, se=FALSE, col=&quot;Red&quot;)+ ggtitle(&quot;Mathematics achievement vs. Socioeconomic status (SES)&quot;) + xlab(&quot;SES&quot;) + ylab(&quot;Mathematics achievement&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ggplot(data=sub_hsb, aes(x= ses, y= mathach, colour=school.id) )+ geom_point(size = 0.8, alpha = .8)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ ggtitle(&quot;Mathematics achievement vs. Socioeconomic status (SES)&quot;, subtitle =&quot;by cluster (school)&quot; ) + xlab(&quot;SES&quot;) + ylab(&quot;Mathematics achievement&quot;)+ theme(legend.position = &quot;none&quot;) One will suspect from these plots that there is within-school-correlation: Pupils from one school tend tend to be more similar to each other than to pupils from different schools. This type of repeated measures data is commonly referred to as ``clustered’’ data, with clusters here referring to schools. Note that the term clustered is here just to be interpreted in the sense of “items of increased similarity due to a structural reason”, but it has nothing to do with “clustering” such as for instance k-means. Fitting naïve LMs or GLMs to repeated measures data may or may not result in correct inferences for \\(\\boldsymbol{\\beta}\\), but in any case the standard errors \\(SE(\\hat{\\beta}_j)\\) will be incorrect. Fortunately, longitudinal and clustered data can be dealt with in the same framework. 6.2 The marginal model for repeated measures Denote \\(y_{ij}\\), \\(i=1, \\ldots,n\\), \\(j=1, \\ldots, n_i\\), the \\(j\\)th repeated measurement for cluster/individual \\(i\\) (we will just speak of “cluster” henceforth), so that the total sample size is \\(N=\\sum_{i=1}^n n_i\\). Denote further the vector of all responses (repeated messurements) belonging to the \\(i\\)th cluster by \\[ \\boldsymbol{Y}_i=\\left(\\begin{array}{c} y_{i1}\\\\ \\vdots \\\\ y_{in_i}\\end{array}\\right), \\] with associated covariates \\[ \\boldsymbol{X}_i= \\left(\\begin{array}{c} \\boldsymbol{x}_{i1}^T \\\\ \\vdots \\\\\\boldsymbol{x}_{in_i}^T\\end{array} \\right) = \\left(\\begin{array}{ccc} x_{i11}&amp; \\ldots &amp; x_{i1p}\\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{in_i1} &amp; \\ldots &amp; x_{in_ip} \\end{array}\\right). \\] A marginal model for \\(y_{ij}\\) has three components: the mean function (“correctly specified”) \\[ \\mu_{ij}=E(y_{ij})=h(\\boldsymbol{x}_{ij}^T\\boldsymbol{\\beta}); \\] the mean-variance relationship \\[ \\mbox{Var}(y_{ij})=\\phi\\mathcal{V}(\\mu_{ij}); \\] the association between the responses \\[ \\begin{aligned} \\mbox{corr}(y_{ij}, y_{i^{\\prime}k})&amp;=&amp; 0 \\mbox{ for all }&amp; i\\not=i^{\\prime}\\\\ \\mbox{corr}(y_{ij}, y_{ik}) &amp;=&amp; r_{jk}(\\boldsymbol{\\alpha}), \\end{aligned} \\] where \\(r_{jk}(\\cdot)\\) is a known function indexed by \\(j,k=1, \\ldots n_i\\), and \\(\\boldsymbol{\\alpha}\\) is a set of parameters. The specified variances and correlations of the \\(y_{ij}\\) define uniquely the variance matrix \\(\\boldsymbol{\\Sigma}_i\\) of the elements of the \\(i\\)th cluster. The most common settings of the function \\(r_{jk}(\\cdot)\\) are as follows: “Independence”. Here \\(r_{jk}(\\boldsymbol{\\alpha})\\equiv r_{jk}\\) does not depend on parameters, where \\[ r_{jk} =\\left\\{\\begin{array}{ll} 1 &amp; j=k \\\\ 0 &amp; j \\not= k \\end{array}\\right. \\] “Exchangeable” or “Equicorrelation”: For \\(\\boldsymbol{\\alpha}=\\alpha \\in \\mathbb{R}\\), \\[ r_{jk}(\\alpha)= \\left\\{\\begin{array}{ll} 1 &amp; j=k \\\\ \\alpha &amp; j \\not= k \\end{array}\\right. \\] “Autoregressive model” (AR-1): For \\(\\boldsymbol{\\alpha}=\\alpha \\in \\mathbb{R}\\), \\[ r_{jk}(\\alpha)= \\alpha^{|j-k|} \\] “Unstructured”: For \\(n_i \\equiv n^*\\) for all \\(i\\), then with \\(\\boldsymbol{\\alpha} \\in \\mathbb{R}^{n^* \\times n^*}\\), and \\[ r_{jk}(\\boldsymbol{\\alpha})= \\boldsymbol{\\alpha}_{jk} \\] Some notes: There is no distributional assumption and there is no likelihood (Beyond the repeated measures context, this approach is also useful for complex modelling scenarios where building a likelihood may be very hard) \\(\\boldsymbol{\\alpha}\\) and \\(\\boldsymbol{\\beta}\\) are overall model parameters which do not depend on the cluster, \\(i\\). Marginal models provide “population-averaged” effects (unlike “conditional effects”, which provide effects conditional on each cluster \\(i\\), as we will see later for the mixed models). What can we say about the matrices \\(\\boldsymbol{\\Sigma}_i\\)? Firstly, element-wise, combining the specifications of variances and correlations, we have \\[ \\mbox{Cov}(y_{ij}, y_{ik})= \\phi r_{jk}(\\boldsymbol{\\alpha})\\sqrt{\\mathcal{V}(\\mu_{ij})}\\sqrt{\\mathcal{V}(\\mu_{ik})}. \\] Globally, defining the working correlation matrix \\[ R_i(\\boldsymbol{\\alpha})= (r_{jk}(\\boldsymbol{\\alpha}))_{1\\le j \\le n_i, 1 \\le k \\le n_i} \\] and \\[ C_i(\\boldsymbol{\\beta}, \\phi)= \\mbox{diag}(\\phi \\mathcal{V}(\\mu_{ij})), \\] one has \\[ \\boldsymbol{\\Sigma}_i=C_i^{1/2}(\\boldsymbol{\\beta}, \\phi)R_i(\\mathcal{\\alpha})C_i^{1/2}(\\boldsymbol{\\beta}, \\phi) \\] (clearly depends on \\(\\boldsymbol{\\alpha}\\), \\(\\boldsymbol{\\beta}\\), \\(\\phi\\)). 6.2.1 Example 6.3 We exemplify the model structure through some of our recent data examples. US Polio data (Example 13.3) \\[ \\begin{aligned} \\mu_{ij}&amp;=&amp; \\exp(\\beta_0+\\beta_1t_{ij})\\\\ \\mathcal{V}(\\mu_{ij})&amp;=&amp;\\mu_{ij}\\\\ r_{jk}(\\alpha)&amp;=&amp; \\alpha^{|j-k|}\\\\ \\mbox{Cov}(y_{ij}, y_{ik})&amp;=&amp;\\phi \\alpha^{|j-k|}\\sqrt{\\mu_{ij}\\mu_{ik}} \\end{aligned} \\] Oxford boys data (Example 6.1) \\[ \\begin{aligned} \\mu_{ij}&amp;=&amp; \\beta_0+\\beta_1t_{ij}\\\\ \\mathcal{V}(\\mu_{ij})&amp;=&amp;1, \\mbox{ where } \\phi\\equiv \\sigma^2 \\\\ r_{jk}(\\alpha)&amp;=&amp; \\alpha^{|j-k|}\\\\ \\mbox{Cov}(y_{ij}, y_{ik})&amp;=&amp; \\sigma^2 \\alpha^{|j-k|} \\end{aligned} \\] Mathematics achievement data (Example 6.2) \\[ \\begin{aligned} \\mu_{ij}&amp;=&amp; \\beta_0+\\beta_1\\mbox{SES}_{ij}\\\\ \\mathcal{V}(\\mu_{ij})&amp;=&amp;1, \\mbox{ where } \\phi\\equiv \\sigma^2 \\\\ r_{jk}(\\alpha)&amp;=&amp; \\alpha\\\\ \\mbox{Cov}(y_{ij}, y_{ik})&amp;=&amp;\\left\\{ \\begin{array}{lc}\\alpha\\sigma^2\\,&amp;\\mbox{ if } j \\not=k\\\\ \\sigma^2 \\,&amp;\\mbox{ if } j =k \\end{array}\\right. \\end{aligned} \\] 6.3 Estimation All what is needed to estimate the model elaborated above is the (generalized version of the) Quasi-Score function \\[ S(\\boldsymbol{\\beta})= \\boldsymbol{X}^T\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{\\mu}) \\] where \\[ \\boldsymbol{Y}=\\left(\\begin{array}{c}\\boldsymbol{Y}_1\\\\ \\vdots\\\\ \\boldsymbol{Y}_n\\end{array}\\right) \\in \\mathbb{R}^N,\\,\\,\\, \\boldsymbol{X} = \\left(\\begin{array}{c}\\boldsymbol{X}_1\\\\ \\vdots\\\\ \\boldsymbol{X}_n\\end{array}\\right) \\in \\mathbb{R}^{N \\times p},\\,\\,\\, \\boldsymbol{\\mu} = \\left(\\begin{array}{c}\\boldsymbol{\\mu}_1\\\\ \\vdots\\\\ \\boldsymbol{\\mu}_n\\end{array}\\right) \\in \\mathbb{R}^{N},\\,\\,\\, \\] where \\(\\boldsymbol{Y}_i\\) and \\(\\boldsymbol{X}_i\\) are as defined in Section 6.2, and \\(\\boldsymbol{\\mu}_i= (\\mu_{i1}, \\ldots, \\mu_{in_i})^T\\), with \\(\\mu_{ij}=h(\\boldsymbol{x}_{ij}^T\\boldsymbol{\\beta})\\) and \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\). Furthermore, \\[ \\boldsymbol{D}=\\left(\\begin{array}{cccc} h^{\\prime}(\\boldsymbol{x}_{11}^T\\boldsymbol{\\beta}) &amp; &amp; &amp; \\\\ &amp; h^{\\prime}(\\boldsymbol{x}_{12}^T\\boldsymbol{\\beta}) &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp;&amp; &amp; h^{\\prime}(\\boldsymbol{x}_{nn_n}^T\\boldsymbol{\\beta})\\end{array}\\right) \\in \\mathbb{R}^{N\\times N},\\, \\boldsymbol{\\Sigma}= \\left(\\begin{array}{cccc}\\boldsymbol{\\Sigma}_1 &amp;&amp;&amp;\\\\ &amp;\\boldsymbol{\\Sigma_2} &amp;&amp;\\\\ &amp;&amp; \\ddots &amp; \\\\ &amp;&amp;&amp; \\boldsymbol{\\Sigma}_n \\end{array}\\right) \\in \\mathbb{R}^{N \\times N}, \\] noting that \\(\\boldsymbol{\\Sigma}_i \\in \\mathbb{R}^{n_i \\times n_i}\\). Setting these to 0 yields the generalized estimating equation (GEE), \\[ \\boldsymbol{X}^T\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{\\mu})=0 \\] If \\(\\boldsymbol{\\Sigma}\\) is known (and correctly specified) up to a multiplicative function of \\(\\phi\\) (but not depending on further parameters, \\(\\boldsymbol{\\alpha}\\)), then solve the GEE via Iteratively Weighted Least Squares (IWLS). That is, in complete analogy to the GLM estimation routines in Sec 8.8, one has \\[ \\hat{\\boldsymbol{\\beta}}_{m+1}=(\\boldsymbol{X}^T\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{D}\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{D}\\hat{\\boldsymbol{Y}}_m \\] where \\(\\hat{\\boldsymbol{Y}}_m\\) is a vector of working observations (which is also the same as in Sec 8.8). If \\(\\boldsymbol{Y}\\) is in fact multivariate normal and \\(h(\\cdot)\\) the identity link (so \\(\\boldsymbol{D}=\\boldsymbol{I}\\)), then one iteration \\((\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{Y}\\) is sufficient. In either case, estimation of \\(\\boldsymbol{\\beta}\\) does not depend on \\(\\phi\\), so \\(\\phi\\) it can be estimated separately after the last iteration. Otherwise (if \\(\\boldsymbol{\\Sigma}\\) depends on unknown parameters \\(\\boldsymbol{\\alpha}\\)), cycle between: Given current \\(\\hat{\\boldsymbol{\\alpha}}\\) and \\(\\hat{\\phi}\\), estimate \\(\\hat{\\boldsymbol{\\beta}}\\) by (one iteration of) IWLS; Given current \\(\\hat{\\boldsymbol{\\beta}}\\), estimate \\(\\hat{\\boldsymbol{\\alpha}}\\) and \\(\\hat{\\phi}\\) [explicit formulae in Fahrmeir &amp; Tutz page 125]. Variance estimation (under either scenario): “naïve”: \\[ \\mbox{Var}(\\hat{\\boldsymbol{\\beta}})= (\\boldsymbol{X}^T\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{D}\\boldsymbol{X})^{-1}\\equiv \\boldsymbol{F}^{-1} \\] “robust”: Sandwich variance estimator \\[ \\mbox{Var}_s(\\hat{\\boldsymbol{\\beta}})= \\boldsymbol{F}^{-1}\\boldsymbol{V}\\boldsymbol{F}^{-1} \\] where \\(\\boldsymbol{V}=\\boldsymbol{X}^T\\boldsymbol{D}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{D}\\boldsymbol{X}\\), and \\(\\boldsymbol{S}\\) is the so-called “true” variance matrix estimated as \\[ \\boldsymbol{S}= \\left(\\begin{array}{ccc}\\boldsymbol{S}_1 &amp;&amp;\\\\ &amp; \\ddots &amp; \\\\ &amp;&amp; \\boldsymbol{S}_n \\end{array}\\right) \\in \\mathbb{R}^{N \\times N}. \\] with \\(\\boldsymbol{S}_i=(\\boldsymbol{Y}_i-\\hat{\\boldsymbol{\\mu}}_i)(\\boldsymbol{Y}_i-\\hat{\\boldsymbol{\\mu}}_i)^T\\). Theoretical properties [Fahrmeir &amp; Tutz page 126/127]: Under some regularity conditions, \\(\\hat{\\boldsymbol{\\beta}}\\) is consistent (i.e. \\(\\hat{\\boldsymbol{\\beta}} \\longrightarrow \\boldsymbol{\\beta}\\) for \\(N \\longrightarrow \\infty\\)) and asymptotically normal, \\[ \\hat{\\boldsymbol{\\beta}} \\sim N(\\hat{\\boldsymbol{\\beta}}, \\boldsymbol{F}^{-1}\\boldsymbol{V}\\boldsymbol{F}^{-1} ) \\] even if the specification of \\(\\boldsymbol{\\Sigma}\\) is wrong. Correct specification of \\(\\boldsymbol{\\mu}\\) is therefore more important than that of \\(\\boldsymbol{\\Sigma}\\). 6.3.1 Example 6.4 GEE for Mathematics achievement data: require(gee) hsb.gee &lt;- gee(mathach~ses, data=sub_hsb, id=school.id, corstr = &quot;exchangeable&quot;) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) ses ## 12.886358 3.453019 hsb.gee ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Identity ## Variance to Mean Relation: Gaussian ## Correlation Structure: Exchangeable ## ## Call: ## gee(formula = mathach ~ ses, id = school.id, data = sub_hsb, ## corstr = &quot;exchangeable&quot;) ## ## Number of observations : 1329 ## ## Maximum cluster size : 67 ## ## ## Coefficients: ## (Intercept) ses ## 12.884541 2.170503 ## ## Estimated Scale Parameter: 41.85127 ## Number of Iterations: 4 ## ## Working Correlation[1:4,1:4] ## [,1] [,2] [,3] [,4] ## [1,] 1.0000000 0.1253383 0.1253383 0.1253383 ## [2,] 0.1253383 1.0000000 0.1253383 0.1253383 ## [3,] 0.1253383 0.1253383 1.0000000 0.1253383 ## [4,] 0.1253383 0.1253383 0.1253383 1.0000000 ## ## ## Returned Error Value: ## [1] 0 # Compare to hsb.lm&lt;- lm(mathach~ses, data=sub_hsb) sub_hsb$pred1 &lt;- predict(hsb.gee) ggplot(data = sub_hsb, aes(x = ses, y = mathach))+ geom_point(size = 0.8, alpha = .8)+ geom_smooth(method=&quot;lm&quot;, se=FALSE, col=&quot;Red&quot;)+ geom_line(aes(x=ses,y=pred1), col=&quot;Blue&quot;)+ ggtitle(&quot;Mathematics achievement vs. Socioeconomic status (SES)&quot;, subtitle=&quot;Blue line is GEE solution&quot; ) + xlab(&quot;SES&quot;) + ylab(&quot;Mathematics achievement&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; What about standard errors? summary(hsb.gee)$coef ## Estimate Naive S.E. Naive z Robust S.E. Robust z ## (Intercept) 12.884541 0.4524909 28.474697 0.4784090 26.93206 ## ses 2.170503 0.2538904 8.548976 0.3576248 6.06922 # Compare to summary(hsb.lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.886358 0.1752970 73.51156 0.000000e+00 ## ses 3.453019 0.2222153 15.53907 3.738528e-50 Observations: Both the actual estimates, and their standard errors, are quite different for the GEE and the LM. The robust standard errors are still a bit larger than the naive ones. GEE for Oxford boys data: data(Oxboys, package=&quot;nlme&quot;) oxboys.gee &lt;- gee(height ~ age, data=Oxboys, id=Subject, corstr=&quot;AR-M&quot;, Mv=1) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) age ## 149.371801 6.521022 oxboys.gee ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Identity ## Variance to Mean Relation: Gaussian ## Correlation Structure: AR-M , M = 1 ## ## Call: ## gee(formula = height ~ age, id = Subject, data = Oxboys, corstr = &quot;AR-M&quot;, ## Mv = 1) ## ## Number of observations : 234 ## ## Maximum cluster size : 9 ## ## ## Coefficients: ## (Intercept) age ## 149.719096 6.547328 ## ## Estimated Scale Parameter: 65.41743 ## Number of Iterations: 2 ## ## Working Correlation[1:4,1:4] ## [,1] [,2] [,3] [,4] ## [1,] 1.0000000 0.9892949 0.9787045 0.9682274 ## [2,] 0.9892949 1.0000000 0.9892949 0.9787045 ## [3,] 0.9787045 0.9892949 1.0000000 0.9892949 ## [4,] 0.9682274 0.9787045 0.9892949 1.0000000 ## ## ## Returned Error Value: ## [1] 0 # Compare to oxboys.lm &lt;- lm(height ~ age, data=Oxboys) oxboys.lm ## ## Call: ## lm(formula = height ~ age, data = Oxboys) ## ## Coefficients: ## (Intercept) age ## 149.372 6.521 summary(oxboys.gee)$coef ## Estimate Naive S.E. Naive z Robust S.E. Robust z ## (Intercept) 149.719096 1.5531285 96.39840 1.5847569 94.47449 ## age 6.547328 0.3177873 20.60286 0.3042478 21.51972 summary(oxboys.lm)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 149.371801 0.5285648 282.598864 1.987406e-296 ## age 6.521022 0.8169867 7.981797 6.635134e-14 Standard errors under the GEE can also decrease! "],["linear_mixed_models.html", "Chapter 7 Linear mixed models 7.1 Random intercept models 7.2 Random slope models 7.3 The Linear Mixed Model (LMM) 7.4 Estimation of fixed effects 7.5 Inference for fixed effects 7.6 Prediction of random effects 7.7 Inference for random effects", " Chapter 7 Linear mixed models 7.1 Random intercept models 7.1.1 Example 7.1 (Oxford boys data) We revisit the Oxford boys data from Example 6.1 and 6.4. require(nlme) require(ggplot2) ggplot(data = Oxboys, aes(x = age, y = height, col = Subject)) + geom_line(size = 1.2, alpha = .8) + labs(title = &quot;Height vs. Age&quot;, subtitle=&quot;by subject&quot;)+ theme(legend.position = &quot;none&quot;) The growth of the boys appears to be governed by an (overall) linear trend with subject-specific intercepts. In this section we are interested in modelling these subject-specific effects explicitly, not just the marginal, population-averaged effects as in the previous section. In order to do this, one could consider the following modelling strategies: Option 1: Fit a traditional regression model into which we include as many levels as subjects. oxboys.int.lm &lt;- lm(height~age+Subject, data=Oxboys) oxboys.int.lm$coef ## (Intercept) age Subject.L Subject.Q Subject.C Subject^4 ## 149.3717351 6.5239272 38.2711944 -1.0176524 9.7862876 -0.2592347 ## Subject^5 Subject^6 Subject^7 Subject^8 Subject^9 Subject^10 ## 2.4811449 -1.7440210 -0.1519776 -0.8033613 0.0949727 -5.1229859 ## Subject^11 Subject^12 Subject^13 Subject^14 Subject^15 Subject^16 ## 1.4768831 -0.1076707 -1.4004605 1.5915615 -1.9752706 0.6604833 ## Subject^17 Subject^18 Subject^19 Subject^20 Subject^21 Subject^22 ## 1.3405697 2.0599463 1.7441624 -2.3586125 -1.3409881 1.3729270 ## Subject^23 Subject^24 Subject^25 ## 3.4359538 1.1353750 -1.8758449 oxboys.int.pred&lt;- predict(oxboys.int.lm) ggplot(data = Oxboys, aes(x = age, y = height)) + geom_point(aes(col=Subject)) + geom_line(aes(y=oxboys.int.pred, col=Subject))+ labs(title = &quot;Height vs. Age&quot;, subtitle=&quot;with subject-specific intercepts&quot;)+ theme(legend.position = &quot;none&quot;) While this seems to fit well, the approach does not appear very practicable, for two reasons: Firstly, one potentially needs very many parameters (one for each subject/cluster \\(i=1, \\ldots,n\\)). Secondly, the approach is useless for prediction of a new subject (since the intercept of that new subject will be unknown) Option 2: Consider the subject-specific intercepts to be drawn from a distribution centered at the overall intercept. This view implies a “hierarchical” model. One also speaks of a “two-level-model” (or more generally multilevel models), where however this notion of levels has nothing to do with the notion of levels of a factor! Specifically, one has: Lower level (observations/ repeated measurements): \\[ y_{ij}= a_i+ \\beta x_{ij}+ \\epsilon_{ij} \\,\\,\\mbox{ with } \\epsilon_{ij} \\sim N(0, \\sigma^2) \\] Upper level (clusters/ subjects): \\[ a_i= \\alpha + u_i \\,\\,\\mbox{ with } u_{i} \\sim N(0, \\sigma_u^2) \\] where all “random effects” \\(u_i\\) and model errors \\(\\epsilon_{ij}\\) are independent. (In the above, Greek letters indicate fixed effect parameters, and Latin letters, when indexed by \\(i\\) or \\(ij\\), random quantities). Random effect model with subjects-specific random intercepts and a singe covariate \\(x_{ij}=\\texttt{age}_{ij}\\): require(lme4) ## Loading required package: lme4 ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following object is masked from &#39;package:npmlreg&#39;: ## ## expand ## ## Attaching package: &#39;lme4&#39; ## The following object is masked from &#39;package:nlme&#39;: ## ## lmList oxboys.lmm &lt;- lmer(height ~ age + (1 | Subject), data=Oxboys) oxboys.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: height ~ age + (1 | Subject) ## Data: Oxboys ## REML criterion at convergence: 940.0297 ## Random effects: ## Groups Name Std.Dev. ## Subject (Intercept) 8.097 ## Residual 1.311 ## Number of obs: 234, groups: Subject, 26 ## Fixed Effects: ## (Intercept) age ## 149.372 6.524 oxboys.lmm.pred&lt;- predict(oxboys.lmm) # predict xi^T beta + zi_T u_i # will study later how u_i are predicted! # oxboys.lmm.marg&lt;- predict(oxboys.lmm, re.form=NA) # predict xi^T beta # corresponds to predicting the marginal model fit ggplot(data = Oxboys, aes(x = age, y = height)) + geom_point(aes(col=Subject)) + geom_line(aes(y=oxboys.lmm.pred, col=Subject))+ # geom_line(aes(y=oxboys.lmm.marg), lwd=3, colour=2)+ labs(title = &quot;Height vs. Age&quot;, subtitle=&quot;with subject-specific intercepts&quot;)+ theme(legend.position = &quot;none&quot;) Note that we can combine the two-level representation of the model displayed above into a single model (we also slightly generalize the notation here to allow for more than one covariate): \\[ \\begin{aligned} y_{ij}&amp;=&amp; \\alpha+ \\boldsymbol{\\beta}^T\\boldsymbol{x}_{ij}+ u_i + \\epsilon_{ij}\\\\ &amp;=&amp; \\alpha+ u_i + \\boldsymbol{\\beta}^T\\boldsymbol{x}_{ij}+ \\epsilon_{ij} \\end{aligned} \\] where the first representation is useful as it highlights the separation of the model into a “fixed part” (first two terms) and a “random part” (last two terms), and the second representation is useful because it highlights its role as a random intercept model. Also, it is interesting to look at the marginal effects of this model. Specifically, the marginal means are \\[ E(y_{ij}) = \\alpha+ \\boldsymbol{\\beta}^T\\boldsymbol{x}_{ij} \\] and the marginal variances are \\[ \\mbox{Var}(y_{ij}) = \\sigma_u^2+ \\sigma^2 \\] We see that fixed effects specify the marginal mean; random effects specify the marginal variance. Since models of the type above contain a mixture of fixed effect parameters and random effects, they are also often termed “mixed effect models”. What can we say about marginal covariances? \\[ \\begin{aligned} \\mbox{Cov}(y_{ij}, y_{ik})&amp;=&amp; E\\left((y_{ij}-E(y_{ij}))(y_{ik}-E(y_{ik}))\\right)\\\\ &amp;=&amp; E\\left((u_i+\\epsilon_{ij})(u_i+\\epsilon_{ik})\\right)\\\\ &amp;=&amp; E(u_i^2)+ E(u_i)E(\\epsilon_{ij})+E(u_i)E(\\epsilon_{ik})+E(\\epsilon_{ij})E(\\epsilon_{ik})\\\\ &amp;=&amp; \\sigma_u^2 \\end{aligned} \\] This implies also \\[ \\mbox{Corr}(y_{ij}, y_{ik})= \\frac{\\sigma_u^2}{\\sigma_u^2+\\sigma^2} \\] This quantity is commonly known as the intra-class-correlation. It can be interpreted as the proprtion of “total” variation explained by the cluster structure. Alternatively it can be interpreted as the correlation of two items randomly drawn from the same cluster. In this context, one can show easily that \\[ \\mbox{Corr}(y_{ij}, y_{i^{\\prime}k})=0 \\,\\, \\mbox{ for } i \\not= i^{\\prime} \\] that is, observations from different clusters are uncorrelated. It is clear from the equations above that covariates have not played a role in this derivation. In fact, it is common to compute the ICC for a model which does not contain any fixed-effect parameters at all, i.e. \\(\\boldsymbol{\\beta}\\equiv 0\\). However, while the equation for ICC is then unchanged, the estimates of \\(\\sigma^2\\) and \\(\\sigma_u^2\\) may still be different for the “empty random intercept” model \\(y_{ij}=\\alpha+u_i+\\epsilon_{ij}\\) and the “random intercept model with fixed effect covariates”. In the literature, ICCs which are based on the empty model (without fixed effect covariates) are sometimes called unconditional ICC, and the ones including fixed effects “conditional ICC”, with terminology not being entirely consistent across resources. ICCs are often the “first shot” when assessing whether or not a repeated measure structure needs to be explicitly addressed through a (say) two-level model. 7.1.2 Example 7.2 (Oxford boys data) We compute the intra-class correlation for this data set; firstly “conditional” oxboys.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: height ~ age + (1 | Subject) ## Data: Oxboys ## REML criterion at convergence: 940.0297 ## Random effects: ## Groups Name Std.Dev. ## Subject (Intercept) 8.097 ## Residual 1.311 ## Number of obs: 234, groups: Subject, 26 ## Fixed Effects: ## (Intercept) age ## 149.372 6.524 oxboys.v &lt;- as.data.frame(summary(oxboys.lmm)$varcor) oxboys.v ## grp var1 var2 vcov sdcor ## 1 Subject (Intercept) &lt;NA&gt; 65.554956 8.096602 ## 2 Residual &lt;NA&gt; &lt;NA&gt; 1.718066 1.310750 icc&lt;- oxboys.v[1,4]/(oxboys.v[1,4]+oxboys.v[2,4]) icc ## [1] 0.9744613 Then “unconditional”: oxboys.int_only.lmm &lt;- lmer(height ~ (1 | Subject), data=Oxboys) oxboys.int_only.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: height ~ (1 | Subject) ## Data: Oxboys ## REML criterion at convergence: 1466.59 ## Random effects: ## Groups Name Std.Dev. ## Subject (Intercept) 7.957 ## Residual 4.661 ## Number of obs: 234, groups: Subject, 26 ## Fixed Effects: ## (Intercept) ## 149.5 oxboys.int_only.v&lt;- as.data.frame(summary(oxboys.int_only.lmm)$varcor) icc&lt;- oxboys.int_only.v[1,4]/(oxboys.int_only.v[1,4]+oxboys.int_only.v[2,4]) icc ## [1] 0.7445153 Automated (confusing output!): require(performance) ## Loading required package: performance icc(oxboys.lmm) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.974 ## Unadjusted ICC: 0.770 icc(oxboys.int_only.lmm) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.745 ## Unadjusted ICC: 0.745 7.2 Random slope models What is if not only the intercept, but also the slopes subject-specific? For ease of presentation, let us now just consider a single covariate \\(x_{ij}\\). In this case, we have lower level: \\[ y_{ij}= a_i+b_i x_{ij}+\\epsilon_{ij} \\] upper level: \\[ \\begin{aligned} a_i &amp;= &amp; \\alpha + u_i\\\\ b_i &amp;= &amp; \\beta + v_i \\end{aligned} \\] where \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\), \\(u_i \\sim N(0, \\sigma^2_u)\\), \\(v_i \\sim (0,\\sigma^2_v)\\) are independent. Combined this gives \\[ y_{ij}= \\alpha+\\beta x_{ij}+ u_i+v_ix_{ij}+\\epsilon_{ij} \\] where \\(\\alpha+\\beta x_{ij}\\) is the fixed part and \\(u_i+v_ix_{ij}+\\epsilon_{ij}\\) the random part of the model. Marginally this implies \\[ \\begin{aligned} E(y_{ij})&amp;=&amp; \\alpha+\\beta x_{ij}\\\\ \\mbox{Var}(y_{ij})&amp;=&amp; \\sigma^2+ \\sigma_u^2+\\sigma_v^2x_{ij}^2+2r\\sigma_u\\sigma_vx_{ij} \\end{aligned} \\] where we define \\(r=\\mbox{Corr}(u_j,v_j)\\) which is sometimes assumed to be 0 [Dobson, p. 221]. 7.2.1 Example 7.3 (Oxford boys) require(lme4) oxboys.slope.lmm &lt;- lmer(height ~ age + (age | Subject), data=Oxboys) oxboys.slope.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: height ~ age + (age | Subject) ## Data: Oxboys ## REML criterion at convergence: 724.091 ## Random effects: ## Groups Name Std.Dev. Corr ## Subject (Intercept) 8.0811 ## age 1.6807 0.64 ## Residual 0.6599 ## Number of obs: 234, groups: Subject, 26 ## Fixed Effects: ## (Intercept) age ## 149.372 6.525 oxboys.slope.lmm.pred&lt;- predict(oxboys.slope.lmm) # oxboys.slope.lmm.marg&lt;- predict(oxboys.slope.lmm, re.form=NA) So, here \\(r=0.64\\). ggplot(data = Oxboys, aes(x = age, y = height)) + geom_point(aes(col=Subject)) + geom_line(aes(y=oxboys.slope.lmm.pred, col=Subject))+ # geom_line(aes(y=oxboys.slope.lmm.marg), lwd=3, colour=2)+ labs(title = &quot;Height vs. Age&quot;, subtitle=&quot;with subject-specific intercepts and slopes&quot;)+ theme(legend.position = &quot;none&quot;) 7.3 The Linear Mixed Model (LMM) General framework encompassing all previous models (but still only Gaussian response): \\[ \\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon} \\] where we have \\[ \\begin{aligned} \\boldsymbol{Y}= (y_{11}, \\ldots, y_{1n_1}, y_{21}, \\ldots, y_{2n_2}, \\ldots, y_{n1}, \\ldots, y_{nn_n})^T \\in \\mathbb{R}^N\\\\ \\boldsymbol{\\epsilon}= (\\epsilon_{11}, \\ldots, \\epsilon_{1n_1}, \\epsilon_{21}, \\ldots, \\epsilon_{2n_2}, \\ldots, \\epsilon_{n1}, \\ldots, \\epsilon_{nn_n})^T \\in \\mathbb{R}^N \\end{aligned} \\] where we we recall that \\(N=\\sum_{i=1}^n n_i\\), and \\(p\\) fixed effects; i.e. \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\), with design matrix \\(\\boldsymbol{X} \\in \\mathbb{R}^{N \\times p}\\); \\(q\\) random effects, i.e. \\(\\boldsymbol{u}= (\\tilde{\\boldsymbol{u}}_1, \\ldots,\\tilde{\\boldsymbol{u}}_n )^T\\) with \\(\\tilde{\\boldsymbol{u}}_1 \\in \\mathbb{R}^q\\), and random-efffects design matrix \\(\\boldsymbol{Z} \\in \\mathbb{R}^{N \\times nq}\\). 7.3.1 Example 7.4 For the “empty model” with random intercept, \\[ y_{ij}= \\alpha+u_i+\\epsilon_{ij}, \\] one has \\[ \\begin{aligned} \\boldsymbol{\\beta}&amp;=&amp;\\alpha,\\\\ \\boldsymbol{u}&amp;=&amp;(u_1, \\ldots, u_n)^T,\\\\ \\boldsymbol{X}&amp;=&amp;(1, \\ldots, 1)^T,\\\\ \\end{aligned} \\] as well as \\[ \\boldsymbol{Z}= \\left(\\begin{array}{ccc} 1 &amp; &amp; \\\\ \\vdots &amp; &amp; \\\\ 1 &amp; &amp; \\\\ &amp;\\ddots &amp; \\\\ &amp; &amp;1 \\\\ &amp; &amp; \\vdots \\\\ &amp; &amp; 1 \\\\ \\end{array}\\right). \\] For the random intercept model with fixed slope, \\[ y_{ij}= \\alpha+\\boldsymbol{\\beta}x_{ij} +u_i+\\epsilon_{ij}, \\] one has \\[ \\begin{aligned} \\boldsymbol{\\beta}&amp;=&amp; \\left(\\begin{array}{c} \\alpha \\\\ \\beta \\end{array}\\right),\\\\ \\boldsymbol{u}&amp;=&amp; (u_1, \\ldots, u_n)^T,\\\\ \\boldsymbol{X}&amp;=&amp; \\left(\\begin{array}{cc} 1 &amp; x_{11} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{nn_n}\\\\ \\end{array}\\right) \\end{aligned} \\] and \\(\\boldsymbol{Z}\\) as above. For the random slope model, \\[ y_{ij}= \\alpha+\\beta x_{ij}+u_i+ v_ix_{ij}+\\epsilon_{ij}, \\] one has \\[ \\begin{aligned} \\boldsymbol{\\beta}&amp;=&amp; \\left(\\begin{array}{c} \\alpha \\\\ \\beta \\end{array}\\right),\\\\ \\boldsymbol{u}&amp;=&amp; (u_1, v_1, \\ldots, u_n, v_n)^T\\\\ \\end{aligned} \\] \\(\\boldsymbol{X}\\) as above, and \\[ \\boldsymbol{Z}= \\left(\\begin{array}{ccccc} 1 &amp; x_{11} &amp; &amp; &amp; \\\\ 1 &amp; x_{1n_1} &amp; &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; &amp; \\\\ &amp; &amp; &amp; 1 &amp; x_{n1}\\\\ &amp; &amp; &amp; 1 &amp; x_{nn_n} \\end{array}\\right). \\] In the LMM, one (commonly) assumes: \\[ \\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2\\boldsymbol{I}_N) \\] (this is because correlation structures have already been induced by the random effect, hence there will be rarely a reason to make further specifications of such correlations) and \\[ \\boldsymbol{u}\\sim N(0, \\boldsymbol{Q}) \\] where \\(\\boldsymbol{\\epsilon}\\) and \\(\\boldsymbol{u}\\) are independent, and \\[ \\boldsymbol{Q}= \\mbox{Var}(\\boldsymbol{u})= \\left( \\begin{array}{ccc} \\mbox{Var}(\\tilde{\\boldsymbol{u}}_1) &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\mbox{Var}(\\tilde{\\boldsymbol{u}}_n) \\end{array} \\right)= \\left( \\begin{array}{ccc} \\tilde{\\boldsymbol{Q}} &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\tilde{\\boldsymbol{Q}} \\end{array} \\right) \\] That is, the \\(\\tilde{\\boldsymbol{Q}}\\) is the variance matrix of the random effects of the \\(i\\)th cluster (which usually does not depend \\(i\\)). This implies marginally \\[ \\begin{aligned} E(\\boldsymbol{Y})&amp;=&amp; \\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{Z}E(\\boldsymbol{u})+E(\\boldsymbol{\\epsilon})= \\boldsymbol{X}\\boldsymbol{\\beta}\\\\ \\mbox{Var}(\\boldsymbol{Y})&amp;=&amp; \\boldsymbol{Z}\\mbox{Var}(\\boldsymbol{u}) \\boldsymbol{Z}^T+\\sigma^2\\boldsymbol{I}_N= \\boldsymbol{Z}\\boldsymbol{Q} \\boldsymbol{Z}^T+\\sigma^2\\boldsymbol{I}_N \\end{aligned} \\] Summarizing, this gives us the ``structured” marginal variance matrix \\[ \\boldsymbol{\\Sigma}= \\mbox{Var}(\\boldsymbol{Y})= \\boldsymbol{Z}\\boldsymbol{Q}\\boldsymbol{Z}^T+ \\sigma^2\\boldsymbol{I}_N. \\] 7.4 Estimation of fixed effects Recall our modelling framework: \\[ \\begin{aligned} \\boldsymbol{u} &amp;\\sim&amp; N(0, \\boldsymbol{Q}),\\\\ \\boldsymbol{Y}|\\boldsymbol{u} &amp; \\sim &amp; N(\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{Z}\\boldsymbol{u}, \\sigma^2\\boldsymbol{I}_N),\\\\ \\boldsymbol{Y}&amp;\\sim&amp; N(\\boldsymbol{X}\\boldsymbol{\\beta},\\boldsymbol{Z}\\boldsymbol{Q}\\boldsymbol{Z}^T+ \\sigma^2\\boldsymbol{I}_N ) \\end{aligned} \\] Denote the set of variance parameters (“variance components”) by \\(\\boldsymbol{\\gamma}= \\{\\boldsymbol{Q}, \\sigma^2\\}\\). Then \\(\\boldsymbol{\\Sigma}=\\boldsymbol{\\Sigma}(\\boldsymbol{\\gamma})\\); i.e. \\(\\boldsymbol{\\Sigma}\\) is only fully known when \\(\\boldsymbol{\\gamma}\\) is known. For the estimation of the fixed effect parameters \\(\\boldsymbol{\\beta}\\), we distinguish several cases: \\(\\boldsymbol{\\gamma}\\) known (hence \\(\\boldsymbol{\\Sigma}\\) known). Then the solution is the same as for the marginal model with fixed \\(\\boldsymbol{\\Sigma}\\), i.e. \\[ \\hat{\\boldsymbol{\\beta}}=(\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{Y} \\] Note that this is just the solution corresponding to the GEE \\(\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{\\mu})=0\\) with known (and correctly specified) variance matrix \\(\\boldsymbol{\\Sigma}\\) (noting that in the current context we have \\(\\boldsymbol{D}=\\boldsymbol{I}\\) since our setup is fully Gaussian, without link functions). If \\(\\boldsymbol{\\gamma}\\) is unknown, a possible approach is to estimate it through maximization of the (marginal) likelihood \\[ L^*(\\boldsymbol{\\beta}, \\boldsymbol{\\gamma}) \\propto \\frac{1}{|\\boldsymbol{\\Sigma}|^{1/2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{\\beta})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\gamma})(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{\\beta})\\right) \\] In order to maximize this likelihood, one typically employs a profile-likelihood-type approach. Consider therefore the same estimator as in case 1 above, but evaluated at \\(\\boldsymbol{\\gamma}\\), i.e. \\[ \\hat{\\boldsymbol{\\beta}}(\\boldsymbol{\\gamma})=(\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\gamma})\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\gamma})\\boldsymbol{Y} \\] which can be plugged into \\(L^*(\\boldsymbol{\\beta}, \\boldsymbol{\\gamma})\\), yielding \\[ L(\\boldsymbol{\\gamma})= L^*(\\hat{\\boldsymbol{\\beta}}(\\boldsymbol{\\gamma}),\\boldsymbol{\\gamma} ) \\] Maximizing \\(L(\\boldsymbol{\\gamma})\\) w.r.t. \\(\\boldsymbol{\\gamma}\\) yields \\[ \\hat{\\boldsymbol{\\gamma}}_{ML} = \\mbox{arg max}_{\\boldsymbol{\\gamma}}L(\\boldsymbol{\\gamma}). \\] REML estimation addresses the following problem in the ML solution: Just like, in the linear model (LM), the Maximum Likelihood estimate of the error variance \\(\\sigma^2\\) is biased, in the LMM the estimator \\(\\hat{\\boldsymbol{\\gamma}}_{ML}\\) is biased for \\(\\boldsymbol{\\gamma}\\), due to a “loss” of degress of freedom in the estimation of \\(\\boldsymbol{\\beta}\\). The idea of REML (Restricted Maximum Likelihood estimation) is to multiply the model equation \\(\\boldsymbol{Y}=\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{Z}\\boldsymbol{u}+\\boldsymbol{\\epsilon}\\) by any matrix \\(\\boldsymbol{A}\\) which is orthogonal to \\(\\boldsymbol{X}\\), i.e. \\(\\boldsymbol{A}^T\\boldsymbol{X}=0\\), then \\[ \\boldsymbol{A}^T\\boldsymbol{Y}= \\boldsymbol{A}^T\\boldsymbol{Z}\\boldsymbol{u}+ \\boldsymbol{A}^T\\boldsymbol{\\epsilon} \\] and based on this one can find a likelihood (of \\(\\boldsymbol{A}^T\\boldsymbol{Y}\\)) which does not depend on \\(\\boldsymbol{\\beta}\\) [FT page 290/291]. This “restricted likelihood” (the logarithm of which is called REML criterion in lmer output) does not depend on \\(\\boldsymbol{A}\\) and takes the shape \\[ L_{REML}(\\boldsymbol{\\gamma}) \\propto|\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\gamma})\\boldsymbol{X}|^{-1/2}L(\\boldsymbol{\\gamma}) \\] Then the REML estimator of \\(\\gamma\\) is \\[ \\hat{\\boldsymbol{\\gamma}}_{REML}= \\mbox{arg max}_{\\boldsymbol{\\gamma}}L_{REML}(\\boldsymbol{\\gamma}). \\] 7.4.1 Example 7.5 REML and ML estimates for Oxboys data oxboys.slope.lmm &lt;- lmer(height ~ age + (age | Subject), data=Oxboys) oxboys.slope.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: height ~ age + (age | Subject) ## Data: Oxboys ## REML criterion at convergence: 724.091 ## Random effects: ## Groups Name Std.Dev. Corr ## Subject (Intercept) 8.0811 ## age 1.6807 0.64 ## Residual 0.6599 ## Number of obs: 234, groups: Subject, 26 ## Fixed Effects: ## (Intercept) age ## 149.372 6.525 oxboys.slope.lmm.ml &lt;- lmer(height ~ age + (age | Subject), data=Oxboys, REML=FALSE) oxboys.slope.lmm.ml ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: height ~ age + (age | Subject) ## Data: Oxboys ## AIC BIC logLik deviance df.resid ## 737.9677 758.6996 -362.9838 725.9677 228 ## Random effects: ## Groups Name Std.Dev. Corr ## Subject (Intercept) 7.9240 ## age 1.6467 0.64 ## Residual 0.6599 ## Number of obs: 234, groups: Subject, 26 ## Fixed Effects: ## (Intercept) age ## 149.372 6.525 REML and ML estimates for Mathematics achievement data load(&quot;Datasets/sub_hsb.RData&quot;) school.id &lt;- as.factor(sub_hsb$schid) hsb.lmm&lt;- lmer(mathach~ses+ (1|school.id), data=sub_hsb) hsb.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: mathach ~ ses + (1 | school.id) ## Data: sub_hsb ## REML criterion at convergence: 8601.028 ## Random effects: ## Groups Name Std.Dev. ## school.id (Intercept) 2.518 ## Residual 6.010 ## Number of obs: 1329, groups: school.id, 30 ## Fixed Effects: ## (Intercept) ses ## 12.89 2.12 hsb.lmm.ml&lt;- lmer(mathach~ses+ (1|school.id), data=sub_hsb, REML=FALSE) hsb.lmm.ml ## Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] ## Formula: mathach ~ ses + (1 | school.id) ## Data: sub_hsb ## AIC BIC logLik deviance df.resid ## 8608.516 8629.284 -4300.258 8600.516 1325 ## Random effects: ## Groups Name Std.Dev. ## school.id (Intercept) 2.462 ## Residual 6.008 ## Number of obs: 1329, groups: school.id, 30 ## Fixed Effects: ## (Intercept) ses ## 12.886 2.131 7.5 Inference for fixed effects Recall that, for known \\(\\boldsymbol{\\Sigma}= \\boldsymbol{Z}\\boldsymbol{Q}\\boldsymbol{Z}^T+ \\sigma^2\\boldsymbol{I}_N\\), one has \\[ \\hat{\\boldsymbol{\\beta}}(\\boldsymbol{\\gamma})=(\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{Y} \\] which means that \\[ \\begin{aligned} \\mbox{Var}(\\hat{\\boldsymbol{\\beta}})&amp;=&amp; (\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\mbox{Var}(\\boldsymbol{Y})\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X}(\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1}\\\\ &amp;=&amp; (\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1} \\end{aligned} \\] However, if \\(\\boldsymbol{\\Sigma}= \\boldsymbol{\\Sigma}(\\boldsymbol{\\gamma})\\) needs to be estimated by \\(\\hat{\\boldsymbol{\\Sigma}}= \\boldsymbol{\\Sigma}(\\hat{\\boldsymbol{\\gamma}})\\), this variance estimator can be poor. Therefore, it has been suggested in the literature to use the sandwich variance estimator (as we have seen for GEEs in Section 6.3) also here. However, R function lmer does not actually do this. The LMM implementation in SAS does. Asymptotic normality and unbiasedness of the \\(\\hat{\\boldsymbol{\\beta}}\\) hold approximately. So, for some fixed effects coefficient \\(\\beta_j\\), \\(j=1, \\ldots, p\\), p-values for \\(H_0: \\beta_j=0\\) can be approximately based on t-values \\(\\hat{\\beta}_j/SE(\\hat{\\beta}_j)\\); \\(\\hat{\\beta}_j \\pm z_{\\alpha/2} SE(\\hat{\\beta}_j)\\) will give reasonable confidence intervals, where \\(z_{\\alpha/2}\\) is the right-hand tail \\(\\alpha/2\\) quantile of the standard normal distribution. 7.5.1 Example 7.6 (Mathematics achievement data) We consider the random intercept model for mathematics achievement with fixed effect for socioeconomic status (SES) as fitted in Example 7.5: summary(hsb.lmm) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: mathach ~ ses + (1 | school.id) ## Data: sub_hsb ## ## REML criterion at convergence: 8601 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.81268 -0.70959 -0.03616 0.76678 2.74101 ## ## Random effects: ## Groups Name Variance Std.Dev. ## school.id (Intercept) 6.339 2.518 ## Residual 36.119 6.010 ## Number of obs: 1329, groups: school.id, 30 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 12.8865 0.4908 26.256 ## ses 2.1202 0.2536 8.359 ## ## Correlation of Fixed Effects: ## (Intr) ## ses -0.010 The t-value for the fixed effect slope ses is given by \\(2.1202/0.2536=8.359\\), which is clearly \\(&gt;&gt;2\\) and hence significantly different from 0 at the \\(5\\%\\) (or any other reasonable) level of significance. We can easily obtain an approximate 95% confidence interval for the fixed effect coefficient ses: CI &lt;- 2.1202+ qnorm(0.975) *c(-1,1)*0.2536 CI ## [1] 1.623153 2.617247 # confint(hsb.lmm) However, as stated the methods mentioned above have only approximate character. The more principled approach is to use Likelihood ratio (LR)– based methods. Therefore, assume there is a smaller model \\(M_0\\) and a larger model \\(M_1\\), in the sense that the smaller model is nested in the larger model \\(M_1\\), but with the only difference being in the fixed effects. Let us further denote the likelihoods (of the fitted models, evaluated at the respective MLEs) by \\(L_0\\) and \\(L_1\\) respectively, so that clearly \\(L_0&lt;L_1\\). Finally, let \\(D_i=-2\\log L_i+c\\), with \\(c\\) denoting a constant depending on the saturated likelihood. Then \\[ D_0-D_1= -2 \\log L_0-2\\log L_1 = -2 \\log \\frac{L_0}{L_1} \\sim \\chi^2(df) \\] where \\(df\\) is the difference in the number of fixed effect parameters of the two models (it is not allowed here to have a difference in the number of random effect parameters). That is, for the test problem \\[ H_0: M_0\\,\\,\\, \\mbox{ versus }\\,\\,\\, H_1: M_1 \\] one needs to fit both models and then reject \\(H_0\\) if \\[ D_0-D_1&gt; \\chi^2_{\\alpha}(df) \\] where \\(\\chi^2_{\\alpha}(df)\\) is the right-tail \\(\\alpha\\) quantile of the \\(\\chi^2\\) distribution with \\(df\\) degrees of freedom. Consider now the problem of finding a \\(1-\\alpha\\) confidence interval (or region) for some fixed effect parameters \\(\\boldsymbol{\\beta}\\) (We may be interested in the whole parameter vector, or a subset of it, or just a single coefficient. We assume that \\(k \\le p\\) parameters are needed to estimate \\(\\hat{\\boldsymbol{\\beta}}\\).) Then we find the confidence interval (region) by identifying the range of \\(\\boldsymbol{\\beta}\\) values for which \\[ \\log L(\\boldsymbol{\\beta})\\ge \\log L(\\hat{\\boldsymbol{\\beta}})- \\frac{1}{2}\\chi^2_{\\alpha}(k) \\] where \\[ L(\\boldsymbol{\\beta})= L^*(\\boldsymbol{\\beta}, \\gamma(\\boldsymbol{\\beta})) \\] (Such a function does not really exist, it is evaluated by software, purely computationally). 7.5.2 Example 7.7 (Mathematics achievement data) We are interested in testing \\(H_0\\): “no linear trend for SES” versus \\(H_1\\): “There is a linear trend for SES”. To carry out the test, we need to fit both models (the one under the alternative is already available, via hsb.lmm) and find the difference in deviances. hsb.flat.lmm&lt;- lmer(mathach~1+ (1|school.id), data=sub_hsb) anova(hsb.flat.lmm, hsb.lmm) ## refitting model(s) with ML (instead of REML) ## Data: sub_hsb ## Models: ## hsb.flat.lmm: mathach ~ 1 + (1 | school.id) ## hsb.lmm: mathach ~ ses + (1 | school.id) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## hsb.flat.lmm 3 8670.6 8686.2 -4332.3 8664.6 ## hsb.lmm 4 8608.5 8629.3 -4300.3 8600.5 64.102 1 1.182e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can also obtain the confidence intervals via confint(hsb.lmm) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## .sig01 1.830620 3.380955 ## .sigma 5.783941 6.246918 ## (Intercept) 11.910502 13.864699 ## ses 1.613066 2.649187 (This also gives confidence intervals for the random effects but we have not studied these methods yet.) Addendum: Comparison of LMM to GEE # require(gee) # hsb.gee &lt;- gee(mathach~ses, data=sub_hsb, id=school.id, corstr = &quot;exchangeable&quot;) # summary(hsb.gee)$coef # summary(hsb.lmm)$coef 7.6 Prediction of random effects Recall again \\[ \\begin{aligned} \\boldsymbol{u} &amp;\\sim&amp; N(0, \\boldsymbol{Q}),\\\\ \\boldsymbol{Y}|\\boldsymbol{u} &amp; \\sim &amp; N(\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{Z}\\boldsymbol{u}, \\sigma^2\\boldsymbol{I}_N),\\\\ \\boldsymbol{Y}&amp;\\sim&amp; N(\\boldsymbol{X}\\boldsymbol{\\beta},\\boldsymbol{Z}\\boldsymbol{Q}\\boldsymbol{Z}^T+ \\sigma^2\\boldsymbol{I}_N ) \\end{aligned} \\] where \\(\\boldsymbol{u} \\in \\mathbb{R}^{nq}\\) contains all random effects, so for instance in the case of the random intercept model, one has \\(\\boldsymbol{u}=(u_1, \\ldots, u_n)^T\\), with \\(q=1\\). What can we say about the distribution of \\(\\boldsymbol{u}|\\boldsymbol{Y}\\)? In principle, this is fully available from Bayes’ theorem, \\[ f(\\boldsymbol{u}|\\boldsymbol{Y})= \\frac{f(\\boldsymbol{Y}|\\boldsymbol{u})f(\\boldsymbol{u})}{\\int (\\boldsymbol{Y}|\\boldsymbol{u})f(\\boldsymbol{u})d\\boldsymbol{u}} \\] In order to work out this posterior, we get help by a general result: If \\[ \\left(\\begin{array}{c}\\boldsymbol{y}_1 \\\\ \\boldsymbol{y}_2 \\end{array} \\right) \\sim N\\left(\\left(\\begin{array}{c}\\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{array} \\right), \\left(\\begin{array}{cc}\\boldsymbol{\\Sigma}_{11} &amp; \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21} &amp; \\boldsymbol{\\Sigma}_{22} \\end{array} \\right) \\right) \\] then \\[ \\boldsymbol{y}_1|\\boldsymbol{y}_2 \\sim N\\left(\\boldsymbol{\\mu}_1+ \\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1} (\\boldsymbol{y}_2- \\boldsymbol{\\mu}_2), \\boldsymbol{\\Sigma}_{11}- \\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_{22}^{-1}\\boldsymbol{\\Sigma}_{21} \\right) \\] so here with \\[ \\left(\\begin{array}{c}\\boldsymbol{u} \\\\ \\boldsymbol{Y} \\end{array} \\right) \\sim N\\left(\\left(\\begin{array}{c}0 \\\\ \\boldsymbol{X}\\boldsymbol{\\beta} \\end{array} \\right), \\left(\\begin{array}{cc}\\boldsymbol{Q} &amp; \\boldsymbol{C} \\\\ \\boldsymbol{C}^T &amp; \\boldsymbol{\\Sigma} \\end{array} \\right) \\right) \\] we obtain \\[ \\boldsymbol{u}|\\boldsymbol{Y}\\sim N\\left(\\boldsymbol{C}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{\\beta}), \\boldsymbol{Q}- \\boldsymbol{C}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{C}^T \\right) \\] with a covariance matrix \\(\\boldsymbol{C}\\) that we will work out later. So in summary we can predict \\(\\boldsymbol{u}\\) by \\[ E(\\boldsymbol{u}|\\boldsymbol{Y})= \\boldsymbol{C}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{\\beta}). \\] It remains to work out \\(\\boldsymbol{C}=\\mbox{Cov}(\\boldsymbol{u}, \\boldsymbol{Y}) \\in \\mathbb{R}^{nq \\times N}\\). Here again we make use of a general result. According to the law of total covariance, one has for any random vectors \\(\\boldsymbol{x}\\), \\(\\boldsymbol{y}\\), \\(\\boldsymbol{z}\\), \\[ \\mbox{Cov}(\\boldsymbol{x}, \\boldsymbol{y})= E\\left(\\mbox{Cov}(\\boldsymbol{x},\\boldsymbol{y})|\\boldsymbol{z}\\right)+ \\mbox{Cov}(E(\\boldsymbol{x}|\\boldsymbol{z}), E\\left(\\boldsymbol{y}| \\boldsymbol{z})\\right) \\] so when using \\(\\boldsymbol{z}=\\boldsymbol{y}\\) this gives \\[ \\mbox{Cov}(\\boldsymbol{x}, \\boldsymbol{y})= E\\left(\\mbox{Cov}(\\boldsymbol{x},\\boldsymbol{y})|\\boldsymbol{y})\\right)+ \\mbox{Cov}(E(\\boldsymbol{x}|\\boldsymbol{y}), E\\left(\\boldsymbol{y}| \\boldsymbol{y})\\right)= \\mbox{Cov}(E(\\boldsymbol{x}|\\boldsymbol{y}), \\boldsymbol{y}) \\] i.e. \\[ \\begin{aligned} \\mbox{Cov}(\\boldsymbol{Y},\\boldsymbol{u} )&amp;=&amp; \\mbox{Cov}(E(\\boldsymbol{Y}|\\boldsymbol{u}), \\boldsymbol{u})=\\mbox{Cov}(\\boldsymbol{X}\\boldsymbol{\\beta}+\\boldsymbol{Z}\\boldsymbol{u}, \\boldsymbol{u}) \\\\ &amp;=&amp;\\mbox{Cov}(\\boldsymbol{X}\\boldsymbol{\\beta}, \\boldsymbol{u})+ \\mbox{Cov}(\\boldsymbol{Z}, \\boldsymbol{u})= \\boldsymbol{Z}\\mbox{Cov}(\\boldsymbol{u},\\boldsymbol{u})\\\\ &amp;=&amp; \\boldsymbol{Z}\\mbox{Var}(\\boldsymbol{u})= \\boldsymbol{Z}\\boldsymbol{Q} \\end{aligned} \\] i.e. \\(\\boldsymbol{C}^T= \\boldsymbol{Z}\\boldsymbol{Q}\\) and \\(\\boldsymbol{C}^T= \\boldsymbol{Q}\\boldsymbol{Z}^T\\). Putting everything together we obtain \\[ E(\\boldsymbol{u}|\\boldsymbol{Y})= \\boldsymbol{Q} \\boldsymbol{Z}^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{\\beta}). \\] Now recall that \\(\\boldsymbol{\\Sigma}= \\boldsymbol{Z}\\boldsymbol{Q}\\boldsymbol{Z}^T+\\sigma^2\\boldsymbol{I}_N= \\boldsymbol{\\Sigma}(\\boldsymbol{\\gamma})\\) with \\(\\boldsymbol{\\gamma}= \\{\\boldsymbol{Q},\\boldsymbol{\\sigma^2} \\}\\). If \\(\\boldsymbol{\\gamma}\\) is known and hence \\(\\boldsymbol{Q}\\) and \\(\\boldsymbol{\\Sigma}\\) known, then plugging \\(\\hat{\\boldsymbol{\\beta}}=(\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{Y}\\) into the expression for \\(E(\\boldsymbol{u}|\\boldsymbol{Y})\\) is called the Best linear unbiased predictor (BLUP) of \\(\\boldsymbol{u}\\), \\[ \\hat{\\boldsymbol{u}}= \\boldsymbol{Q} \\boldsymbol{Z}^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}). \\] If \\(\\boldsymbol{Q}\\) and \\(\\boldsymbol{\\Sigma}\\) are unknown they can be replaced by estimates, \\(\\boldsymbol{\\hat{Q}}\\) and \\(\\hat{\\boldsymbol{\\Sigma}}\\), resulting in \\[ \\hat{\\boldsymbol{u}}= \\hat{\\boldsymbol{Q}} \\boldsymbol{Z}^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{Y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}) \\] which is often still called BLUP (despite not being necessarily unbiased) and which we therefore do not distinguish notationally. Details can be found in McCulloch et al (2008): Generalized, linear, and mixed models. Wiley. Based on the predicted random effects \\(\\hat{\\boldsymbol{u}}\\), we can also straightforwardly define and compute fitted values \\[ \\hat{\\boldsymbol{Y}}= \\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}+ \\boldsymbol{Z}\\hat{\\boldsymbol{u}} \\] and residuals \\[ \\hat{\\boldsymbol{\\epsilon}}= \\boldsymbol{Y}-\\hat{\\boldsymbol{Y}}= \\boldsymbol{Y}-\\boldsymbol{X}\\hat{\\boldsymbol{\\beta}}- \\boldsymbol{Z}\\hat{\\boldsymbol{u}}. \\] 7.6.1 Example 7.8 (Mathematics achievement data) Random effects (predicted via BLUP) can be extracted from the fitted model via ranef: #hsb.lmm hsb.ran&lt;- ranef(hsb.lmm) hsb.ran ## $school.id ## (Intercept) ## 1224 -2.00682516 ## 1288 0.29842627 ## 1296 -3.88702782 ## 1308 1.75072442 ## 1317 -0.39423508 ## 1358 -1.37706846 ## 1374 -2.60181956 ## 1433 4.57777678 ## 1436 3.56937021 ## 1461 2.14874621 ## 1462 -0.88339941 ## 1477 0.91911536 ## 1499 -3.82691109 ## 1637 -3.60587318 ## 1906 1.81658824 ## 1909 0.83990262 ## 1942 3.15791804 ## 1946 0.01160886 ## 2030 -1.34191054 ## 2208 1.48034521 ## 2277 -1.80488696 ## 2305 -0.38458386 ## 2336 2.40354897 ## 2458 0.56018478 ## 2467 -1.83847667 ## 2526 3.15768920 ## 2626 0.56310879 ## 2629 2.10286933 ## 2639 -3.72142061 ## 2651 -1.68348489 ## ## with conditional variances for &quot;school.id&quot; plot(hsb.ran) ## $school.id A bit nicer: require(sjPlot) ## Loading required package: sjPlot plot_model(hsb.lmm, type=&quot;re&quot;, transform=NULL) Repeat for random slope model hsb.slope.lmm&lt;- lmer(mathach~ses+ (ses|school.id), data=sub_hsb) hsb.slope.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: mathach ~ ses + (ses | school.id) ## Data: sub_hsb ## REML criterion at convergence: 8593.115 ## Random effects: ## Groups Name Std.Dev. Corr ## school.id (Intercept) 2.546 ## ses 1.253 0.04 ## Residual 5.951 ## Number of obs: 1329, groups: school.id, 30 ## Fixed Effects: ## (Intercept) ses ## 12.731 2.247 plot(ranef(hsb.slope.lmm)) ## $school.id plot_model(hsb.slope.lmm, type=&quot;re&quot;, transform=NULL) 7.7 Inference for random effects Recall \\(\\boldsymbol{Y}= \\boldsymbol{X}\\boldsymbol{\\beta}+ \\boldsymbol{Z}\\boldsymbol{u}+ \\boldsymbol{\\epsilon}\\) where \\[ \\boldsymbol{u}= \\left(\\begin{array}{c} \\tilde{\\boldsymbol{u}_1}\\\\ \\vdots \\\\ \\tilde{\\boldsymbol{u}_n} \\end{array}\\right) \\] with \\(\\tilde{\\boldsymbol{u}}_i = (u_i, v_i, \\ldots, )^T\\) comprising of the \\(q\\) random effects for cluster \\(i\\), with \\[ \\begin{aligned} u_i &amp;\\sim&amp; N(0, \\sigma_u^2)\\\\ v_i &amp; \\sim &amp; N(0, \\sigma_v^2)\\\\ &amp;\\vdots &amp; \\end{aligned} \\] Usually, the \\(u_i\\), \\(i=1,\\ldots,n\\) will correspond to a random intercept, and the \\(v_i\\) to a random slope for a particular coefficient. In principle, one can have one random slope for each predictor term in the model (but one can also have random slopes just for some or none of them). Let us now assume that we are interested in hypotheses of the type \\[ \\begin{aligned} H_0^{(u)}: \\sigma_u^2=0 &amp; \\mbox{ versus } &amp; H_1^{(u)}: \\sigma_u^2\\not=0\\\\ H_0^{(v)}: \\sigma_v^2=0 &amp; \\mbox{ versus } &amp; H_1^{(v)}: \\sigma_v^2\\not=0.\\\\ \\vdots \\end{aligned} \\] Clearly, if the null hypothesis say \\(H_0^{(u)}\\) is not rejected, then the random effect for the \\(u_i\\) is not needed, since they do not have randomness! In this case, a fixed effect, that is a traditional intercept, or in case of the \\(H_0^{(v)}\\) a usual fixed slope, are sufficient. To carry out these tests,we phrase the test problem again as a model comparison problem. Therefore, denote again \\(M_0\\) as the “smaller” model excluding the random effect in question; \\(M_1\\) as the “larger” model including that random effect. with \\(L_0\\), \\(L_1\\), \\(D_0\\), and \\(D_1\\) the associated likelihoods and deviances. Then consider again the LR statistic \\[ D_0-D_1= -2 \\log \\frac{L_0}{L_1} \\stackrel{H_0}{\\sim} \\chi^2(df) \\] Establishing the \\(df\\) does need some care. Since a \\(q\\)-dimensional vector of random effects will induce a \\(q \\times q\\) matrix \\(\\tilde{\\boldsymbol{Q}}\\), removing one random effect will take one row and one column of \\(\\tilde{\\boldsymbol{Q}}\\) out; and so the removed number of variance components will be the larger, the more random effects are already in the smaller model, \\(M_0\\). This is best illustrated by example (see below). It is further noted since the REML likelihood was explicitly produced to enable accurate estimation of the variance components \\((\\boldsymbol{\\gamma})\\), here one can use either REML- or ML- based likelihoods to carry out these tests. In fact, the function ranova which we will use for this purpose, does use REML likelihoods. 7.7.1 Example 7.9 (Mathematics achievement data) Let’s begin with the random intercept model. This has only one random effect, namely the random intercept. Let’s firstly look at this model once more. hsb.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: mathach ~ ses + (1 | school.id) ## Data: sub_hsb ## REML criterion at convergence: 8601.028 ## Random effects: ## Groups Name Std.Dev. ## school.id (Intercept) 2.518 ## Residual 6.010 ## Number of obs: 1329, groups: school.id, 30 ## Fixed Effects: ## (Intercept) ses ## 12.89 2.12 Indeed there is only one random effect which can possibly be removed, corresponding to the variance component with value \\(\\sigma_u=2.518\\). We are now testing whether this value can be considered significantly different from 0. require(lmerTest) ## Loading required package: lmerTest ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step ranova(hsb.lmm) ## ANOVA-like table for random-effects: Single term deletions ## ## Model: ## mathach ~ ses + (1 | school.id) ## npar logLik AIC LRT Df Pr(&gt;Chisq) ## &lt;none&gt; 4 -4300.5 8609 ## (1 | school.id) 3 -4351.0 8708 101.02 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here we clearly just have 1df since hsb.lmm just had one random effect. Note also \\(-2 \\times (-4300.5)= 8601.0\\), corresponding to the value given at REML criterion above. We also see that \\(D_0-D_1=101.02\\) based on the difference of the values of the REML criterion and so the random intercepts are clearly needed in the model. Now let’s do the same with the random slope model. hsb.slope.lmm ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: mathach ~ ses + (ses | school.id) ## Data: sub_hsb ## REML criterion at convergence: 8593.115 ## Random effects: ## Groups Name Std.Dev. Corr ## school.id (Intercept) 2.546 ## ses 1.253 0.04 ## Residual 5.951 ## Number of obs: 1329, groups: school.id, 30 ## Fixed Effects: ## (Intercept) ses ## 12.731 2.247 ranova(hsb.slope.lmm) ## ANOVA-like table for random-effects: Single term deletions ## ## Model: ## mathach ~ ses + (ses | school.id) ## npar logLik AIC LRT Df Pr(&gt;Chisq) ## &lt;none&gt; 6 -4296.6 8605.1 ## ses in (ses | school.id) 4 -4300.5 8609.0 7.913 2 0.01913 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Removing the random slope requires removing a variance and a covariance term from \\(\\tilde{\\boldsymbol{Q}}\\), hence df=2. Now \\(D_0-D_1=7.913\\) which is significant at the \\(5\\%\\) level but not at the \\(1\\%\\) level. So, at the \\(1\\%\\) level of significance we would decide not to include the random slope for ses. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
