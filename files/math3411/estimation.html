<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Estimation | Advanced Statistical Modelling III (Epiphany term)</title>
  <meta name="description" content="These are the course notes for the module Advanced Statistical Modelling III of Durham University’s degree for Mathematics and Statistics." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Estimation | Advanced Statistical Modelling III (Epiphany term)" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="These are the course notes for the module Advanced Statistical Modelling III of Durham University’s degree for Mathematics and Statistics." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Estimation | Advanced Statistical Modelling III (Epiphany term)" />
  
  <meta name="twitter:description" content="These are the course notes for the module Advanced Statistical Modelling III of Durham University’s degree for Mathematics and Statistics." />
  

<meta name="author" content="Department of Mathematical Sciences at Durham University" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Advanced Statistical Modelling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>General Information</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction and Review</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-1"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#ranmat"><i class="fa fa-check"></i><b>1.2</b> Random Vectors and Random Matrices: A Review</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#generalised-linear-models-a-review"><i class="fa fa-check"></i><b>1.3</b> Generalised Linear Models: A Review</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>2</b> Estimation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="estimation.html"><a href="estimation.html#likelihood-function"><i class="fa fa-check"></i><b>2.1</b> Likelihood Function</a></li>
<li class="chapter" data-level="2.2" data-path="estimation.html"><a href="estimation.html#loglike"><i class="fa fa-check"></i><b>2.2</b> Log-Likelihood Function</a></li>
<li class="chapter" data-level="2.3" data-path="estimation.html"><a href="estimation.html#score-function-and-score-equation"><i class="fa fa-check"></i><b>2.3</b> Score Function and Score Equation</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="estimation.html"><a href="estimation.html#special-case-natural-link"><i class="fa fa-check"></i><b>2.3.1</b> Special Case: Natural Link</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="estimation.html"><a href="estimation.html#fisherinformation"><i class="fa fa-check"></i><b>2.4</b> Fisher Information</a></li>
<li class="chapter" data-level="2.5" data-path="estimation.html"><a href="estimation.html#example-poisson-regression"><i class="fa fa-check"></i><b>2.5</b> Example: Poisson Regression</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="estimation.html"><a href="estimation.html#with-natural-link"><i class="fa fa-check"></i><b>2.5.1</b> With Natural Link</a></li>
<li class="chapter" data-level="2.5.2" data-path="estimation.html"><a href="estimation.html#with-identity-link"><i class="fa fa-check"></i><b>2.5.2</b> With Identity Link</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="estimation.html"><a href="estimation.html#properties"><i class="fa fa-check"></i><b>2.6</b> Properties of <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span> and <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta})\)</span></a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="estimation.html"><a href="estimation.html#expectation-of-boldsymbolsboldsymbolbeta"><i class="fa fa-check"></i><b>2.6.1</b> Expectation of <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span></a></li>
<li class="chapter" data-level="2.6.2" data-path="estimation.html"><a href="estimation.html#variance-of-boldsymbolsboldsymbolbeta"><i class="fa fa-check"></i><b>2.6.2</b> Variance of <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span></a></li>
<li class="chapter" data-level="2.6.3" data-path="estimation.html"><a href="estimation.html#properties-of-boldsymbolfboldsymbolbeta"><i class="fa fa-check"></i><b>2.6.3</b> Properties of <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta})\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="estimation.html"><a href="estimation.html#matrix-notation"><i class="fa fa-check"></i><b>2.7</b> Matrix Notation</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="estimation.html"><a href="estimation.html#matrixform"><i class="fa fa-check"></i><b>2.7.1</b> Score Function and Fisher Information</a></li>
<li class="chapter" data-level="2.7.2" data-path="estimation.html"><a href="estimation.html#natural-link"><i class="fa fa-check"></i><b>2.7.2</b> Natural Link</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="estimation.html"><a href="estimation.html#iterativesolution"><i class="fa fa-check"></i><b>2.8</b> Iterative Solution of <span class="math inline">\(\boldsymbol{S}(\hat{\boldsymbol{\beta}}) = 0\)</span></a>
<ul>
<li class="chapter" data-level="2.8.1" data-path="estimation.html"><a href="estimation.html#iteratively-reweighted-least-squares-irls"><i class="fa fa-check"></i><b>2.8.1</b> Iteratively Reweighted Least Squares (IRLS)</a></li>
<li class="chapter" data-level="2.8.2" data-path="estimation.html"><a href="estimation.html#irls-pseudo-code"><i class="fa fa-check"></i><b>2.8.2</b> IRLS Pseudo-Code</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="estimation.html"><a href="estimation.html#practical-example-us-polio-data"><i class="fa fa-check"></i><b>2.9</b> Practical Example: US Polio Data</a></li>
<li class="chapter" data-level="2.10" data-path="estimation.html"><a href="estimation.html#estimation-of-dispersion-parameter-phi"><i class="fa fa-check"></i><b>2.10</b> Estimation of Dispersion Parameter <span class="math inline">\(\phi\)</span></a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="estimation.html"><a href="estimation.html#special-cases"><i class="fa fa-check"></i><b>2.10.1</b> Special Cases</a></li>
<li class="chapter" data-level="2.10.2" data-path="estimation.html"><a href="estimation.html#practical-example-hospital-stay-data"><i class="fa fa-check"></i><b>2.10.2</b> Practical Example: Hospital Stay Data</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="estimation.html"><a href="estimation.html#asymptotic"><i class="fa fa-check"></i><b>2.11</b> Asymptotic Properties of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span></a>
<ul>
<li class="chapter" data-level="2.11.1" data-path="estimation.html"><a href="estimation.html#fisher-scoring"><i class="fa fa-check"></i><b>2.11.1</b> Fisher Scoring</a></li>
<li class="chapter" data-level="2.11.2" data-path="estimation.html"><a href="estimation.html#expectation"><i class="fa fa-check"></i><b>2.11.2</b> Expectation</a></li>
<li class="chapter" data-level="2.11.3" data-path="estimation.html"><a href="estimation.html#variance"><i class="fa fa-check"></i><b>2.11.3</b> Variance</a></li>
<li class="chapter" data-level="2.11.4" data-path="estimation.html"><a href="estimation.html#asymptotic-normality"><i class="fa fa-check"></i><b>2.11.4</b> Asymptotic Normality</a></li>
<li class="chapter" data-level="2.11.5" data-path="estimation.html"><a href="estimation.html#closing-the-circle"><i class="fa fa-check"></i><b>2.11.5</b> Closing The Circle</a></li>
<li class="chapter" data-level="2.11.6" data-path="estimation.html"><a href="estimation.html#next-step"><i class="fa fa-check"></i><b>2.11.6</b> Next Step</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="estimation.html"><a href="estimation.html#exercises-1"><i class="fa fa-check"></i><b>2.12</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Statistical Modelling III (Epiphany term)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimation" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Estimation<a href="estimation.html#estimation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Suppose we are given a dataset that we would like to model using a GLM. After checking the data and possibly performing some exploratory data analysis, we have already chosen a specific form of GLMs that is most suitable for the dataset (say e.g., the Poisson GLM). Now we wish to estimate the parameters of the model; that is, finding the value of <span class="math inline">\(\boldsymbol{\beta}\)</span> that best explains our data. For this purpose, we can use the maximum likelihood method to obtain an estimate <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>One advantage of the GLM formulation is that it allows us to derive a solution for the maximum likelihood method directly in the general form, thus unifying the estimation methods for various statistical models. In other words, we do not need to derive solutions for the Poisson GLM, the Binomial GLM, etc. separately. Instead, we can just derive the solution for the general form of the GLMs. The solution for each specific form of the GLM will be an instance of this general solution.</p>
<p>For the rest of this chapter,</p>
<div id="likelihood-function" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Likelihood Function<a href="estimation.html#likelihood-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the grouped data setup where we have predictors and data with possible replicates <span class="math inline">\(\left\{(\boldsymbol{x}_{i}, y_{ir_{i}})\right\}_{i\in[1..n], r_{i}\in[1..m_{i}]}\)</span>.
Recall that under a GLM, given predictors <span class="math inline">\(\left\{\boldsymbol{x}_{i}\right\}_{i\in[1..n]}\)</span>, each response <span class="math inline">\(y_{ir_{i}}\)</span> is independent of the other <span class="math inline">\(y_{jr_{j}}\)</span>, and of the values of all predictors <span class="math inline">\(\boldsymbol{x}_{j}\)</span> with <span class="math inline">\(j\neq i\)</span>, so that the joint probability of the data — that is, the likelihood — is given by</p>
<p><span class="math display" id="eq:jointprobability">\[\begin{equation}
  L(\boldsymbol{\beta})
  = P_{}\left(\left\{y_{ir_{i}}\right\} |\left\{\boldsymbol{x}_{i}\right\}, \boldsymbol{\beta}\right)
  = P_{}\left(\left\{y_{ir_{i}}\right\} |\left\{\theta_{i}\right\}, \phi\right)
  = \prod_{i = 1}^{n} \prod_{r_{i} = 1}^{m_{i}} P_{}\left(y_{ir_{i}} |\theta_{i}, \phi\right)
  \tag{2.1}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{equation}
  P_{}\left(y_{ir_{i}} |\theta_{i}, \phi\right)
  = \exp \left( \frac{y_{ir_{i}}\theta_{i} - b(\theta_{i})}{ \phi} + c(y_{ir_{i}}, \phi) \right)
\end{equation}\]</span></p>
<p>with</p>
<p><span class="math display">\[\begin{equation}
  \theta_{i}
  = (b&#39;)^{-1}(\mu_{i})
  = (b&#39;)^{-1}(h(\eta_{i}))
  = (b&#39;)^{-1}(h(\boldsymbol{\beta}^{T}\boldsymbol{x}_{i})).
\end{equation}\]</span></p>
</div>
<div id="loglike" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Log-Likelihood Function<a href="estimation.html#loglike" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The log probability of the data — or log-likelihood — is thus given by</p>
<p><span class="math display">\[\begin{align}
  l(\boldsymbol{\beta})
  &amp; = \log L(\boldsymbol{\beta}) = \log P_{}\left(\left\{y_{ir_{i}}\right\} |\left\{\theta_{i}\right\}, \phi\right) \\
  &amp; = \sum_{i} \sum_{r_{i}} \left( \frac{y_{ir_{i}}\theta_{i} - b(\theta_{i})}{\phi} + c(y_{ir_{i}}, \phi) \right) \\
  &amp; = \sum_{i} \left( m_{i} \frac{y_{i}\theta_{i} - b(\theta_{i})}{\phi} + \sum_{r_{i}} c(y_{ir_{i}}, \phi) \right) \\
  &amp; = \sum_{i} l_{i}
\end{align}\]</span></p>
<p>where we have defined</p>
<p><span class="math display">\[\begin{align}
  y_i &amp;= \frac{1}{m_i} \sum_{r_{i}} y_{ir_{i}} \\
  l_i &amp;= \frac{y_{i}\theta_{i} - b(\theta_{i})}{\phi_i} + \sum_{r_{i}} c(y_{ir_{i}}, \phi) \\
  \phi_i &amp;= \phi/m_i.
\end{align}\]</span></p>
</div>
<div id="score-function-and-score-equation" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Score Function and Score Equation<a href="estimation.html#score-function-and-score-equation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <span style="color: blue;">score function</span> is given by</p>
<p><span class="math display" id="eq:scorefunction">\[\begin{equation}
  \boldsymbol{S}(\boldsymbol{\beta})
  = \frac{\partial l}{\partial \boldsymbol{\beta}^{T}}
  = \sum_{i} \frac{\partial l_{i}}{\partial \boldsymbol{\beta}^{T}}
  = \sum_{i} \frac{\partial l_{i}}{\partial \theta_{i}}
             \frac{\partial \theta_{i}}{\partial \mu_{i}}
             \frac{\partial \mu_{i}}{\partial \eta_{i}}
             \frac{\partial \eta_{i}}{\partial \boldsymbol{\beta}^{T}}
  \tag{2.2}
\end{equation}\]</span></p>
<p>where, recalling that <span class="math inline">\(\mu_i = b&#39;(\theta_i)\)</span>, <span class="math inline">\(\mathcal{V}(\mu_{i}) = b&#39;&#39;(\theta_{i})\)</span>, <span class="math inline">\(\mu_i = h(\eta_i)\)</span> and <span class="math inline">\(\eta_i = \boldsymbol{\beta}^T\boldsymbol{x}_i\)</span>, we have:<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display" id="eq:dmudeta" id="eq:dthetadmu">\[\begin{align}
  \frac{\partial l_{i} }{\partial \theta_{i}}
    &amp; = \frac{y_{i} - b&#39;(\theta_{i})}{\phi_{i}}
      = \frac{y_{i} - \mu_{i}}{\phi_{i}} \\
  \frac{\partial \theta_{i}}{\partial \mu_{i}}
    &amp; = 1 / \left( \frac{\partial \mu_{i}}{\partial \theta_{i}} \right)
      = \frac{1}{b&#39;&#39;(\theta_{i})}
      = \frac{1}{\mathcal{V}(\mu_{i})}
      \tag{2.3} \\
  \frac{\partial \mu_{i}}{\partial \eta_{i}}
    &amp; = h&#39;(\eta_{i})
      \tag{2.4} \\
  \frac{\partial \eta_{i}}{\partial \boldsymbol{\beta}^{T}}
    &amp; = \boldsymbol{x}_{i}.
\end{align}\]</span></p>
<p>The score function is thus given by</p>
<p><span class="math display" id="eq:scorefunction2">\[\begin{align}
  \boldsymbol{S}(\boldsymbol{\beta})
  &amp; = \sum_{i} \left(\frac{y_{i} - \mu_{i}}{\phi_{i}} \right) \;
               \left(\frac{1}{\mathcal{V}(\mu_{i})} \right) \;
               h&#39;(\eta_{i}) \;\boldsymbol{x}_{i} \\
  &amp; = \frac{1}{\phi} \sum_{i} m_{i}(y_{i} - \mu_{i}) \;
                              \frac{1}{\mathcal{V}(\mu_{i})} \;
                              h&#39;(\eta_{i}) \;\boldsymbol{x}_{i}.
  \tag{2.5}
\end{align}\]</span></p>
<p>The maximum likelihood estimate <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> must then satisfy the <span style="color: blue;">score equation</span>:</p>
<p><span class="math display" id="eq:scoreequation">\[\begin{equation}
  \boldsymbol{S}(\hat{\boldsymbol{\beta}}) = 0.
  \tag{2.6}
\end{equation}\]</span></p>
<p>Note that the dispersion parameter <span class="math inline">\(\phi\)</span> cancels from the score equation, which implies that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> does not depend on <span class="math inline">\(\phi\)</span>. This is another important property of EDFs.</p>
<div id="special-case-natural-link" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Special Case: Natural Link<a href="estimation.html#special-case-natural-link" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the natural link, <span class="math inline">\(\theta_{i} = \eta_{i}\)</span>, so Equations <a href="estimation.html#eq:dthetadmu">(2.3)</a> and <a href="estimation.html#eq:dmudeta">(2.4)</a> combine to give</p>
<p><span class="math display">\[\begin{equation}
  \frac{h&#39;(\eta_i)}{\mathcal{V}(\mu_i)}
  = \frac{\partial\theta_{i}}{\partial \mu_{i}} \frac{\partial\mu_{i}}{\partial \eta_{i}}
  = \frac{\partial\theta_{i}}{\partial \eta_{i}}
  = 1.
\end{equation}\]</span></p>
<p>The score function thus simplifies to</p>
<p><span class="math display" id="eq:scorefunctionnaturallink">\[\begin{equation}
  \boldsymbol{S}(\boldsymbol{\beta}) = \frac{1}{\phi}\sum_{i} m_{i}(y_{i} - \mu_{i})\;\boldsymbol{x}_{i}.
  \tag{2.7}
\end{equation}\]</span></p>
</div>
</div>
<div id="fisherinformation" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Fisher Information<a href="estimation.html#fisherinformation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To solve the score equation, we will also need the second derivative of the log-likelihood. Its negative is called the <span style="color: blue;">Observed Fisher Information</span>, defined as</p>
<p><span class="math display" id="eq:observedfisherinformation">\[\begin{equation}
  \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})
  = - \frac{\partial^{2} l}{\partial\boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}}
  = - \frac{\partial \boldsymbol{S}}{\partial \boldsymbol{\beta}}.
  \tag{2.8}
\end{equation}\]</span></p>
<p>Note that, at the MLE, <span class="math inline">\(\boldsymbol{F}_{\text{obs}}(\hat{\boldsymbol{\beta}})\)</span> is positive by definition.
Because it is a function of the data <span class="math inline">\(\left\{y_{i}\right\}\)</span>, <span class="math inline">\(\boldsymbol{F}_{\text{obs}}\)</span> has a probability distribution. In practice, the Observed Fisher Information is often approximated by the <span style="color: blue;">Expected Fisher Information</span>, otherwise known simply as the <span style="color: blue;">Fisher Information</span>:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p><span class="math display" id="eq:fisherinformation">\[\begin{equation}
  \boldsymbol{F}(\boldsymbol{\beta}) = E \left[ - \frac{\partial \boldsymbol{S} }{ \partial \boldsymbol{\beta}} \right]
  \tag{2.9}
\end{equation}\]</span></p>
<p>where the expectation is taken over the joint probability distribution of the data <span class="math inline">\(P_{}\left(\left\{y_{ir_{i}}\right\} |\boldsymbol{\beta}, \left\{\boldsymbol{x}_{i}\right\}\right)\)</span>.</p>
</div>
<div id="example-poisson-regression" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Example: Poisson Regression<a href="estimation.html#example-poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We look at two example calculations of the score function and Fisher Information for Poisson Regression, that is we have</p>
<ul>
<li><span class="math inline">\(y |\boldsymbol{x}, \boldsymbol{\beta} \sim \text{Poi}(\lambda(\boldsymbol{x}, \boldsymbol{\beta}))\)</span></li>
<li><span class="math inline">\(\phi = 1\)</span>.</li>
</ul>
<p>In this example, let us assume <span class="math inline">\(m_i = 1\)</span>, i.e., the data are ungrouped.</p>
<div id="with-natural-link" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> With Natural Link<a href="estimation.html#with-natural-link" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have that:
<span class="math inline">\(\lambda(\boldsymbol{x}, \boldsymbol{\beta}) = \mu(\boldsymbol{x}, \boldsymbol{\beta}) = h(\eta(\boldsymbol{x}, \boldsymbol{\beta})) = e^{\eta(\boldsymbol{x}, \boldsymbol{\beta})} = e^{\boldsymbol{\beta}^T\boldsymbol{x}}\)</span>.</p>
<p>Equation <a href="estimation.html#eq:scorefunctionnaturallink">(2.7)</a> then gives</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{S}(\boldsymbol{\beta}) = \sum_{i} (y_{i} - e^{\boldsymbol{\beta}^{T}\boldsymbol{x}_{i}})\;\boldsymbol{x}_{i}
\end{equation}\]</span></p>
<p>while Equation <a href="estimation.html#eq:observedfisherinformation">(2.8)</a> gives</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}) = \sum_{i} e^{\boldsymbol{\beta}^{T}\boldsymbol{x}_{i}}\;\boldsymbol{x}_{i}\boldsymbol{x}_{i}^{T}.
\end{equation}\]</span></p>
<p>Note that this does not depend on the data, so that Equation <a href="estimation.html#eq:fisherinformation">(2.9)</a> gives</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{F}(\boldsymbol{\beta}) = {\mathrm E}[\boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})] = \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}).
\end{equation}\]</span></p>
</div>
<div id="with-identity-link" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> With Identity Link<a href="estimation.html#with-identity-link" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The identity link is defined such that <span class="math inline">\(h(\eta) = \eta\)</span>.</p>
<p>In this case, we have that:</p>
<ul>
<li><span class="math inline">\(\lambda(\boldsymbol{x}, \boldsymbol{\beta}) = \mu(\boldsymbol{x}, \boldsymbol{\beta}) = h(\eta(\boldsymbol{x}, \boldsymbol{\beta})) = \eta(\boldsymbol{x}, \boldsymbol{\beta}) = \boldsymbol{\beta}^T \boldsymbol{x}\)</span>.</li>
<li><span class="math inline">\(\mathcal{V}(\mu) = \mu\)</span> (see Poisson example in EDF chapter).</li>
<li><span class="math inline">\(h&#39;(\eta) = 1\)</span>.</li>
</ul>
<p>Equation <a href="estimation.html#eq:scorefunction2">(2.5)</a> thus gives</p>
<p><span class="math display">\[\begin{align}
  \boldsymbol{S}(\boldsymbol{\beta})
  &amp; = \sum_{i} (y_{i} - \mu_{i})\;\frac{1}{\mu_{i}}\;1 \;\boldsymbol{x}_{i} \\
  &amp; = \sum_{i} (y_{i} - \boldsymbol{\beta}^{T}\boldsymbol{x}_{i}) \;\frac{1}{\boldsymbol{\beta}^{T}\boldsymbol{x}_{i}}\;\boldsymbol{x}_{i} \\
  &amp; = \sum_{i} \left( \frac{y_{i}}{ \boldsymbol{\beta}^{T}\boldsymbol{x}_{i}} - 1 \right) \;\boldsymbol{x}_{i}.
\end{align}\]</span></p>
<p>Using Equation <a href="estimation.html#eq:observedfisherinformation">(2.8)</a> and the chain rule, we can obtain the Observed Fisher Information:</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})
  = \sum_{i} \frac{y_{i}}{ (\boldsymbol{\beta}^{T}\boldsymbol{x}_{i})^{2}}\;\boldsymbol{x}_{i}\boldsymbol{x}_{i}^{T}.
\end{equation}\]</span></p>
<p>Hence, the Fisher Information is:</p>
<p><span class="math display">\[\begin{align}
  \boldsymbol{F}(\boldsymbol{\beta})
  &amp; = {\mathrm E}[\boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})] \\
  &amp; = \textrm{E} \left[ \sum_{i} \frac{Y_{i}}{(\boldsymbol{\beta}^{T}\boldsymbol{x}_{i})^{2}} \;\boldsymbol{x}_{i}\boldsymbol{x}_{i}^{T} \right] \\
  &amp; = \sum_{i} \frac{{\mathrm E}[Y_{i} |\boldsymbol{\beta}, \boldsymbol{x}_{i}]}{ (\boldsymbol{\beta}^{T}\boldsymbol{x}_{i})^{2}} \;\boldsymbol{x}_{i}\boldsymbol{x}_{i}^{T} \\
  &amp; = \sum_{i} \frac{\boldsymbol{\beta}^{T}\boldsymbol{x}_{i} }{ (\boldsymbol{\beta}^{T}\boldsymbol{x}_{i})^{2}} \;\boldsymbol{x}_{i}\boldsymbol{x}_{i}^{T} \\
  &amp; = \sum_{i} \frac{1 }{ \boldsymbol{\beta}^{T}\boldsymbol{x}_{i}}\;\boldsymbol{x}_{i}\boldsymbol{x}_{i}^{T}.
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}) \neq \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})\)</span> in this case.</p>
</div>
</div>
<div id="properties" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Properties of <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span> and <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta})\)</span><a href="estimation.html#properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Having defined the score function <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span> and the Fisher information <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta})\)</span>, in this section we will investigate some of their properties.
First, let us define <span class="math inline">\(\displaystyle S_i(\boldsymbol{\beta}) = \frac{\partial l_i}{\partial \boldsymbol{\beta}}\)</span>. Then we have <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta}) = \sum_{i} \boldsymbol{S}_{i}(\boldsymbol{\beta})\)</span>.</p>
<div id="expectation-of-boldsymbolsboldsymbolbeta" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Expectation of <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span><a href="estimation.html#expectation-of-boldsymbolsboldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The expectation of <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span> can be computed from Equations <a href="estimation.html#eq:scorefunction">(2.2)</a> and <a href="estimation.html#eq:scorefunction2">(2.5)</a> as follows:</p>
<p><span class="math display" id="eq:expectationSbeta">\[\begin{align}
  {\mathrm E}[\boldsymbol{S}(\boldsymbol{\beta})]
  &amp; = \sum_{i} {\mathrm E}[\boldsymbol{S}_{i}(\boldsymbol{\beta})] \\
  &amp; = \sum_{i} \frac{{\mathrm E}[Y_{i} |\boldsymbol{\beta}, \boldsymbol{x}_{i}] - \mu_{i}}{ \phi_{i}} \frac{1}{\mathcal{V}(\mu_{i})} h&#39;(\eta_{i})\;\boldsymbol{x}_i \\
  &amp; = 0
  \tag{2.10}
\end{align}\]</span></p>
<p>because <span class="math inline">\({\mathrm E}[Y_{i} |\boldsymbol{\beta}, \boldsymbol{x}_{i}] = \mu_{i}\)</span>.</p>
</div>
<div id="variance-of-boldsymbolsboldsymbolbeta" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Variance of <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span><a href="estimation.html#variance-of-boldsymbolsboldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using Equation <a href="estimation.html#eq:scorefunction2">(2.5)</a> and the properties of covariance matrices in Section <a href="introduction.html#ranmat">1.2</a>, we can calculate the variance of <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta})\)</span> as follows:</p>
<p><span class="math display">\[\begin{align}
  {\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta})]
  &amp; = \sum_{i} {\mathrm{Var}}[\boldsymbol{S}_{i}(\boldsymbol{\beta})] \\
  &amp; = \sum_{i} \mathrm{Var} \left[ \frac{h&#39;(\eta_{i})}{\phi_i \mathcal{V}(\mu_{i})} \boldsymbol{x}_{i} (Y_{i} - \mu_{i}) \right] \\
  &amp; = \sum_{i} \left( \frac{h&#39;(\eta_{i})}{\phi_i \mathcal{V}(\mu_{i})} \boldsymbol{x}_{i} \right) {\mathrm{Var}}[Y_{i} - \mu_{i}] \left( \frac{h&#39;(\eta_{i})}{\phi_i \mathcal{V}(\mu_{i})} \boldsymbol{x}_{i}^T \right) \\
  &amp;= \sum_{i} \left( \frac{h&#39;(\eta_{i})^2}{\phi_i^2 \mathcal{V}(\mu_{i})^2} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^T \right) {\mathrm{Var}}[Y_{i}].
\end{align}\]</span></p>
<p>Note that in the first equality above, we can break the variance into sum of smaller components due to the independent data assumption.</p>
<p>Now using <span class="math inline">\({\mathrm{Var}}[Y_i] = \phi_i \mathcal{V}(\mu_{i})\)</span>, we can obtain the expression for <span class="math inline">\({\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta})]\)</span>:</p>
<p><span class="math display" id="eq:vars">\[\begin{equation}
  {\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta})]
  = \sum_{i} \frac{h&#39;(\eta_{i})^2}{\phi_i \mathcal{V}(\mu_{i})} \boldsymbol{x}_{i} \boldsymbol{x}_{i}^T.
  \tag{2.11}
\end{equation}\]</span></p>
</div>
<div id="properties-of-boldsymbolfboldsymbolbeta" class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> Properties of <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta})\)</span><a href="estimation.html#properties-of-boldsymbolfboldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall from Equation <a href="estimation.html#eq:fisherinformation">(2.9)</a> that
<span class="math inline">\(\displaystyle \boldsymbol{F}(\boldsymbol{\beta}) = E \left[ - \frac{\partial \boldsymbol{S} }{ \partial \boldsymbol{\beta}} \right] = - E \left[ \frac{\partial^{2} l}{\partial\boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \right]\)</span>.</p>
<p>We will first show that
<span class="math inline">\(\displaystyle E \left[ \frac{\partial^{2} l}{\partial\boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \right]
= E \left[ - \frac{\partial l}{\partial \boldsymbol{\beta}^{T}} \frac{\partial l}{\partial\boldsymbol{\beta}} \right]\)</span>.</p>
<div id="an-important-identity" class="section level4 hasAnchor" number="2.6.3.1">
<h4><span class="header-section-number">2.6.3.1</span> An Important Identity<a href="estimation.html#an-important-identity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(\rho = e^l\)</span>, where <span class="math inline">\(l\)</span> is the log-likelihood, so that <span class="math inline">\(\rho = L(\boldsymbol{\beta}) = P_{}\left(\left\{y_{ir_{i}}\right\} |\left\{\boldsymbol{x}_{i}\right\}, \boldsymbol{\beta}\right)\)</span> is the likelihood/probability of the data. Then</p>
<p><span class="math display">\[\begin{equation}
  \frac{\partial l}{\partial \boldsymbol{\beta}^{T}}
  = \frac{\partial l}{\partial \rho}\frac{\partial \rho}{\partial \boldsymbol{\beta}^{T}}
  = \frac{1}{\rho}\frac{\partial \rho}{\partial \boldsymbol{\beta}^{T}}.
\end{equation}\]</span></p>
<p>Using the product rule and chain rule, we have</p>
<p><span class="math display">\[\begin{align}
  \frac{\partial^{2} l}{\partial\boldsymbol{\beta}^{T}\partial \boldsymbol{\beta}}
  &amp; = - \frac{1}{\rho^{2}} \frac{\partial \rho}{\partial \boldsymbol{\beta}^{T}} \frac{\partial \rho}{\partial \boldsymbol{\beta}} + \frac{1}{\rho} \frac{\partial^{2} \rho}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \\
  &amp; = - \frac{1}{\rho^{2}} \left( \frac{\partial \rho}{\partial l} \frac{\partial l}{\partial \boldsymbol{\beta}^{T}} \right) \left( \frac{\partial \rho}{\partial l} \frac{\partial l}{\partial \boldsymbol{\beta}} \right) + \frac{1}{\rho} \frac{\partial^{2} \rho}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \\
  &amp; = - \frac{1}{\rho^{2}} \left( \rho \frac{\partial l}{\partial \boldsymbol{\beta}^{T}} \right) \left( \rho \frac{\partial l}{\partial \boldsymbol{\beta}} \right) + \frac{1}{\rho} \frac{\partial^{2} \rho}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \\
  &amp; = - \frac{\partial l}{\partial \boldsymbol{\beta}^{T}} \frac{\partial l}{\partial \boldsymbol{\beta}} + \frac{1}{\rho} \frac{\partial^{2} \rho}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}}.
\end{align}\]</span></p>
<p>Note that the expectation (over the data) of the second term is</p>
<p><span class="math display">\[\begin{equation}
  \mathrm{E} \left[ \frac{1}{\rho} \frac{\partial^{2} \rho}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \right]
  = \int \rho \;\frac{1}{\rho} \frac{\partial^{2} \rho}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}}
  = \int \frac{\partial^{2} \rho}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}}
  = \frac{\partial^{2}}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \int \rho
  = \frac{\partial^{2}}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \;1
  = 0.
\end{equation}\]</span></p>
<p>Thus, we have:</p>
<p><span class="math display" id="eq:importantidentity">\[\begin{equation}
  E \left[ \frac{\partial^{2} l}{\partial\boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \right]
  = E \left[ - \frac{\partial l}{\partial \boldsymbol{\beta}^{T}} \frac{\partial l}{\partial\boldsymbol{\beta}} \right].
  \tag{2.12}
\end{equation}\]</span></p>
</div>
<div id="relating-boldsymbolfboldsymbolbeta-and-mathrmvarboldsymbolsboldsymbolbeta" class="section level4 hasAnchor" number="2.6.3.2">
<h4><span class="header-section-number">2.6.3.2</span> Relating <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta})\)</span> and <span class="math inline">\({\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta})]\)</span><a href="estimation.html#relating-boldsymbolfboldsymbolbeta-and-mathrmvarboldsymbolsboldsymbolbeta" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Using Equation <a href="estimation.html#eq:importantidentity">(2.12)</a>, we have that:</p>
<p><span class="math display">\[\begin{align}
  \boldsymbol{F}(\boldsymbol{\beta})
  &amp; = - \mathrm{E} \left[ \frac{\partial^{2} l}{\partial \boldsymbol{\beta}^{T}\partial\boldsymbol{\beta}} \right]
    = \mathrm{E} \left[ \frac{\partial l}{\partial \boldsymbol{\beta}^{T}} \frac{\partial l}{\partial\boldsymbol{\beta}} \right] \\
  &amp; = {\mathrm E}[\boldsymbol{S}(\boldsymbol{\beta})\boldsymbol{S}(\boldsymbol{\beta})^{T}] \\
  &amp; = {\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta})] + {\mathrm E}[\boldsymbol{S}(\boldsymbol{\beta})] \;{\mathrm E}[\boldsymbol{S}(\boldsymbol{\beta})]^{T} \\
  &amp; = {\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta})]
\end{align}\]</span></p>
<p>where the last equality is due to <span class="math inline">\({\mathrm E}[\boldsymbol{S}(\boldsymbol{\beta})] = 0\)</span>.</p>
<p>Therefore, an important property of the Fisher Information is that it is equal to the variance of the score function, whose expression is given in Equation <a href="estimation.html#eq:vars">(2.11)</a>.</p>
</div>
<div id="special-case-natural-link-1" class="section level4 hasAnchor" number="2.6.3.3">
<h4><span class="header-section-number">2.6.3.3</span> Special Case: Natural Link<a href="estimation.html#special-case-natural-link-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For the natural link, recall that <span class="math inline">\(\displaystyle \frac{h&#39;(\eta_i)}{\mathcal{V}(\mu_i)} = 1\)</span>. So we have:</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{S}(\boldsymbol{\beta}) = \sum_i \frac{1}{\phi_i} (y_i - h(\eta_i))\boldsymbol{x}_i.
\end{equation}\]</span></p>
<p>Let <span class="math inline">\(\displaystyle \boldsymbol{S}_i = \frac{1}{\phi_i} (y_i - h(\eta_i))\boldsymbol{x}_i\)</span>. We have <span class="math inline">\(\boldsymbol{S} = \sum_i \boldsymbol{S}_i\)</span> and thus:</p>
<p><span class="math display">\[\begin{align}
  \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}) &amp; = - \frac{\partial \boldsymbol{S}}{\partial \boldsymbol{\beta}} = - \sum_i \frac{\partial \boldsymbol{S}_i}{\partial \boldsymbol{\beta}} = - \sum_i \frac{\partial \boldsymbol{S}_i}{\partial \eta_i} \frac{\partial \eta_i}{\partial \boldsymbol{\beta}} = \sum_i \frac{h&#39;(\eta_i)}{\phi_i} \boldsymbol{x}_i\boldsymbol{x}_i^T \\
  \boldsymbol{F}(\boldsymbol{\beta}) &amp; = {\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta})] = \sum_i \frac{h&#39;(\eta_i)}{\phi_i}\boldsymbol{x}_i\boldsymbol{x}_i^T .
\end{align}\]</span></p>
<p>Thus, for the natural link, we see that <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}) = \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})\)</span>.</p>
</div>
</div>
</div>
<div id="matrix-notation" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Matrix Notation<a href="estimation.html#matrix-notation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For the next section, it is useful to establish a condensed, matrix notation for some of the previous quantities, analogous to the matrix notation used for linear models.</p>
<ul>
<li><p>Let <span class="math inline">\(\boldsymbol{Y}\in{\mathbb R}^{n}\)</span> be the random vector with components <span class="math inline">\(Y_{i}\)</span>, the response values. This is exactly the same quantity as in the linear model case.</p></li>
<li><p>Let <span class="math inline">\(\boldsymbol{X}\in{\mathbb R}^{n\times p}\)</span> be the <em>design matrix</em>, the matrix with components <span class="math inline">\(x_{i, a}\)</span>, the value of the <span class="math inline">\(a^{\text{th}}\)</span> component of the predictor vector for the <span class="math inline">\(i^{\text{th}}\)</span> data point. This is exactly the same quantity as in the linear model case. This matrix is sometimes called the <em>model matrix</em>.</p></li>
<li><p>Let <span class="math inline">\(\boldsymbol{\mu}\in{\mathbb R}^{n}\)</span> be the vector with components <span class="math inline">\(\mu_{i} = h(\boldsymbol{\beta}^{T}x_{i})\)</span>, so that <span class="math inline">\(\boldsymbol{\mu} = {\mathrm E}[\boldsymbol{Y}]\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\boldsymbol{D}\in {\mathbb R}^{n\times n}\)</span> be the diagonal matrix with components <span class="math inline">\(D_{ii} = h&#39;(\eta_{i})\)</span>. For example, if <span class="math inline">\(h(\eta) = e^{\eta}\)</span>, then</p></li>
</ul>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{D} =
    \begin{pmatrix}
      e^{\boldsymbol{\beta}^{T}x_{1}} &amp; 0 &amp; \cdots &amp; 0 \\
      0 &amp; \ddots &amp; \ddots &amp; \vdots \\
      \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
      0 &amp; \cdots &amp; 0 &amp; e^{\boldsymbol{\beta}^{T}x_{n}}
    \end{pmatrix}.
\end{equation}\]</span></p>
<ul>
<li>Let <span class="math inline">\(\boldsymbol{\Sigma}\in{\mathbb R}^{n\times n}\)</span> be the covariance matrix for <span class="math inline">\(\boldsymbol{Y}\)</span>, with components:</li>
</ul>
<p><span class="math display">\[\begin{equation}
  \Sigma_{ij}
  = \text{Cov}[Y_{i}, Y_{j}]
  = \text{Var}[Y_{i}]\;\delta_{ij}
  = \phi_{i}\mathcal{V}(\mu_{i})\;\delta_{ij}.
\end{equation}\]</span></p>
<p>That is,</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{\Sigma} =
    \begin{pmatrix}
      \text{Var}[Y_{1}] &amp; 0 &amp; \cdots &amp; 0 \\
      0 &amp; \ddots &amp; \ddots &amp; \vdots \\
      \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
      0 &amp; \cdots &amp; 0 &amp; \text{Var}[Y_{n}]
    \end{pmatrix}
  = \begin{pmatrix}
      \phi_{1}\mathcal{V}(\mu_{1}) &amp; 0 &amp; \cdots &amp; 0 \\
      0 &amp; \ddots &amp; \ddots &amp; \vdots \\
      \vdots &amp; \ddots &amp; \ddots &amp; 0 \\
      0 &amp; \cdots &amp; 0 &amp; \phi_{n}\mathcal{V}(\mu_{n})
    \end{pmatrix}.
\end{equation}\]</span></p>
<div id="matrixform" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Score Function and Fisher Information<a href="estimation.html#matrixform" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that</p>
<p><span class="math display">\[\begin{align}
  \boldsymbol{S}(\boldsymbol{\beta})
  &amp; = \sum_{i} \left( \frac{y_{i} - \mu_{i}}{ \phi_{i}\mathcal{V}(\mu_{i})} \right) h&#39;(\eta_{i}) \;\boldsymbol{x}_{i} \\
  \boldsymbol{F}(\boldsymbol{\beta})
  &amp; = \sum_{i} \frac{h&#39;(\eta_{i})^{2}}{ \phi_{i}\mathcal{V}(\mu_{i})} \;\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{T}.
\end{align}\]</span></p>
<p>In terms of the matrix notation, these become</p>
<p><span class="math display">\[\begin{align}
  \boldsymbol{S} &amp; = \boldsymbol{X}^{T}\boldsymbol{D}\boldsymbol{\Sigma}^{-1}(\boldsymbol{Y} - \boldsymbol{\mu}) \\
  \boldsymbol{F} &amp; = \boldsymbol{X}^{T}\boldsymbol{D}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{D}\boldsymbol{X}.
\end{align}\]</span></p>
</div>
<div id="natural-link" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Natural Link<a href="estimation.html#natural-link" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note that for the natural link,</p>
<p><span class="math display">\[\begin{equation}
  \frac{\partial \theta_{i}}{\partial \eta_{i}}
  = \frac{h&#39;(\eta_{i})}{\mathcal{V}(\mu_{i})}
  = 1.
\end{equation}\]</span></p>
<p>Thus, with <span class="math inline">\(\phi_{i} = \phi/m_{i}\)</span>, we have:</p>
<p><span class="math display">\[\begin{equation}
  h&#39;(\eta_{i})
  = \mathcal{V}(\mu_{i})
  = \frac{\text{Var}[Y_{i}] }{ \phi_{i}}
  = m_{i}\frac{{\mathrm{Var}}[Y_{i}] }{ \phi}.
\end{equation}\]</span></p>
<p>Now let <span class="math inline">\(\boldsymbol{G} \in {\mathbb R}^{n \times n}\)</span> be the diagonal matrix with components <span class="math inline">\(m_{i}\delta_{ij}\)</span>, known as the <em>grouping</em> matrix. Then</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{D}
  = \frac{1}{\phi} \boldsymbol{G}\boldsymbol{\Sigma}
  = \frac{1}{\phi}\boldsymbol{\Sigma} \boldsymbol{G},
\end{equation}\]</span></p>
<p>and therefore,</p>
<p><span class="math display">\[\begin{align}
  \boldsymbol{S}(\boldsymbol{\beta}) &amp; = \frac{1}{\phi} \boldsymbol{X}^{T}\boldsymbol{G}(\boldsymbol{Y} - \boldsymbol{\mu}) \\
  \boldsymbol{F}(\boldsymbol{\beta}) &amp; = \frac{1}{\phi^{2}} \boldsymbol{X}^{T}\boldsymbol{G}^{T}\boldsymbol{\Sigma}\boldsymbol{G}\boldsymbol{X}.
\end{align}\]</span></p>
</div>
</div>
<div id="iterativesolution" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Iterative Solution of <span class="math inline">\(\boldsymbol{S}(\hat{\boldsymbol{\beta}}) = 0\)</span><a href="estimation.html#iterativesolution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we have seen how to set up the score equation for the maximum likelihood estimate. We have also proven some of its properties as well as those of the Fisher Information. We now turn to the question of how to solve the score equation. In general, this cannot be done in closed form, except in rare cases. So we need to turn to numerical methods implemented on a computer.</p>
<p>We have the same two options here as in the binary regression case: we can try to optimise <span class="math inline">\(l\)</span> directly, or we can attempt to solve the score equation. There are many algorithms that can be used to perform these tasks. Here we focus on one: <span style="color: blue;">Iteratively Reweighted Least Squares (IRLS)</span>, also known as <span style="color: blue;">Iterative Weighted Least Squares (IWLS)</span>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>We start by recalling the Newton-Raphson method for finding the zero of a function. Note that we wish to solve the equation:</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{S}(\hat{\boldsymbol{\beta}}) = 0.
\end{equation}\]</span></p>
<p>We then approximate <span class="math inline">\(\boldsymbol{S}\)</span> linearly about some point <span class="math inline">\(\boldsymbol{\beta}_{0}\)</span>:</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{S}(\boldsymbol{\beta}_{0} + \delta\boldsymbol{\beta}_{0})
  = \boldsymbol{S}(\boldsymbol{\beta}_{0}) +
    \frac{\partial \boldsymbol{S}(\boldsymbol{\beta}_{0})}{\partial\boldsymbol{\beta}}\delta\boldsymbol{\beta}_{0} +
    \mathcal{O}(\delta\boldsymbol{\beta}_{0}^{2})
\end{equation}\]</span></p>
<p>where the reason for the subscript <span class="math inline">\(0\)</span> will become apparent soon. In the case when <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta}_{0} + \delta\boldsymbol{\beta}_{0}) = 0\)</span> (such as the case we are interested in), we have approximately that:</p>
<p><span class="math display" id="eq:solutionnewton">\[\begin{equation}
  \frac{\partial \boldsymbol{S}(\boldsymbol{\beta}_{0})}{\partial\boldsymbol{\beta}}\delta\boldsymbol{\beta}_{0}
  = - \boldsymbol{S}(\boldsymbol{\beta}_{0}).
  \tag{2.13}
\end{equation}\]</span></p>
<p>Now in our case,</p>
<p><span class="math display">\[\begin{equation}
  -\frac{\partial \boldsymbol{S}(\boldsymbol{\beta}_{0})}{\partial\boldsymbol{\beta}}
  = \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}_{0}),
\end{equation}\]</span></p>
<p>so Equation <a href="estimation.html#eq:solutionnewton">(2.13)</a> becomes</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}_{0})\delta\boldsymbol{\beta}_{0}
  = \boldsymbol{S}(\boldsymbol{\beta}_{0}),
\end{equation}\]</span></p>
<p>or equivalently,</p>
<p><span class="math display">\[\begin{equation}
  \delta\boldsymbol{\beta}_{0}
  = \left( \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}_{0}) \right)^{-1} \boldsymbol{S}(\boldsymbol{\beta}_{0}).
\end{equation}\]</span></p>
<p>This then gives us a new value:</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{\beta}_{1}
  = \boldsymbol{\beta}_{0} + \delta\boldsymbol{\beta}_{0}
  = \boldsymbol{\beta}_{0} +
    \left( \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}_{0}) \right)^{-1} \boldsymbol{S}(\boldsymbol{\beta}_{0}).
\end{equation}\]</span></p>
<p>Then we can iterate the steps above for <span class="math inline">\(m = 1, 2, \ldots,\)</span></p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{\beta}_{m+1} = \boldsymbol{\beta}_{m} + \delta\boldsymbol{\beta}_{m}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{equation}
  \delta\boldsymbol{\beta}_{m}
  = \left( \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}_{m}) \right)^{-1}
    \boldsymbol{S}(\boldsymbol{\beta}_{m}).
\end{equation}\]</span></p>
<p>Because <span class="math inline">\(\boldsymbol{F}_{\text{obs}}\)</span> is hard to find and hard to invert in general, we approximate it with the expected Fisher Information. This is known as the <span style="color: blue;">Fisher scoring</span> method, where we compute <span class="math inline">\(\delta\boldsymbol{\beta}_{m}\)</span> by:</p>
<p><span class="math display" id="eq:fisherscoring">\[\begin{equation}
  \delta\boldsymbol{\beta}_{m}
  = \left( \boldsymbol{F}(\boldsymbol{\beta}_{m}) \right)^{-1} \boldsymbol{S}(\boldsymbol{\beta}_{m}).
  \tag{2.14}
\end{equation}\]</span></p>
<div id="iteratively-reweighted-least-squares-irls" class="section level3 hasAnchor" number="2.8.1">
<h3><span class="header-section-number">2.8.1</span> Iteratively Reweighted Least Squares (IRLS)<a href="estimation.html#iteratively-reweighted-least-squares-irls" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now we will use Equation <a href="estimation.html#eq:fisherscoring">(2.14)</a> to derive the IRLS method in matrix notation. From Equation <a href="estimation.html#eq:fisherscoring">(2.14)</a>, we have that</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{F}(\boldsymbol{\beta}_{m})\delta\boldsymbol{\beta}_{m}
  = \boldsymbol{S}(\boldsymbol{\beta}_{m})
\end{equation}\]</span></p>
<p>or equivalently that</p>
<p><span class="math display" id="eq:fisheriteration">\[\begin{equation}
  \boldsymbol{F}(\boldsymbol{\beta}_{m})\boldsymbol{\beta}_{m+1}
  = \boldsymbol{F}(\boldsymbol{\beta}_{m})\boldsymbol{\beta}_{m} +
    \boldsymbol{S}(\boldsymbol{\beta}_{m}).
  \tag{2.15}
\end{equation}\]</span></p>
<p>Using the matrix notation in Section <a href="estimation.html#matrixform">2.7.1</a> and defining <span class="math inline">\(\boldsymbol{W} = \boldsymbol{D}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{D}\)</span>, we can write</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{F}
  = \boldsymbol{X}^{T}\boldsymbol{D}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{D}\boldsymbol{X}
  = \boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{X}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{S}
  = \boldsymbol{X}^{T}\boldsymbol{D}\boldsymbol{\Sigma}^{-1}(\boldsymbol{Y} - \boldsymbol{\mu})
  = \boldsymbol{X}^{T}\boldsymbol{D}^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{Y} - \boldsymbol{\mu})
  = \boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{D}^{-1}(\boldsymbol{Y} - \boldsymbol{\mu}),
\end{equation}\]</span></p>
<p>since <span class="math inline">\(\boldsymbol{D}\)</span> is a diagonal matrix.</p>
<p>Thus, using the subscript <span class="math inline">\(m\)</span> to denote the value of a quantity evaluated using <span class="math inline">\(\boldsymbol{\beta}_{m}\)</span> or derived quantities, we can calculate the right hand side of Equation <a href="estimation.html#eq:fisheriteration">(2.15)</a> from:</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{F}_{m}\boldsymbol{\beta}_{m} + \boldsymbol{S}_{m}
  = \boldsymbol{X}^{T}\boldsymbol{W}_{m}\boldsymbol{X}\boldsymbol{\beta}_{m} +
    \boldsymbol{X}^{T}\boldsymbol{W}_{m}\boldsymbol{D}_{m}^{-1}(\boldsymbol{Y} - \boldsymbol{\mu}_{m})
  = \boldsymbol{X}^{T}\boldsymbol{W}_{m}\boldsymbol{\tilde{Y}}_{m}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{\tilde{Y}}_{m}
  = \boldsymbol{X}\boldsymbol{\beta}_{m} + \boldsymbol{D}_{m}^{-1}(\boldsymbol{Y} - \boldsymbol{\mu}_{m})
\end{equation}\]</span></p>
<p>are the so-called <em>working observations</em>.</p>
<p>Now replacing <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}_{m}) = \boldsymbol{F}_{m} = \boldsymbol{X}^{T}\boldsymbol{W}_{m}\boldsymbol{X}\)</span> in the left hand side of Equation <a href="estimation.html#eq:fisheriteration">(2.15)</a>, we have that</p>
<p><span class="math display">\[\begin{equation}
  (\boldsymbol{X}^{T}\boldsymbol{W}_{m}\boldsymbol{X}) \boldsymbol{\beta}_{m+1} = \boldsymbol{X}^{T}\boldsymbol{W}_{m}\boldsymbol{\tilde{Y}}_{m}
\end{equation}\]</span></p>
<p>or</p>
<p><span class="math display" id="eq:irlsupdate">\[\begin{equation}
  \boldsymbol{\beta}_{m+1} = (\boldsymbol{X}^{T}\boldsymbol{W}_{m}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\boldsymbol{W}_{m}\boldsymbol{\tilde{Y}}_{m}.
  \tag{2.16}
\end{equation}\]</span></p>
<p>Thus, to find a solution for <span class="math inline">\(\boldsymbol{S}(\boldsymbol{\beta}) = 0\)</span>, we can start from an arbitrary point <span class="math inline">\(\boldsymbol{\beta}_{0}\)</span> and iteratively apply Equation <a href="estimation.html#eq:irlsupdate">(2.16)</a> until a convergence criterion is met.</p>
<p>This sequence of iterated operations is called <em>iteratively reweighted least squares</em> or <em>iterative weighted least squares</em> since each iteration is the solution to the following least squares problem: minimize the quantity <span class="math inline">\(l_{m}(\boldsymbol{\beta})\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>, where</p>
<p><span class="math display">\[\begin{equation}
  l_{m}(\boldsymbol{\beta})
  = (\boldsymbol{\tilde{Y}}_{m} - \boldsymbol{X}\boldsymbol{\beta})^{T}
    \boldsymbol{W}_{m}
    (\boldsymbol{\tilde{Y}}_{m} - \boldsymbol{X}\boldsymbol{\beta})
\end{equation}\]</span></p>
<p>and <span class="math inline">\(\boldsymbol{W}_{m}\)</span> is known as the <em>weight matrix</em>.</p>
</div>
<div id="irls-pseudo-code" class="section level3 hasAnchor" number="2.8.2">
<h3><span class="header-section-number">2.8.2</span> IRLS Pseudo-Code<a href="estimation.html#irls-pseudo-code" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now give a pseudo-code below for running IRLS. Note that the code will not run without computing <span class="math inline">\(\boldsymbol{\mu}\)</span>, <span class="math inline">\(\boldsymbol{D}\)</span> and <span class="math inline">\(\boldsymbol{W}\)</span> using a specific example.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="estimation.html#cb1-1" tabindex="-1"></a>IRLS <span class="ot">&lt;-</span> <span class="cf">function</span>(Y, X, phi, epsilon) {</span>
<span id="cb1-2"><a href="estimation.html#cb1-2" tabindex="-1"></a>    <span class="co"># Pick an initial value for hatBeta</span></span>
<span id="cb1-3"><a href="estimation.html#cb1-3" tabindex="-1"></a>    hatbeta <span class="ot">=</span> <span class="fu">initializeBeta</span>()</span>
<span id="cb1-4"><a href="estimation.html#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="estimation.html#cb1-5" tabindex="-1"></a>    <span class="co"># Set up convergence</span></span>
<span id="cb1-6"><a href="estimation.html#cb1-6" tabindex="-1"></a>    converged <span class="ot">=</span> false</span>
<span id="cb1-7"><a href="estimation.html#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="estimation.html#cb1-8" tabindex="-1"></a>    <span class="co"># Loop as long as convergence condition is not satisfied</span></span>
<span id="cb1-9"><a href="estimation.html#cb1-9" tabindex="-1"></a>    <span class="cf">while</span> not converged loop</span>
<span id="cb1-10"><a href="estimation.html#cb1-10" tabindex="-1"></a>    {</span>
<span id="cb1-11"><a href="estimation.html#cb1-11" tabindex="-1"></a>        <span class="co"># Compute mu, D, and Sigma (use h, h&#39;, V as subroutines)</span></span>
<span id="cb1-12"><a href="estimation.html#cb1-12" tabindex="-1"></a>        mu <span class="ot">=</span> <span class="fu">computeMu</span>(hatBeta, X)</span>
<span id="cb1-13"><a href="estimation.html#cb1-13" tabindex="-1"></a>        D <span class="ot">=</span> <span class="fu">computeD</span>(hatBeta, X)</span>
<span id="cb1-14"><a href="estimation.html#cb1-14" tabindex="-1"></a>        Sigma <span class="ot">=</span> <span class="fu">computeSigma</span>(hatBeta, phi)</span>
<span id="cb1-15"><a href="estimation.html#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="estimation.html#cb1-16" tabindex="-1"></a>        <span class="co"># Compute the weight matrix W</span></span>
<span id="cb1-17"><a href="estimation.html#cb1-17" tabindex="-1"></a>        W <span class="ot">=</span> <span class="fu">t</span>(D) <span class="sc">%*%</span> <span class="fu">solve</span>(Sigma) <span class="sc">%*%</span> D</span>
<span id="cb1-18"><a href="estimation.html#cb1-18" tabindex="-1"></a></span>
<span id="cb1-19"><a href="estimation.html#cb1-19" tabindex="-1"></a>        <span class="co"># Compute the working observations tildeY</span></span>
<span id="cb1-20"><a href="estimation.html#cb1-20" tabindex="-1"></a>        tildeY <span class="ot">=</span> X <span class="sc">%*%</span> hatBeta <span class="sc">+</span> <span class="fu">solve</span>(D) <span class="sc">%*%</span> (Y <span class="sc">-</span> mu)</span>
<span id="cb1-21"><a href="estimation.html#cb1-21" tabindex="-1"></a></span>
<span id="cb1-22"><a href="estimation.html#cb1-22" tabindex="-1"></a>        <span class="co"># Compute the new value of hatBeta</span></span>
<span id="cb1-23"><a href="estimation.html#cb1-23" tabindex="-1"></a>        newHatBeta <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> W <span class="sc">%*%</span> X) <span class="sc">%*%</span> (<span class="fu">t</span>(X) <span class="sc">%*%</span> W <span class="sc">%*%</span> tildeY)</span>
<span id="cb1-24"><a href="estimation.html#cb1-24" tabindex="-1"></a></span>
<span id="cb1-25"><a href="estimation.html#cb1-25" tabindex="-1"></a>        <span class="co"># Check whether we have converged</span></span>
<span id="cb1-26"><a href="estimation.html#cb1-26" tabindex="-1"></a>        converged <span class="ot">=</span> ((<span class="fu">norm</span>(newHatBeta <span class="sc">-</span> hatBeta) <span class="sc">/</span> <span class="fu">norm</span>(hatBeta)) <span class="sc">&lt;=</span> epsilon)</span>
<span id="cb1-27"><a href="estimation.html#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="estimation.html#cb1-28" tabindex="-1"></a>        <span class="co"># Store new value of hatBeta ready for next iteration or return</span></span>
<span id="cb1-29"><a href="estimation.html#cb1-29" tabindex="-1"></a>        hatBeta <span class="ot">=</span> newHatBeta</span>
<span id="cb1-30"><a href="estimation.html#cb1-30" tabindex="-1"></a>    }</span>
<span id="cb1-31"><a href="estimation.html#cb1-31" tabindex="-1"></a></span>
<span id="cb1-32"><a href="estimation.html#cb1-32" tabindex="-1"></a>    return hatBeta</span>
<span id="cb1-33"><a href="estimation.html#cb1-33" tabindex="-1"></a>}</span></code></pre></div>
</div>
</div>
<div id="practical-example-us-polio-data" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Practical Example: US Polio Data<a href="estimation.html#practical-example-us-polio-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this example, we will use the US Polio data discussed last term to fit the GLMs. Recall that this dataset is a matrix of count data, giving the monthly number of polio cases in the United States from 1970 to 1983. We will convert this dataset into a matrix with two columns:</p>
<ul>
<li>covariate <code>time</code> in the first column ranging from 1 to 168, starting with January 1970.</li>
<li>response <code>cases</code> in the second column, indicating the monthly number of polio cases.</li>
</ul>
<p>We now load the data from the library <code>gamlss.data</code> and do the conversion.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="estimation.html#cb2-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;gamlss.data&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;gamlss.data&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:datasets&#39;:
## 
##     sleep</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="estimation.html#cb5-1" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;polio&quot;</span>)</span>
<span id="cb5-2"><a href="estimation.html#cb5-2" tabindex="-1"></a>uspolio <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">168</span>, <span class="fu">t</span>(polio)), <span class="at">ncol=</span><span class="dv">2</span>))</span>
<span id="cb5-3"><a href="estimation.html#cb5-3" tabindex="-1"></a><span class="fu">colnames</span>(uspolio) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;time&quot;</span>, <span class="st">&quot;cases&quot;</span>)</span></code></pre></div>
<p>First, let us plot the data.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="estimation.html#cb6-1" tabindex="-1"></a><span class="fu">plot</span>(uspolio, <span class="at">type=</span><span class="st">&quot;h&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/plotpolio2-1.png" width="672" /></p>
<p>Note that the main question we wish to consider is: <em>How has Polio incidence changed over time?</em></p>
<p>Since this is count data, we begin by fitting a Poisson model with a linear time trend.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="estimation.html#cb7-1" tabindex="-1"></a><span class="co"># Poisson model with linear time trend</span></span>
<span id="cb7-2"><a href="estimation.html#cb7-2" tabindex="-1"></a>polio.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(cases <span class="sc">~</span> time, <span class="at">family=</span><span class="fu">poisson</span>(<span class="at">link=</span>log), <span class="at">data=</span>uspolio)</span>
<span id="cb7-3"><a href="estimation.html#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="estimation.html#cb7-4" tabindex="-1"></a><span class="co"># Look at the model summary</span></span>
<span id="cb7-5"><a href="estimation.html#cb7-5" tabindex="-1"></a><span class="fu">summary</span>(polio.glm)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cases ~ time, family = poisson(link = log), data = uspolio)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.626639   0.123641   5.068 4.02e-07 ***
## time        -0.004263   0.001395  -3.055  0.00225 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 343.00  on 167  degrees of freedom
## Residual deviance: 333.55  on 166  degrees of freedom
## AIC: 594.59
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>We can then plot the model as follows.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="estimation.html#cb9-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), uspolio<span class="sc">$</span>cases, <span class="at">type=</span><span class="st">&quot;h&quot;</span>)</span>
<span id="cb9-2"><a href="estimation.html#cb9-2" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), polio.glm<span class="sc">$</span>fitted)</span></code></pre></div>
<p><img src="_main_files/figure-html/poliopoissonmodelplot-1.png" width="672" /></p>
<p>We can see that this is perhaps unsatisfactory. To improve the model, we can explore a linear trend with seasonal (annual) components.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="estimation.html#cb10-1" tabindex="-1"></a><span class="co"># Poisson model with linear trend and annual components</span></span>
<span id="cb10-2"><a href="estimation.html#cb10-2" tabindex="-1"></a>polio1.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(cases <span class="sc">~</span> time <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">12</span>)) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">12</span>)),</span>
<span id="cb10-3"><a href="estimation.html#cb10-3" tabindex="-1"></a><span class="at">family=</span><span class="fu">poisson</span>(<span class="at">link=</span>log), <span class="at">data=</span>uspolio)</span>
<span id="cb10-4"><a href="estimation.html#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="estimation.html#cb10-5" tabindex="-1"></a><span class="fu">summary</span>(polio1.glm)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * 
##     pi * time/12)), family = poisson(link = log), data = uspolio)
## 
## Coefficients:
##                           Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)               0.606612   0.124800   4.861 1.17e-06 ***
## time                     -0.004644   0.001401  -3.315 0.000916 ***
## I(cos(2 * pi * time/12))  0.181254   0.096160   1.885 0.059442 .  
## I(sin(2 * pi * time/12)) -0.423187   0.097590  -4.336 1.45e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 343.00  on 167  degrees of freedom
## Residual deviance: 310.72  on 164  degrees of freedom
## AIC: 575.77
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="estimation.html#cb12-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), uspolio<span class="sc">$</span>cases, <span class="at">type=</span><span class="st">&quot;h&quot;</span>)</span>
<span id="cb12-2"><a href="estimation.html#cb12-2" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), polio1.glm<span class="sc">$</span>fitted, <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/poliopoissonannual-1.png" width="672" /></p>
<p>We can also add six-monthly components into the model.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="estimation.html#cb13-1" tabindex="-1"></a><span class="co"># Poisson model with linear trend and seasonal (annual + six-monthly) components</span></span>
<span id="cb13-2"><a href="estimation.html#cb13-2" tabindex="-1"></a>polio2.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(cases <span class="sc">~</span> time <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">12</span>)) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">12</span>))</span>
<span id="cb13-3"><a href="estimation.html#cb13-3" tabindex="-1"></a><span class="sc">+</span> <span class="fu">I</span>(<span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">6</span>)) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">6</span>)), <span class="at">family=</span><span class="fu">poisson</span>(<span class="at">link=</span>log),</span>
<span id="cb13-4"><a href="estimation.html#cb13-4" tabindex="-1"></a><span class="at">data=</span>uspolio)</span>
<span id="cb13-5"><a href="estimation.html#cb13-5" tabindex="-1"></a></span>
<span id="cb13-6"><a href="estimation.html#cb13-6" tabindex="-1"></a><span class="fu">summary</span>(polio2.glm)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cases ~ time + I(cos(2 * pi * time/12)) + I(sin(2 * 
##     pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * pi * 
##     time/6)), family = poisson(link = log), data = uspolio)
## 
## Coefficients:
##                           Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)               0.557241   0.127303   4.377 1.20e-05 ***
## time                     -0.004799   0.001403  -3.421 0.000625 ***
## I(cos(2 * pi * time/12))  0.137132   0.089479   1.533 0.125384    
## I(sin(2 * pi * time/12)) -0.534985   0.115476  -4.633 3.61e-06 ***
## I(cos(2 * pi * time/6))   0.458797   0.101467   4.522 6.14e-06 ***
## I(sin(2 * pi * time/6))  -0.069627   0.098123  -0.710 0.477957    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 343.00  on 167  degrees of freedom
## Residual deviance: 288.85  on 162  degrees of freedom
## AIC: 557.9
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="estimation.html#cb15-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), uspolio<span class="sc">$</span>cases, <span class="at">type=</span><span class="st">&quot;h&quot;</span>)</span>
<span id="cb15-2"><a href="estimation.html#cb15-2" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), polio2.glm<span class="sc">$</span>fitted, <span class="at">col=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/poliopoissonsizmonthly-1.png" width="672" /></p>
<p>Assuming we have annual temperature data over the 14 years, we can add them into the model to investigate their effects.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="estimation.html#cb16-1" tabindex="-1"></a><span class="co"># Average annual temperature data over the 14 years</span></span>
<span id="cb16-2"><a href="estimation.html#cb16-2" tabindex="-1"></a>temp_data <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="fl">5.195</span>, <span class="fl">5.138</span>, <span class="fl">5.316</span>, <span class="fl">5.242</span>, <span class="fl">5.094</span>, <span class="fl">5.108</span>, <span class="fl">5.260</span>, <span class="fl">5.153</span>, </span>
<span id="cb16-3"><a href="estimation.html#cb16-3" tabindex="-1"></a>                   <span class="fl">5.155</span>, <span class="fl">5.231</span>, <span class="fl">5.234</span>, <span class="fl">5.142</span>, <span class="fl">5.173</span>, <span class="fl">5.167</span>), <span class="at">each=</span><span class="dv">12</span>)</span>
<span id="cb16-4"><a href="estimation.html#cb16-4" tabindex="-1"></a></span>
<span id="cb16-5"><a href="estimation.html#cb16-5" tabindex="-1"></a><span class="co"># Scale the data so that it plots nicely</span></span>
<span id="cb16-6"><a href="estimation.html#cb16-6" tabindex="-1"></a>scaled_temp <span class="ot">=</span> <span class="dv">10</span> <span class="sc">*</span> (temp_data <span class="sc">-</span> <span class="fu">min</span>(temp_data))<span class="sc">/</span>(<span class="fu">max</span>(temp_data) <span class="sc">-</span> <span class="fu">min</span>(temp_data))</span>
<span id="cb16-7"><a href="estimation.html#cb16-7" tabindex="-1"></a>uspolio<span class="sc">$</span>temp <span class="ot">=</span> scaled_temp</span>
<span id="cb16-8"><a href="estimation.html#cb16-8" tabindex="-1"></a></span>
<span id="cb16-9"><a href="estimation.html#cb16-9" tabindex="-1"></a><span class="co"># Plot temperature data against cases data to see interest</span></span>
<span id="cb16-10"><a href="estimation.html#cb16-10" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), uspolio<span class="sc">$</span>cases, <span class="at">type=</span><span class="st">&quot;h&quot;</span>)</span>
<span id="cb16-11"><a href="estimation.html#cb16-11" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), uspolio<span class="sc">$</span>temp, <span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/polioaddtempdata-1.png" width="672" /></p>
<p>Poisson GLM with temperature data.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="estimation.html#cb17-1" tabindex="-1"></a><span class="co"># Poisson model with additional temperature covariate</span></span>
<span id="cb17-2"><a href="estimation.html#cb17-2" tabindex="-1"></a>polio3.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(cases <span class="sc">~</span> time <span class="sc">+</span> temp <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">12</span>)) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">12</span>))</span>
<span id="cb17-3"><a href="estimation.html#cb17-3" tabindex="-1"></a><span class="sc">+</span> <span class="fu">I</span>(<span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">6</span>)) <span class="sc">+</span> <span class="fu">I</span>(<span class="fu">sin</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span>time<span class="sc">/</span><span class="dv">6</span>)) , <span class="at">family=</span><span class="fu">poisson</span>(<span class="at">link=</span>log),</span>
<span id="cb17-4"><a href="estimation.html#cb17-4" tabindex="-1"></a><span class="at">data=</span>uspolio)</span>
<span id="cb17-5"><a href="estimation.html#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="estimation.html#cb17-6" tabindex="-1"></a><span class="fu">summary</span>(polio3.glm)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = cases ~ time + temp + I(cos(2 * pi * time/12)) + 
##     I(sin(2 * pi * time/12)) + I(cos(2 * pi * time/6)) + I(sin(2 * 
##     pi * time/6)), family = poisson(link = log), data = uspolio)
## 
## Coefficients:
##                           Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)               0.129643   0.186352   0.696 0.486623    
## time                     -0.003972   0.001439  -2.761 0.005770 ** 
## temp                      0.080308   0.023139   3.471 0.000519 ***
## I(cos(2 * pi * time/12))  0.136094   0.089489   1.521 0.128314    
## I(sin(2 * pi * time/12)) -0.531668   0.115466  -4.605 4.13e-06 ***
## I(cos(2 * pi * time/6))   0.457487   0.101435   4.510 6.48e-06 ***
## I(sin(2 * pi * time/6))  -0.068345   0.098149  -0.696 0.486218    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 343.00  on 167  degrees of freedom
## Residual deviance: 276.84  on 161  degrees of freedom
## AIC: 547.88
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="estimation.html#cb19-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), uspolio<span class="sc">$</span>cases, <span class="at">type=</span><span class="st">&quot;h&quot;</span>)</span>
<span id="cb19-2"><a href="estimation.html#cb19-2" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1970</span> <span class="sc">+</span> ((uspolio<span class="sc">$</span>time <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span><span class="dv">12</span>), polio3.glm<span class="sc">$</span>fitted, <span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/poliopoissontemperature-1.png" width="672" /></p>
</div>
<div id="estimation-of-dispersion-parameter-phi" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Estimation of Dispersion Parameter <span class="math inline">\(\phi\)</span><a href="estimation.html#estimation-of-dispersion-parameter-phi" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Because the dispersion <span class="math inline">\(\phi\)</span> cancels from the score equation <span class="math inline">\(\boldsymbol{S}(\hat{\boldsymbol{\beta}}) = 0\)</span>, there is no need to estimate <span class="math inline">\(\phi\)</span> in order to estimate <span class="math inline">\(\boldsymbol{\beta}\)</span>. However, <span class="math inline">\({\mathrm{Var}}[\hat{\boldsymbol{\beta}}]\)</span> does depend on <span class="math inline">\(\phi\)</span>, as one might expect. Thus, if necessary or of interest, <span class="math inline">\(\phi\)</span> can be estimated via:</p>
<p><span class="math display" id="eq:estimatephi">\[\begin{equation}
  \hat{\phi}
  = \frac{1}{n-p} \sum_{i} m_{i} \frac{(y_{i} - \hat{\mu}_{i})^{2}}{\mathcal{V}(\hat{\mu}_{i})}
  \tag{2.17}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of parameters of the model. The motivation for the above estimation is that:</p>
<p><span class="math display">\[\begin{equation}
  {\mathrm{Var}}[y_{i}]
  = {\mathrm E}[(y_{i} - \mu_{i})^{2}]
  = \phi_{i}\mathcal{V}(\mu_{i})
  = \frac{\phi}{m_{i}} \mathcal{V}(\mu_{i}),
\end{equation}\]</span></p>
<p>which can be rearranged to</p>
<p><span class="math display">\[\begin{equation}
  \phi
  = \frac{m_{i}}{\mathcal{V}(\mu_{i})} {\mathrm E}[(y_{i} - \mu_{i})^{2}]
  = \mathrm{E} \left[ m_{i} \frac{(y_{i} - \mu_{i})^{2}}{\mathcal{V}(\mu_{i})} \right].
\end{equation}\]</span></p>
<p>Thus, after estimating <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, we can use its value and Equation <a href="estimation.html#eq:estimatephi">(2.17)</a> to estimate <span class="math inline">\(\hat{\phi}\)</span>.</p>
<div id="special-cases" class="section level3 hasAnchor" number="2.10.1">
<h3><span class="header-section-number">2.10.1</span> Special Cases<a href="estimation.html#special-cases" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="gaussian" class="section level4 hasAnchor" number="2.10.1.1">
<h4><span class="header-section-number">2.10.1.1</span> Gaussian<a href="estimation.html#gaussian" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When <span class="math inline">\(Y |\boldsymbol{\beta}, x \sim {\mathcal N}(\mu, \sigma^{2})\)</span> with <span class="math inline">\(m_i = 1\)</span>, we have <span class="math inline">\(\mathcal{V}(\mu_{i}) = 1\)</span> and thus,</p>
<p><span class="math display">\[\begin{equation}
  \hat{\phi}
  = \frac{1}{n - p} \sum_{i} (y_{i} - \hat{\mu}_{i})^{2}
  = \hat{\sigma}^{2}.
\end{equation}\]</span></p>
</div>
<div id="gamma" class="section level4 hasAnchor" number="2.10.1.2">
<h4><span class="header-section-number">2.10.1.2</span> Gamma<a href="estimation.html#gamma" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall from Exercise 7.1 last term that we can parameterise the Gamma function in terms of its mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>, and we found that <span class="math inline">\(\mathcal{V}(\mu) = \mu^2\)</span>. Thus, when <span class="math inline">\(Y |\boldsymbol{\beta}, x \sim \text{Gamma}(\mu, \sigma^{2})\)</span>, we have</p>
<p><span class="math display">\[\begin{equation}
  \frac{1}{\hat{\nu}}
  = \hat{\phi}
  = \frac{1}{n - p} \sum_{i} m_{i} \frac{(y_{i} - \hat{\mu}_{i})^{2}}{\hat{\mu}_{i}^{2}}.
\end{equation}\]</span></p>
</div>
</div>
<div id="practical-example-hospital-stay-data" class="section level3 hasAnchor" number="2.10.2">
<h3><span class="header-section-number">2.10.2</span> Practical Example: Hospital Stay Data<a href="estimation.html#practical-example-hospital-stay-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this example, we will use the Hospital Stay data introduced last term to fit a Gamma GLM and estimate its dispersion parameter.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="estimation.html#cb20-1" tabindex="-1"></a><span class="fu">library</span>(npmlreg)</span>
<span id="cb20-2"><a href="estimation.html#cb20-2" tabindex="-1"></a><span class="fu">data</span>(hosp)</span>
<span id="cb20-3"><a href="estimation.html#cb20-3" tabindex="-1"></a></span>
<span id="cb20-4"><a href="estimation.html#cb20-4" tabindex="-1"></a><span class="co"># Fit the GLM and print the summary</span></span>
<span id="cb20-5"><a href="estimation.html#cb20-5" tabindex="-1"></a>hosp.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(duration <span class="sc">~</span> age <span class="sc">+</span> temp1, <span class="at">data=</span>hosp, <span class="at">family=</span><span class="fu">Gamma</span>(<span class="at">link=</span>log))</span>
<span id="cb20-6"><a href="estimation.html#cb20-6" tabindex="-1"></a><span class="fu">summary</span>(hosp.glm)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = duration ~ age + temp1, family = Gamma(link = log), 
##     data = hosp)
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -28.654096  16.621018  -1.724   0.0987 .
## age           0.014900   0.005698   2.615   0.0158 *
## temp1         0.306624   0.168141   1.824   0.0818 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Gamma family taken to be 0.2690233)
## 
##     Null deviance: 8.1722  on 24  degrees of freedom
## Residual deviance: 5.7849  on 22  degrees of freedom
## AIC: 142.73
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="estimation.html#cb22-1" tabindex="-1"></a><span class="co"># From the summary, note the line:</span></span>
<span id="cb22-2"><a href="estimation.html#cb22-2" tabindex="-1"></a><span class="co"># (Dispersion parameter for Gamma family taken to be 0.2690233)</span></span>
<span id="cb22-3"><a href="estimation.html#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="estimation.html#cb22-4" tabindex="-1"></a><span class="co"># Compute by hand</span></span>
<span id="cb22-5"><a href="estimation.html#cb22-5" tabindex="-1"></a><span class="dv">1</span><span class="sc">/</span>(hosp.glm<span class="sc">$</span>df.res)<span class="sc">*</span><span class="fu">sum</span>((hosp<span class="sc">$</span>duration<span class="sc">-</span>hosp.glm<span class="sc">$</span>fitted)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(hosp.glm<span class="sc">$</span>fitted<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.2690233</code></pre>
</div>
</div>
<div id="asymptotic" class="section level2 hasAnchor" number="2.11">
<h2><span class="header-section-number">2.11</span> Asymptotic Properties of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span><a href="estimation.html#asymptotic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our context, <em>asymptotic</em> means that <span class="math inline">\(M = \sum_{i = 1}^{n} m_{i}\rightarrow \infty\)</span>. This could be because <span class="math inline">\(n\rightarrow \infty\)</span>, or because the <span class="math inline">\(m_{i}\rightarrow\infty\)</span>, or a combination of both.</p>
<p>Let us denote the true value of <span class="math inline">\(\boldsymbol{\beta}\)</span> by <span class="math inline">\(\boldsymbol{\beta}^*\)</span>. In the following, we assume consistency of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, i.e., <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> converges in probability to <span class="math inline">\(\boldsymbol{\beta}^*\)</span>, meaning that <span class="math inline">\(P(||\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*|| \geq \varepsilon) \rightarrow 0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. We will denote this by <span class="math inline">\(\hat{\boldsymbol{\beta}}\stackrel{a}{=} \boldsymbol{\beta}^*\)</span>. We will also abuse this notation to mean “tends to asymptotically” for expectations, i.e., if we write <span class="math inline">\(E[Z] \stackrel{a}{=} z\)</span>, that means <span class="math inline">\(E[Z] \stackrel{n \rightarrow \infty}{\longrightarrow} z\)</span>.</p>
<p>From the consistency assumption, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> will be close to <span class="math inline">\(\boldsymbol{\beta}^*\)</span> asymptotically, and we can expand <span class="math inline">\(\boldsymbol{S}\)</span> around it:</p>
<p><span class="math display">\[\begin{align}
  \boldsymbol{S}(\hat{\boldsymbol{\beta}}) = 0
  &amp; \stackrel{a}{=} \boldsymbol{S}(\boldsymbol{\beta}^*) +
    \frac{\partial \boldsymbol{S}(\boldsymbol{\beta}^*)}{\partial \boldsymbol{\beta}^{T}}(\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*) \\
  &amp; = \boldsymbol{S}(\boldsymbol{\beta}^*)
      -\boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}^*) (\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*)
\end{align}\]</span></p>
<p>or equivalently,</p>
<p><span class="math display" id="eq:expandS">\[\begin{equation}
  \hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*
  \stackrel{a}{=} \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}^*)^{-1} \boldsymbol{S}(\boldsymbol{\beta}^*).
  \tag{2.18}
\end{equation}\]</span></p>
<div id="fisher-scoring" class="section level3 hasAnchor" number="2.11.1">
<h3><span class="header-section-number">2.11.1</span> Fisher Scoring<a href="estimation.html#fisher-scoring" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Section <a href="estimation.html#iterativesolution">2.8</a>, we stated that we often use the (expected) Fisher Information in place of the Observed Fisher Information (known as the Fisher Scoring method). Doing so in the context of asymptotic arguments is acceptable. We can roughly see this as follows. For any <span class="math inline">\(\boldsymbol{\beta}\)</span>,</p>
<p><span class="math display">\[\begin{equation}
  \frac{1}{n}\boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})
  = - \frac{1}{n} \frac{\partial l}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^T} (\boldsymbol{\beta})
  = - \frac{1}{n} \sum_{i=1}^n \frac{\partial l_i}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^T} (\boldsymbol{\beta})
  \rightarrow - \mathrm{E} \left[\frac{\partial l_1}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^T} (\boldsymbol{\beta}) \right]
  = F_1(\boldsymbol{\beta})
\end{equation}\]</span></p>
<p>where <span class="math inline">\(F_1(\boldsymbol{\beta})\)</span> is the expected Fisher Information for a sample of size <span class="math inline">\(1\)</span> and we are using the law of large numbers as <span class="math inline">\(n \rightarrow \infty\)</span> here.
It can be shown (see exercise section) that <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}) = nF_1(\boldsymbol{\beta})\)</span>, thus justifying use of <span class="math inline">\(\boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}) \stackrel{a}{=} \boldsymbol{F}(\boldsymbol{\beta})\)</span> in the forthcoming asymptotic arguments.</p>
</div>
<div id="expectation" class="section level3 hasAnchor" number="2.11.2">
<h3><span class="header-section-number">2.11.2</span> Expectation<a href="estimation.html#expectation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From Equation <a href="estimation.html#eq:expandS">(2.18)</a>, we have:</p>
<p><span class="math display" id="eq:betaintermsofs">\[\begin{equation}
  \hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*
  \stackrel{a}{=} \boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta}^*)^{-1} \boldsymbol{S}(\boldsymbol{\beta}^*)
  \stackrel{a}{=} \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1} \boldsymbol{S}(\boldsymbol{\beta}^*).
  \tag{2.19}
\end{equation}\]</span></p>
<p>Because convergence in probability implies convergence in distribution, this in turn implies that</p>
<p><span class="math display">\[\begin{equation}
  E[\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*]
  \stackrel{a}{=} \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1} E[\boldsymbol{S}(\boldsymbol{\beta}^*)]
  = 0.
\end{equation}\]</span></p>
<p>In other words, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is asymptotically unbiased.</p>
</div>
<div id="variance" class="section level3 hasAnchor" number="2.11.3">
<h3><span class="header-section-number">2.11.3</span> Variance<a href="estimation.html#variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since <span class="math inline">\(E[\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*] \stackrel{a}{=} 0\)</span>, we have that</p>
<p><span class="math display">\[\begin{align}
  {\mathrm{Var}}[\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*]
  &amp; \stackrel{a}{=} {\mathrm E}[(\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*)(\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*)^{T}] \\
  &amp; \stackrel{a}{=} {\mathrm E}[\boldsymbol{F}(\boldsymbol{\beta}^*)^{-1} \boldsymbol{S}(\boldsymbol{\beta}^*)
    \boldsymbol{S}(\boldsymbol{\beta}^*)^{T} \boldsymbol{F}(\boldsymbol{\beta}^*)^{-T}] \\
  &amp; = \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1} {\mathrm E}[\boldsymbol{S}(\boldsymbol{\beta}^*)
    \boldsymbol{S}(\boldsymbol{\beta}^*)^{T}] \boldsymbol{F}(\boldsymbol{\beta}^*)^{-T} \\
  &amp; = \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1} {\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta}^*)] \boldsymbol{F}(\boldsymbol{\beta}^*)^{-T} \\
  &amp; = \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1}
\end{align}\]</span></p>
<p>where we have used symmetry of <span class="math inline">\(\boldsymbol{F}\)</span> and the fact that <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}^*) = {\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta}^*)]\)</span>.</p>
<p>Thus,</p>
<p><span class="math display" id="eq:varhbeta">\[\begin{equation}
  {\mathrm{Var}}[\hat{\boldsymbol{\beta}}] = {\mathrm{Var}}[\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*]
  \stackrel{a}{=} \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1}.
  \tag{2.20}
\end{equation}\]</span></p>
</div>
<div id="asymptotic-normality" class="section level3 hasAnchor" number="2.11.4">
<h3><span class="header-section-number">2.11.4</span> Asymptotic Normality<a href="estimation.html#asymptotic-normality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following is a sketch of the argument of asymptotic normality for <span class="math inline">\(\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*\)</span>, i.e., <span class="math inline">\(\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*\)</span> converges asymptotically to a normal distribution. We start from</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{S}(\boldsymbol{\beta}) = \sum_{i} \boldsymbol{S}_{i}(\boldsymbol{\beta})
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{S}_{i}(\boldsymbol{\beta})\)</span> is defined in Section <a href="estimation.html#properties">2.6</a>. This is a sum of independent random variables with zero mean and finite variance. As the number of terms in the sum tends to infinity, then under a certain condition, the distribution of the sum converges in distribution to a normal distribution. Since <span class="math inline">\({\mathrm E}[\boldsymbol{S}(\boldsymbol{\beta})] = 0\)</span> and <span class="math inline">\({\mathrm{Var}}[\boldsymbol{S}(\boldsymbol{\beta})] = \boldsymbol{F}(\boldsymbol{\beta})\)</span>, we have:</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{S}(\boldsymbol{\beta}) \stackrel{a}{\sim} {\mathcal N}(0, \boldsymbol{F}(\boldsymbol{\beta})).
\end{equation}\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[\begin{equation}
  \hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*
  \stackrel{a}{=} \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1} \boldsymbol{S}(\boldsymbol{\beta}^*)
  \stackrel{a}{\sim} {\mathcal N}(0, \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1} \boldsymbol{F}(\boldsymbol{\beta}^*) \boldsymbol{F}(\boldsymbol{\beta}^*)^{-T}).
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(\boldsymbol{F}\)</span> is symmetric and convergence in probability implies convergence in distribution, we have:</p>
<p><span class="math display" id="eq:disthbeta">\[\begin{equation}
  \hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*
  \stackrel{a}{\sim}
  {\mathcal N}(0, \boldsymbol{F}(\boldsymbol{\beta}^*)^{-1}).
  \tag{2.21}
\end{equation}\]</span></p>
<p>This also implies that the square of Mahalanobis distance between <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\boldsymbol{\beta}^*\)</span> is asymptotically chi-square distributed:</p>
<p><span class="math display" id="eq:distmahalanobishbeta">\[\begin{equation}
  (\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*)^{T} \boldsymbol{F}(\boldsymbol{\beta}^*) (\hat{\boldsymbol{\beta}}- \boldsymbol{\beta}^*)
  \stackrel{a}{\sim} \chi^{2}(p)
  \tag{2.22}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of parameters.</p>
</div>
<div id="closing-the-circle" class="section level3 hasAnchor" number="2.11.5">
<h3><span class="header-section-number">2.11.5</span> Closing The Circle<a href="estimation.html#closing-the-circle" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>At the beginning of this section, we assumed that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> converges in probability to <span class="math inline">\(\boldsymbol{\beta}^*\)</span>. We may want to justify that this assumption is reasonable. Note that under some regularity conditions,</p>
<p><span class="math display">\[\begin{equation}
  \boldsymbol{F}(\boldsymbol{\beta})^{-1}
  = \left(\sum_{i} m_{i} \ldots \right)^{-1}
  \rightarrow 0
\end{equation}\]</span></p>
<p>as <span class="math inline">\(M\rightarrow\infty\)</span>. Thus <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> converges in distribution to a constant random variable, which means that it converges in probability too. This is what we were assuming.</p>
<p>Equations <a href="estimation.html#eq:varhbeta">(2.20)</a>, <a href="estimation.html#eq:disthbeta">(2.21)</a>, and <a href="estimation.html#eq:distmahalanobishbeta">(2.22)</a> remain valid when <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}^*)\)</span> is replaced by <span class="math inline">\(\boldsymbol{F}(\hat{\boldsymbol{\beta}})\)</span>.</p>
</div>
<div id="next-step" class="section level3 hasAnchor" number="2.11.6">
<h3><span class="header-section-number">2.11.6</span> Next Step<a href="estimation.html#next-step" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we have seen how to estimate the parameters and some of their sampling properties (asymptotically), we can move on to use these estimates to make inferences: predictions about new values and confidence intervals.</p>
</div>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="2.12">
<h2><span class="header-section-number">2.12</span> Exercises<a href="estimation.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Show that <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}) = nF_1(\boldsymbol{\beta})\)</span>, where <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta})\)</span> and <span class="math inline">\(F_1(\boldsymbol{\beta})\)</span> are the Fisher Information on datasets of size <span class="math inline">\(n\)</span> and <span class="math inline">\(1\)</span> respectively.</p></li>
<li><p>Using the notation and argument in Section <a href="estimation.html#asymptotic">2.11</a>, show that <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}^*) \stackrel{a}{=} \boldsymbol{F}(\hat{\boldsymbol{\beta}})\)</span> and thus we can replace <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}^*)\)</span> by <span class="math inline">\(\boldsymbol{F}(\hat{\boldsymbol{\beta}})\)</span> in Equations <a href="estimation.html#eq:varhbeta">(2.20)</a>, <a href="estimation.html#eq:disthbeta">(2.21)</a>, and <a href="estimation.html#eq:distmahalanobishbeta">(2.22)</a>.</p></li>
</ol>

<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div class="csl-entry">
Green, P. J. 1984. <span>“Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and Some Robust and Resistant Alternatives.”</span> <em>JRSSB</em> 46 (2): 149–92.
</div>
<div class="csl-entry">
Nelder, John A, and Robert WM Wedderburn. 1972. <span>“Generalized Linear Models.”</span> <em>Journal of the Royal Statistical Society Series A: Statistics in Society</em> 135 (3): 370–84.
</div>
</div>
</div>
</div>








<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-green1984irl" class="csl-entry">
Green, P. J. 1984. <span>“Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and Some Robust and Resistant Alternatives.”</span> <em>JRSSB</em> 46 (2): 149–92.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Note that here and in this module, we assume that <span class="math inline">\(\phi\)</span> does not depend on <span class="math inline">\(\boldsymbol{\beta}\)</span>.<a href="estimation.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Some texts refer to <span class="math inline">\(\boldsymbol{F}_{\text{obs}}(\hat{\boldsymbol{\beta}})\)</span> as the Observed Fisher Information, and to <span class="math inline">\(\boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})\)</span> simply as the Fisher Information. Some don’t refer to either of these at all. Just to be clear, we will refer to <span class="math inline">\(\boldsymbol{F}_{\text{obs}}(\boldsymbol{\beta})\)</span> as the Observed Fisher Information and the Expected Fisher Information <span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta})\)</span> simply as the Fisher Information.<a href="estimation.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>You have cause to be particularly interested in this algorithm as a Durham student. It is on the undergraduate syllabus of nearly every maths degree in the world which includes a large statistical component and some of the important early development was researched by Dr Peter Green when he was a lecturer at Durham: <span class="citation">Green (<a href="#ref-green1984irl">1984</a>)</span>.<a href="estimation.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-Estimation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
